{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is easier to debug, I like it.\n",
    "\n",
    "TODO:\n",
    "- [x] prioritised experience replay, need to grab loss for each sample, and sample based on loss\n",
    "- [ ] check it for my data, can it overfit?, does the normalisation make sense?\n",
    "- [x] better metrics\n",
    "- [ ] do cnn model\n",
    "- [x] read papers\n",
    "- [ ] check i'm prioristising by the right things, should lead to lowest loss\n",
    "- [ ] test on cartpole\n",
    "\n",
    "Refs: \n",
    "- implementations:\n",
    "    - PPO\n",
    "        - **pytorch implementation https://github.com/alexis-jacq/Pytorch-DPPO/blob/master/ppo.py**\n",
    "        - tensorflow implementation https://github.com/reinforceio/tensorforce/blob/master/tensorforce/models/ppo_model.py\n",
    "    - Prioritised memory\n",
    "    - Other\n",
    "        - http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training\n",
    "        - https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py\n",
    "- papers:\n",
    "    - DPPO https://arxiv.org/pdf/1707.02286.pdf\n",
    "    - PPO \n",
    "        - https://arxiv.org/abs/1707.06347\n",
    "        - https://blog.openai.com/openai-baselines-ppo/\n",
    "    - TRPO https://arxiv.org/abs/1502.05477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:50.626113Z",
     "start_time": "2017-08-05T08:02:49.851346Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:__main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# numeric\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display\n",
    "from pprint import pprint\n",
    "\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:50.876315Z",
     "start_time": "2017-08-05T08:02:50.627935Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:50.928708Z",
     "start_time": "2017-08-05T08:02:50.877693Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:50.990960Z",
     "start_time": "2017-08-05T08:02:50.930677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/agent_portfolio-ddpo/2017-07-21_weights.pickle'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        # env\n",
    "        self.window_length = 50\n",
    "        # Model\n",
    "        self.batch_size = 250\n",
    "        self.lr = 3e-4\n",
    "        self.gamma = 0.05\n",
    "        self.gae_param = 0.95\n",
    "        self.clip = 0.2 # epsilon from eq 7, default 0.2\n",
    "        self.ent_coeff = 0.\n",
    "        self.num_epoch = 20\n",
    "        self.num_steps = 2048\n",
    "        self.time_horizon = 1000000\n",
    "        self.max_episode_length = 10000\n",
    "        self.seed = 1\n",
    "\n",
    "params = Params()\n",
    "\n",
    "save_path= 'outputs/agent_portfolio-ddpo/{}_weights.pickle'.format('2017-07-21')\n",
    "try:\n",
    "    os.makedirs(os.path.dirname(save_path))\n",
    "except OSError:\n",
    "    pass\n",
    "save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "refs\n",
    "- https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "- https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:51.049262Z",
     "start_time": "2017-08-05T08:02:50.992981Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, events):\n",
    "        for event in zip(*events):\n",
    "            self.memory.append(event)\n",
    "            if len(self.memory)>self.capacity:\n",
    "                del self.memory[0]\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        return map(lambda x: torch.cat(x, 0), samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:51.165537Z",
     "start_time": "2017-08-05T08:02:51.050814Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data.sampler\n",
    "\n",
    "class PrioritisedReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, events):\n",
    "        # event is [states, actions, returns, advantages]\n",
    "        # [1x3x5x50,1x6,1x1,1x1]\n",
    "        for event in zip(*events):\n",
    "            self.memory.append(event)\n",
    "            if len(self.memory)>self.capacity:\n",
    "                del self.memory[0]\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def sample(self, batch_size, beta=0.5):\n",
    "        \"\"\"\n",
    "        Take a weighted sample based on advantages.\n",
    "        \n",
    "        Half hearted implementation of algorithm 1 from https://arxiv.org/pdf/1511.05952.pdf\n",
    "        \n",
    "        Better one here https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py#L157\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        beta: float\n",
    "            To what degree to use importance weights\n",
    "            (0 - minimal corrections, 1 - full correction)\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        states\n",
    "        actions\n",
    "        returns\n",
    "        advantages\n",
    "        \n",
    "        \"\"\"\n",
    "        # Sample transition\n",
    "        # we want to minimise loss so\n",
    "        #     weights ~ -loss (bigger the weight when the loss is lower)\n",
    "        # and\n",
    "        #     loss ~= -batch_advantages\n",
    "        # so \n",
    "        #     weights ~ batch_advantages\n",
    "        ps1 = torch.FloatTensor([m[-1].squeeze().data[0] for m in self.memory])        \n",
    "        ps1 -= ps1.min()\n",
    "        ps1 /= ps1.sum()\n",
    "        \n",
    "        # batch_returns\n",
    "#         ps2 = torch.FloatTensor([m[-2].squeeze().data[0] for m in self.memory])             \n",
    "#         ps2 -= ps2.min()\n",
    "#         ps2 /= ps2.sum()\n",
    "        \n",
    "        ps = ps1#+ps2 # priority        \n",
    "        ps -= ps.min() - 1.0\n",
    "        P = ps/ps.sum() # normalize\n",
    "        \n",
    "        # Compute importance-sampling weight\n",
    "        P = (len(P) * P) ** (-beta)\n",
    "        w = P / P.max()\n",
    "        \n",
    "        # to list and remove nans\n",
    "        w = w.numpy()\n",
    "#         w[np.isfinite(w)==False] = 0\n",
    "        w = w.tolist()\n",
    "        \n",
    "        idxs = torch.utils.data.sampler.WeightedRandomSampler(w, params.batch_size, replacement=False)\n",
    "        \n",
    "        # concatenate it into batches\n",
    "        samples = zip(*[self.memory[n] for n in idxs])        \n",
    "        return map(lambda x: torch.cat(x, 0), samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-04T12:46:15.785326Z",
     "start_time": "2017-08-04T20:46:15.643789+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:51.209445Z",
     "start_time": "2017-08-05T08:02:51.166883Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Test by making sure the sample has a higher returns than the mean\n",
    "# batch_states, batch_actions, batch_returns, batch_advantages = memory.sample(200)\n",
    "\n",
    "# ps1 = torch.FloatTensor([m[-1].squeeze().data[0] for m in memory.memory])        \n",
    "# print(batch_advantages.mean(),ps1.mean())\n",
    "# assert batch_advantages.mean().data[0]>ps1.mean()\n",
    "\n",
    "# # ps2 = torch.FloatTensor([m[-2].squeeze().data[0] for m in memory.memory]) \n",
    "# # assert batch_returns.mean().data[0]>ps2.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T00:55:29.885772Z",
     "start_time": "2017-08-02T08:55:29.883459+08:00"
    }
   },
   "source": [
    "# Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:51.404403Z",
     "start_time": "2017-08-05T08:02:51.210958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.environments.portfolio import PortfolioEnv, sharpe, max_drawdown\n",
    "\n",
    "# we want to pemute the channels a little\n",
    "\n",
    "class PermutedPortfolioEnv(PortfolioEnv):\n",
    "    def reset(self, *args, **kwargs):\n",
    "        return np.transpose(super().reset(*args, **kwargs),(0,1,2))\n",
    "    def step(self, *args, **kwargs):\n",
    "        observation, reward, done, info = super().step(*args, **kwargs)\n",
    "        observation = np.transpose(observation,(2,0,1))\n",
    "        return observation, reward, done, info\n",
    "\n",
    "\n",
    "df_train = pd.read_hdf('./data/poloniex_30m.hf',key='train')\n",
    "env = PermutedPortfolioEnv(\n",
    "    df=df_train,\n",
    "    steps=128, \n",
    "    scale=True, \n",
    "    augment=0.00025, # let just overfit first,\n",
    "    trading_cost=0, #0.0025, # let just overfit first,\n",
    "    window_length = params.window_length,   \n",
    ")\n",
    "env.seed(params.seed)\n",
    "env.reset().shape\n",
    "\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n",
    "env_test = PermutedPortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=128, \n",
    "    scale=True, \n",
    "    augment=0.00025, # let just overfit first,\n",
    "    trading_cost=0, #0.0025, # let just overfit first,\n",
    "    window_length = params.window_length,   \n",
    ")\n",
    "env_test.seed(params.seed)\n",
    "env_test.reset().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T01:11:38.199434Z",
     "start_time": "2017-08-02T09:11:38.155811+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:51.493374Z",
     "start_time": "2017-08-05T08:02:51.405559Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class EIIE_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN implementation of Ensemble of Identical Independent Evaluators (EIIE).\n",
    "    \n",
    "    https://arxiv.org/abs/1706.10059\n",
    "    \"\"\"\n",
    "#     def __init__(self, price_his_len, nb_actions):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs=inputs\n",
    "        self.outputs=outputs\n",
    "        super(EIIE_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 2, (1, 3))\n",
    "        self.conv2 = nn.Conv2d(2, 20, (1, inputs[1] - 2))\n",
    "        self.conv3 = nn.Conv2d(20, 1, (1, 1))        \n",
    "        self.head  = nn.Linear(outputs[0]-1,outputs[0]) # add cash bias?\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.zeros(num_outputs))\n",
    "        \n",
    "        self.v = nn.Linear(outputs[0]-1,1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # actor\n",
    "        x = F.relu(self.conv1(inputs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        h = x.view(x.size(0),self.outputs[0]-1) # Flatten\n",
    "        \n",
    "        mu = F.softmax(self.head(h)) # action\n",
    "        log_std = torch.exp(self.log_std).unsqueeze(0).expand_as(mu) # exploration param?\n",
    "        v = self.v(h) # critic\n",
    "        \n",
    "        \n",
    "        # critic\n",
    "        return mu, log_std, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:51.608129Z",
     "start_time": "2017-08-05T08:02:51.495261Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GenericSharedModel(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(GenericSharedModel, self).__init__()\n",
    "        num_inputs = int(np.prod(env.observation_space.shape))\n",
    "        num_outputs = int(np.prod(env.action_space.shape))\n",
    "        \n",
    "        # hidden layer sizes\n",
    "        h_size_1 = 128\n",
    "        h_size_2 = 128\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_inputs, h_size_1)\n",
    "        self.fc2 = nn.Linear(h_size_1, h_size_2)\n",
    "        self.mu = nn.Linear(h_size_2, num_outputs)\n",
    "        self.log_std = nn.Parameter(torch.zeros(num_outputs))\n",
    "        self.v = nn.Linear(h_size_2,1)\n",
    "        \n",
    "        for name, p in self.named_parameters():\n",
    "            # init parameters\n",
    "            if 'bias' in name:\n",
    "                p.data.fill_(0)\n",
    "            '''\n",
    "            if 'mu.weight' in name:\n",
    "                p.data.normal_()\n",
    "                p.data /= torch.sum(p.data**2,0).expand_as(p.data)'''\n",
    "        \n",
    "        # mode\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # flatten\n",
    "        inputs = inputs.view((inputs.size()[0],-1))\n",
    "        \n",
    "        # actor\n",
    "        x = F.tanh(self.fc1(inputs))\n",
    "        h = F.tanh(self.fc2(x))\n",
    "        \n",
    "        # the action\n",
    "        mu = F.softmax(self.mu(h))\n",
    "        \n",
    "        # exploration multiplier\n",
    "        log_std = F.sigmoid(torch.exp(self.log_std).unsqueeze(0).expand_as(mu))\n",
    "        \n",
    "        # critic\n",
    "        v = self.v(h)\n",
    "        return mu, log_std, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:51.657902Z",
     "start_time": "2017-08-05T08:02:51.609628Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:51.723228Z",
     "start_time": "2017-08-05T08:02:51.659522Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# class Shared_grad_buffers():\n",
    "#     def __init__(self, model):\n",
    "#         self.grads = {}\n",
    "#         for name, p in model.named_parameters():\n",
    "#             self.grads[name+'_grad'] = torch.ones(p.size()).share_memory_()\n",
    "\n",
    "#     def add_gradient(self, model):\n",
    "#         for name, p in model.named_parameters():\n",
    "#             self.grads[name+'_grad'] += p.grad.data\n",
    "\n",
    "#     def reset(self):\n",
    "#         for name,grad in self.grads.items():\n",
    "#             self.grads[name].fill_(0)\n",
    "\n",
    "class Shared_obs_stats():\n",
    "    \"\"\"Like batchnorm for input data\"\"\"\n",
    "    def __init__(self, num_inputs):\n",
    "        self.n = torch.zeros(num_inputs).share_memory_()\n",
    "        self.mean = torch.zeros(num_inputs).share_memory_()\n",
    "        self.mean_diff = torch.zeros(num_inputs).share_memory_()\n",
    "        self.var = torch.zeros(num_inputs).share_memory_()\n",
    "\n",
    "    def observes(self, obs):\n",
    "        # observation mean var updates\n",
    "        x = obs.data.squeeze()\n",
    "        self.n += 1.\n",
    "        last_mean = self.mean.clone()\n",
    "        self.mean += (x-self.mean)/self.n\n",
    "        self.mean_diff += (x-last_mean)*(x-self.mean)\n",
    "        self.var = torch.clamp(self.mean_diff/self.n, min=1e-2)\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = Variable(self.mean.unsqueeze(0).expand_as(inputs))\n",
    "        obs_std = Variable(torch.sqrt(self.var).unsqueeze(0).expand_as(inputs))\n",
    "        return torch.clamp((inputs-obs_mean)/obs_std, -5., 5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:51.769363Z",
     "start_time": "2017-08-05T08:02:51.724740Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(x-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*np.pi).sqrt()\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:02:51.853005Z",
     "start_time": "2017-08-05T08:02:51.770791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EIIE_CNN (\n",
       "  (conv1): Conv2d(3, 2, kernel_size=(1, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(2, 20, kernel_size=(1, 48), stride=(1, 1))\n",
       "  (conv3): Conv2d(20, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (head): Linear (5 -> 6)\n",
       "  (v): Linear (5 -> 1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cuda = False\n",
    "torch.manual_seed(params.seed)\n",
    "work_dir = mkdir('exp', 'ppo')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "\n",
    "# env = gym.make(params.env_name)\n",
    "#env = wrappers.Monitor(env, monitor_dir, force=True)\n",
    "\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_outputs = env.action_space.shape[0]\n",
    "\n",
    "\n",
    "#initialize network and optimizer\n",
    "Model = GenericSharedModel\n",
    "Model = EIIE_CNN\n",
    "model = Model(env.observation_space.shape, env.action_space.shape)\n",
    "if cuda: model.cuda()\n",
    "\n",
    "# shared_obs_stats = Shared_obs_stats(num_inputs)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.lr)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-04T02:50:27.128726Z",
     "start_time": "2017-08-04T10:50:01.031402+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-05T08:04:12.080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=15, loss=9.719e-06, cash_bias=0.1576, market_value=1.023, portfolio_value=1.036, reward=-5.649e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isisilon/.pyenv/versions/3.6.0/envs/jupyter3/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type EIIE_CNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=31, loss=0.0001381, cash_bias=0.1966, market_value=1.042, portfolio_value=1.052, reward=7.133e-06\n",
      "episode=47, loss=0.0002781, cash_bias=0.1989, market_value=1.042, portfolio_value=1.045, reward=-1.019e-06\n",
      "episode=63, loss=0.0001417, cash_bias=0.2253, market_value=1.051, portfolio_value=1.049, reward=6.361e-06\n",
      "episode=79, loss=5.829e-05, cash_bias=0.1883, market_value=1.029, portfolio_value=1.033, reward=2.362e-05\n",
      "episode=95, loss=3.899e-05, cash_bias=0.1925, market_value=1.051, portfolio_value=1.069, reward=3.269e-06\n",
      "episode=111, loss=4.012e-05, cash_bias=0.1828, market_value=1.059, portfolio_value=1.059, reward=2.533e-06\n",
      "episode=127, loss=4.866e-05, cash_bias=0.3374, market_value=1.016, portfolio_value=1.034, reward=5.887e-06\n",
      "episode=143, loss=5.706e-05, cash_bias=0.2415, market_value=1.044, portfolio_value=1.03, reward=1.553e-05\n",
      "episode=159, loss=4.973e-05, cash_bias=0.1724, market_value=1.036, portfolio_value=1.042, reward=1.092e-05\n",
      "episode=175, loss=4.329e-05, cash_bias=0.2812, market_value=1.03, portfolio_value=1.008, reward=-1.396e-05\n",
      "episode=191, loss=3.363e-05, cash_bias=0.269, market_value=1.022, portfolio_value=0.9982, reward=-7.393e-06\n",
      "episode=207, loss=2.331e-05, cash_bias=0.2075, market_value=1.042, portfolio_value=1.033, reward=1.259e-05\n",
      "episode=223, loss=1.733e-05, cash_bias=0.1812, market_value=1.068, portfolio_value=1.062, reward=1.421e-05\n",
      "episode=239, loss=1.367e-05, cash_bias=0.1185, market_value=1.033, portfolio_value=1.07, reward=4.348e-05\n",
      "episode=255, loss=9.83e-06, cash_bias=0.1196, market_value=1.043, portfolio_value=1.034, reward=5.893e-06\n",
      "episode=271, loss=8.681e-06, cash_bias=0.1785, market_value=1.027, portfolio_value=1.013, reward=1.019e-05\n",
      "episode=287, loss=9.708e-06, cash_bias=0.2493, market_value=1.02, portfolio_value=1.03, reward=1.493e-05\n",
      "episode=303, loss=6.095e-06, cash_bias=0.1618, market_value=1.045, portfolio_value=1.061, reward=1.379e-05\n",
      "episode=319, loss=3.4e-06, cash_bias=0.2196, market_value=1.051, portfolio_value=1.06, reward=1.304e-05\n",
      "episode=335, loss=2.664e-06, cash_bias=0.1819, market_value=1.034, portfolio_value=1.04, reward=-8.854e-06\n",
      "episode=351, loss=2.45e-06, cash_bias=0.1681, market_value=1.013, portfolio_value=0.9938, reward=1.268e-05\n",
      "episode=367, loss=2.298e-06, cash_bias=0.1588, market_value=1.039, portfolio_value=1.045, reward=1.856e-05\n",
      "episode=383, loss=2.516e-06, cash_bias=0.08414, market_value=1.024, portfolio_value=1.05, reward=-4.661e-06\n",
      "episode=399, loss=3.538e-06, cash_bias=0.1423, market_value=1.014, portfolio_value=1.045, reward=1.982e-05\n",
      "episode=415, loss=3.943e-06, cash_bias=0.1699, market_value=1.031, portfolio_value=1.012, reward=2.443e-05\n",
      "episode=431, loss=5.225e-06, cash_bias=0.1844, market_value=1.013, portfolio_value=1.019, reward=-7.907e-06\n",
      "episode=447, loss=7.845e-06, cash_bias=0.1174, market_value=1.042, portfolio_value=1.041, reward=-7.996e-07\n",
      "episode=463, loss=1.051e-05, cash_bias=0.09644, market_value=1.008, portfolio_value=1.001, reward=-4.708e-06\n",
      "episode=479, loss=1.385e-05, cash_bias=0.2407, market_value=1.025, portfolio_value=0.9878, reward=2.19e-05\n",
      "episode=495, loss=1.311e-05, cash_bias=0.09056, market_value=1.042, portfolio_value=1.05, reward=1.359e-05\n",
      "episode=511, loss=1.151e-05, cash_bias=0.1332, market_value=1.046, portfolio_value=1.031, reward=-1.609e-05\n",
      "episode=527, loss=9.768e-06, cash_bias=0.1144, market_value=1.038, portfolio_value=1.023, reward=-2.461e-06\n",
      "episode=543, loss=1.104e-05, cash_bias=0.2107, market_value=1.064, portfolio_value=1.028, reward=1.669e-05\n",
      "episode=559, loss=1.51e-05, cash_bias=0.1418, market_value=1.034, portfolio_value=1.043, reward=4.713e-06\n",
      "episode=575, loss=1.527e-05, cash_bias=0.09662, market_value=1.039, portfolio_value=1.022, reward=8.044e-06\n",
      "episode=591, loss=1.149e-05, cash_bias=0.2528, market_value=1.066, portfolio_value=1.081, reward=1.694e-05\n",
      "episode=607, loss=8.339e-06, cash_bias=0.1027, market_value=1.02, portfolio_value=1.01, reward=2.052e-05\n",
      "episode=623, loss=7.454e-06, cash_bias=0.3493, market_value=1.017, portfolio_value=1.032, reward=2.072e-05\n",
      "episode=639, loss=8.586e-06, cash_bias=0.2922, market_value=1.037, portfolio_value=1.018, reward=2.574e-05\n",
      "episode=655, loss=7.801e-06, cash_bias=0.1131, market_value=1.038, portfolio_value=1.052, reward=2.444e-06\n",
      "episode=671, loss=8.807e-06, cash_bias=0.1068, market_value=1.051, portfolio_value=1.033, reward=-1.114e-05\n",
      "episode=687, loss=7.608e-06, cash_bias=0.1793, market_value=1.007, portfolio_value=0.9866, reward=-6.258e-06\n",
      "episode=703, loss=5.228e-06, cash_bias=0.1771, market_value=1.021, portfolio_value=0.9835, reward=-1.553e-05\n",
      "episode=719, loss=4.687e-06, cash_bias=0.04639, market_value=1.034, portfolio_value=1.035, reward=-8.806e-06\n",
      "episode=735, loss=2.862e-06, cash_bias=0.1449, market_value=1.049, portfolio_value=1.085, reward=3.02e-05\n",
      "episode=751, loss=1.168e-06, cash_bias=0.1572, market_value=1.039, portfolio_value=1.054, reward=8.411e-06\n",
      "episode=767, loss=8.27e-07, cash_bias=0.1818, market_value=1.059, portfolio_value=1.053, reward=5.634e-06\n",
      "episode=783, loss=7.095e-07, cash_bias=0.2748, market_value=1.066, portfolio_value=1.068, reward=-3.955e-06\n",
      "episode=799, loss=6.994e-07, cash_bias=0.1681, market_value=1.044, portfolio_value=1.042, reward=1.805e-05\n",
      "episode=815, loss=4.586e-07, cash_bias=0.2524, market_value=1.048, portfolio_value=1.041, reward=-8.016e-06\n",
      "episode=831, loss=2.019e-07, cash_bias=0.1101, market_value=1.054, portfolio_value=1.086, reward=-3.308e-06\n",
      "episode=847, loss=2.292e-07, cash_bias=0.1575, market_value=1.024, portfolio_value=1.023, reward=-2.917e-06\n",
      "episode=863, loss=3.291e-07, cash_bias=0.1384, market_value=1.033, portfolio_value=1.041, reward=-8.712e-06\n",
      "episode=879, loss=3.559e-07, cash_bias=0.25, market_value=1.047, portfolio_value=1.048, reward=-8.839e-06\n",
      "episode=895, loss=3.101e-07, cash_bias=0.1359, market_value=1.052, portfolio_value=1.027, reward=-6.901e-06\n",
      "episode=911, loss=3.64e-07, cash_bias=0.1054, market_value=1.041, portfolio_value=1.039, reward=-3.593e-06\n",
      "episode=927, loss=4.17e-07, cash_bias=0.1341, market_value=1.038, portfolio_value=1.037, reward=-5.346e-06\n",
      "episode=943, loss=4.986e-07, cash_bias=0.1028, market_value=1.018, portfolio_value=1.037, reward=1.352e-05\n",
      "episode=959, loss=5.584e-07, cash_bias=0.1695, market_value=1.044, portfolio_value=1.045, reward=1.394e-05\n",
      "episode=975, loss=6.081e-07, cash_bias=0.1929, market_value=1.037, portfolio_value=1.015, reward=-6.042e-06\n",
      "episode=991, loss=7.257e-07, cash_bias=0.1238, market_value=1.04, portfolio_value=1.036, reward=6.518e-06\n",
      "episode=1007, loss=1.008e-06, cash_bias=0.09522, market_value=1.034, portfolio_value=1.033, reward=-1.205e-05\n",
      "episode=1023, loss=1.299e-06, cash_bias=0.1065, market_value=1.021, portfolio_value=0.9992, reward=-7.847e-06\n"
     ]
    }
   ],
   "source": [
    "memory = ReplayMemory(params.num_steps)\n",
    "# memory = PrioritisedReplayMemory(params.num_steps)\n",
    "\n",
    "num_inputs = int(np.prod(env.observation_space.shape))\n",
    "num_outputs = int(np.prod(env.action_space.shape))\n",
    "\n",
    "state = env.reset()\n",
    "state = Variable(torch.Tensor(state).unsqueeze(0))\n",
    "done = True\n",
    "episode_length = 0\n",
    "reports = []\n",
    "\n",
    "while True:\n",
    "#     episode_length += 1\n",
    "    episode = -1\n",
    "    \n",
    "    # horizon loop\n",
    "    for t in range(params.time_horizon):\n",
    "        infos = []\n",
    "        episode_length = 0\n",
    "        # Sample data from the policy\n",
    "        while (len(memory.memory) < params.num_steps):\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            values = []\n",
    "            returns = []\n",
    "            advantages = []\n",
    "            av_reward = 0\n",
    "            cum_reward = 0\n",
    "            cum_done = 0\n",
    "            # n steps loops\n",
    "            for step in range(params.num_steps):\n",
    "                #                 shared_obs_stats.observes(state)\n",
    "                #                 state = shared_obs_stats.normalize(state)\n",
    "                states.append(state)\n",
    "                \n",
    "                mu, sigma_sq, v = model(state)\n",
    "                eps = torch.randn(mu.size())\n",
    "                action = (mu + sigma_sq.sqrt() * Variable(eps))\n",
    "                env_action = action.data.squeeze().numpy()\n",
    "                state, reward, done, info = env.step(env_action)\n",
    "                done = (done or episode_length >= params.max_episode_length)\n",
    "                \n",
    "                cum_reward += reward\n",
    "                reward = max(min(reward, 1), -1)\n",
    "                rewards.append(reward)\n",
    "                actions.append(action)\n",
    "                values.append(v)\n",
    "                \n",
    "                if done:\n",
    "                    episode += 1\n",
    "                    cum_done += 1\n",
    "                    av_reward += cum_reward\n",
    "                    cum_reward = 0\n",
    "                    episode_length = 0\n",
    "                    infos.append(info)\n",
    "                    state = env.reset()\n",
    "                \n",
    "                state = Variable(torch.Tensor(state).unsqueeze(0))\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # one last step\n",
    "            R = torch.zeros(1, 1)\n",
    "            if not done:\n",
    "                _, _, v = model(state)\n",
    "                R = v.data\n",
    "            \n",
    "            # compute returns and GAE(lambda) advantages:\n",
    "            values.append(Variable(R))\n",
    "            R = Variable(R)\n",
    "            A = Variable(torch.zeros(1, 1))\n",
    "            for i in reversed(range(len(rewards))):\n",
    "                td = rewards[i] + params.gamma*values[i+1].data[0,0] - values[i].data[0,0]\n",
    "                A = float(td) + params.gamma * params.gae_param * A\n",
    "                advantages.insert(0, A)\n",
    "                R = A + values[i]\n",
    "                returns.insert(0, R)\n",
    "            \n",
    "            # store useful info:\n",
    "            memory.push([states, actions, returns, advantages])\n",
    "\n",
    "        # perform several epochs of optimization on the sampled data\n",
    "        model_old = Model(env.observation_space.shape,\n",
    "                             env.action_space.shape)\n",
    "        model_old.load_state_dict(model.state_dict())\n",
    "        av_loss = 0\n",
    "        for k in range(params.num_epoch):\n",
    "            # cf https://github.com/openai/baselines/blob/master/baselines/pposgd/pposgd_simple.py\n",
    "            batch_states, batch_actions, batch_returns, batch_advantages = memory.sample(\n",
    "                params.batch_size)\n",
    "            \n",
    "            # old probas\n",
    "            mu_old, sigma_sq_old, v_pred_old = model_old(batch_states.detach())\n",
    "            probs_old = normal(batch_actions, mu_old, sigma_sq_old)\n",
    "            \n",
    "            # new probas\n",
    "            mu, sigma_sq, v_pred = model(batch_states)\n",
    "            probs = normal(batch_actions, mu, sigma_sq)\n",
    "            \n",
    "            # ratio\n",
    "            ratio = probs / (1e-15 + probs_old)\n",
    "            \n",
    "            # surrogate clip loss\n",
    "            surr1 = ratio * torch.cat([batch_advantages]*num_outputs,1) # surrogate from conservative policy iteration\n",
    "            surr2 = ratio.clamp(1-params.clip, 1+params.clip) * torch.cat([batch_advantages]*num_outputs,1)\n",
    "            loss_clip = -torch.mean(torch.min(surr1, surr2))\n",
    "            # should this be a mean along axis 0?\n",
    "            \n",
    "            # state-value function loss, do we even need this if they don't share params?\n",
    "            vfloss1 = (v_pred - batch_returns)**2\n",
    "            v_pred_clipped = v_pred_old + (v_pred - v_pred_old).clamp(-params.clip, params.clip)\n",
    "            vfloss2 = (v_pred_clipped - batch_returns)**2\n",
    "            loss_value = 0.5 * torch.mean(torch.max(vfloss1, vfloss2))\n",
    "            # should this be a mean along axis 0?\n",
    "            \n",
    "            # loss on entropy bonus to ensure sufficient exploration\n",
    "            loss_ent = -params.ent_coeff*torch.mean(probs*torch.log(probs+1e-5))\n",
    "            \n",
    "            # total\n",
    "            total_loss = (loss_clip + loss_value + loss_ent)\n",
    "#             total_loss = (loss_clip - loss_value + loss_ent)\n",
    "            av_loss += loss_value.data[0] / float(params.num_epoch)\n",
    "            \n",
    "            # before step, update old_model:\n",
    "            model_old.load_state_dict(model.state_dict())\n",
    "            \n",
    "            # step\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward(retain_variables=True)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # t finish, print:\n",
    "        df_infos = pd.DataFrame(infos)\n",
    "        \n",
    "        # show stats?\n",
    "#         display(df_infos[[\"cash_bias\",\"return\",\"portfolio_value\",\"market_value\"]].describe().loc[[\"min\",\"mean\",\"max\"]])\n",
    "        \n",
    "        report=OrderedDict(\n",
    "            episode=episode,\n",
    "#             reward=av_reward / float(cum_done),\n",
    "            loss=av_loss,\n",
    "            cash_bias=df_infos.cash_bias.mean(),\n",
    "            market_value=df_infos.market_value.mean(),\n",
    "            portfolio_value=df_infos.portfolio_value.mean(),\n",
    "            reward=df_infos.reward.mean()\n",
    "        )\n",
    "        \n",
    "        s = ', '.join(['{}={:2.4g}'.format(key,value) for key,value in report.items()])\n",
    "        print(s)\n",
    "        \n",
    "        reports.append(report)\n",
    "    \n",
    "        memory.clear()\n",
    "        torch.save(model_old, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show progress\n",
    "df=pd.DataFrame(reports)\n",
    "g = sns.jointplot(x=\"episode\", y=\"loss\", data=df, kind=\"reg\", size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T07:57:14.883453Z",
     "start_time": "2017-08-05T07:57:14.841057Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:03:17.462371Z",
     "start_time": "2017-08-05T08:02:49.855Z"
    }
   },
   "outputs": [],
   "source": [
    "# env_test = env\n",
    "\n",
    "# Test\n",
    "model.train(False)\n",
    "state = env_test.reset()\n",
    "for i in range(250):\n",
    "    state = Variable(torch.Tensor(state).unsqueeze(0))\n",
    "    mu, sigma_sq, v = model(state)\n",
    "    eps = torch.randn(mu.size())\n",
    "    action = (mu + sigma_sq.sqrt() * Variable(eps))\n",
    "    env_action = action.data.squeeze().numpy()\n",
    "    state, reward, done, info = env_test.step(env_action)\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T08:03:17.463005Z",
     "start_time": "2017-08-05T08:02:49.857Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T06:59:47.486363Z",
     "start_time": "2017-08-05T06:59:47.404265Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-05T06:55:09.249081Z",
     "start_time": "2017-08-05T06:55:09.199392Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
