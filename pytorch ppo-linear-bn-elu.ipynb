{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is easier to debug, I like it.\n",
    "\n",
    "TODO:\n",
    "- [x] prioritised experience replay, need to grab loss for each sample, and sample based on loss\n",
    "- [ ] check it for my data, can it overfit?, does the normalisation make sense?\n",
    "- [x] better metrics\n",
    "- [ ] do cnn model\n",
    "- [x] read papers\n",
    "- [ ] check i'm prioristising by the right things, should lead to lowest loss\n",
    "- [ ] test on cartpole\n",
    "\n",
    "Refs: \n",
    "- implementations:\n",
    "    - PPO\n",
    "        - **pytorch implementation https://github.com/alexis-jacq/Pytorch-DPPO/blob/master/ppo.py**\n",
    "        - tensorflow implementation https://github.com/reinforceio/tensorforce/blob/master/tensorforce/models/ppo_model.py\n",
    "    - Prioritised memory\n",
    "    - Other\n",
    "        - http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training\n",
    "        - https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py\n",
    "- papers:\n",
    "    - DPPO https://arxiv.org/pdf/1707.02286.pdf\n",
    "    - PPO \n",
    "        - https://arxiv.org/abs/1707.06347\n",
    "        - https://blog.openai.com/openai-baselines-ppo/\n",
    "    - TRPO https://arxiv.org/abs/1502.05477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:03.079244Z",
     "start_time": "2017-08-06T07:15:02.444122Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:__main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# numeric\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display\n",
    "from pprint import pprint\n",
    "\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:03.300383Z",
     "start_time": "2017-08-06T07:15:03.081000Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:03.325846Z",
     "start_time": "2017-08-06T07:15:03.302056Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T06:41:13.661746Z",
     "start_time": "2017-08-06T06:41:13.627681Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:03.369096Z",
     "start_time": "2017-08-06T07:15:03.327470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/agent_portfolio-ddpo/2017-07-21_seperate_weights.pickle'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        # env\n",
    "        self.window_length = 50\n",
    "        # Model\n",
    "        self.batch_size = 250\n",
    "        self.lr = 3e-4\n",
    "        self.gamma = 0.00\n",
    "        self.gae_param = 0.95\n",
    "        self.clip = 0.2 # epsilon from eq 7, default 0.2\n",
    "        self.ent_coeff = 0.\n",
    "        self.num_epoch = 50\n",
    "        self.num_steps = 2048*4\n",
    "        self.time_horizon = 2000000\n",
    "        self.max_episode_length = 10000\n",
    "        self.seed = 1\n",
    "\n",
    "params = Params()\n",
    "\n",
    "save_path= 'outputs/agent_portfolio-ddpo/{}_seperate_weights.pickle'.format('2017-07-21')\n",
    "try:\n",
    "    os.makedirs(os.path.dirname(save_path))\n",
    "except OSError:\n",
    "    pass\n",
    "save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "refs\n",
    "- https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "- https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:03.401745Z",
     "start_time": "2017-08-06T07:15:03.370854Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, events):\n",
    "        for event in zip(*events):\n",
    "            self.memory.append(event)\n",
    "            if len(self.memory)>self.capacity:\n",
    "                del self.memory[0]\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        return map(lambda x: torch.cat(x, 0), samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T00:55:29.885772Z",
     "start_time": "2017-08-02T08:55:29.883459+08:00"
    }
   },
   "source": [
    "# Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:03.659450Z",
     "start_time": "2017-08-06T07:15:03.403233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 50)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.environments.portfolio import PortfolioEnv, sharpe, max_drawdown\n",
    "\n",
    "# we want to pemute the channels a little\n",
    "\n",
    "class PermutedPortfolioEnv(PortfolioEnv):\n",
    "    def reset(self, *args, **kwargs):\n",
    "        return np.transpose(super().reset(*args, **kwargs),(0,1,2))\n",
    "    def step(self, *args, **kwargs):\n",
    "        observation, reward, done, info = super().step(*args, **kwargs)\n",
    "        observation = np.transpose(observation,(2,0,1))\n",
    "        return observation, reward, done, info\n",
    "\n",
    "\n",
    "df_train = pd.read_hdf('./data/poloniex_30m.hf',key='train')\n",
    "env = PermutedPortfolioEnv(\n",
    "    df=df_train,\n",
    "    steps=128, \n",
    "    scale=True, \n",
    "    augment=0.0025, # let just overfit first,\n",
    "    trading_cost=0, #0.0025, # let just overfit first,\n",
    "    window_length = params.window_length,   \n",
    ")\n",
    "env.seed(params.seed)\n",
    "env.reset().shape\n",
    "\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n",
    "env_test = PermutedPortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=1280, \n",
    "    scale=True, \n",
    "    trading_cost=0, #0.0025, # let just overfit first,\n",
    "    window_length = params.window_length,   \n",
    ")\n",
    "env_test.seed(params.seed)\n",
    "env_test.reset().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:03.703296Z",
     "start_time": "2017-08-06T07:15:03.661087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape\n",
    "# 20*50-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:03.919796Z",
     "start_time": "2017-08-06T07:15:03.705060Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.init\n",
    "\n",
    "class GenericSharedModel(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(GenericSharedModel, self).__init__()\n",
    "        num_inputs = int(np.prod(env.observation_space.shape))\n",
    "        num_outputs = int(np.prod(env.action_space.shape))\n",
    "        \n",
    "        # hidden layer sizes\n",
    "        h_size_1 = 100\n",
    "        h_size_2 = 64\n",
    "        \n",
    "        # shared conv block\n",
    "        self.conv1 = nn.Conv2d(3, 2, (1, 3))\n",
    "        self.bn_conv1   = nn.BatchNorm2d(2)\n",
    "        self.conv2 = nn.Conv2d(2, 20, (1, inputs[1] - 2))\n",
    "        self.bn_conv2   = nn.BatchNorm2d(20)\n",
    "        \n",
    "        # Actor mean\n",
    "        self.fc1 = nn.Linear(20*inputs[0], h_size_1)\n",
    "        self.bn_fc1   = nn.BatchNorm1d(h_size_1)\n",
    "        self.fc2 = nn.Linear(h_size_1, h_size_2)\n",
    "        self.bn_fc2   = nn.BatchNorm1d(h_size_2)     \n",
    "        self.mu = nn.Linear(h_size_2, num_outputs)\n",
    "        \n",
    "        # Actor std\n",
    "        self.log_std = nn.Parameter(torch.zeros(num_outputs))\n",
    "        \n",
    "        # Critic\n",
    "        self.fc1b = nn.Linear(20*inputs[0], h_size_1)\n",
    "        self.bn_fcb1   = nn.BatchNorm1d(h_size_1)\n",
    "        self.fc2b = nn.Linear(h_size_1, h_size_2)\n",
    "        self.bn_fcb2   = nn.BatchNorm1d(h_size_2)        \n",
    "        self.v = nn.Linear(h_size_2,1)\n",
    "        \n",
    "        for name, p in self.named_parameters():\n",
    "            # init parameters like in keras\n",
    "            if 'bias' in name:\n",
    "                p.data.fill_(0)\n",
    "            if ('weight' in name) and ('conv' in name):\n",
    "                if len(p.size())>1:\n",
    "                    torch.nn.init.xavier_uniform(p)\n",
    "                else:\n",
    "                    pass # leave as uniform\n",
    "        \n",
    "        # mode\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # shared conv block\n",
    "        x = F.elu(self.bn_conv1(self.conv1(inputs)))\n",
    "        x = F.elu(self.bn_conv2(self.conv2(x)))\n",
    "        # flatten\n",
    "        h = x.view(x.size(0),-1)\n",
    "        \n",
    "        # the action mean\n",
    "        x = F.elu(self.bn_fc1(self.fc1(h)))\n",
    "        x = F.elu(self.bn_fc2(self.fc2(x)))       \n",
    "        mu = self.mu(x)\n",
    "        \n",
    "        # the log standard debian of the action\n",
    "        log_std = torch.exp(self.log_std).unsqueeze(0).expand_as(mu)\n",
    "        \n",
    "        # critic\n",
    "        x = F.elu(self.bn_fcb1(self.fc1b(h)))\n",
    "        x = F.elu(self.bn_fcb2(self.fc2b(x)))\n",
    "        v = self.v(x)\n",
    "        return mu, log_std, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T03:41:34.447139Z",
     "start_time": "2017-08-06T03:41:34.417249Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T03:41:22.398392Z",
     "start_time": "2017-08-06T03:41:22.364246Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:03.952805Z",
     "start_time": "2017-08-06T07:15:03.921663Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:03.997538Z",
     "start_time": "2017-08-06T07:15:03.954528Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Shared_obs_stats():\n",
    "    \"\"\"Like batchnorm for input data\"\"\"\n",
    "    def __init__(self, num_inputs):\n",
    "        self.n = torch.zeros(num_inputs).share_memory_()\n",
    "        self.mean = torch.zeros(num_inputs).share_memory_()\n",
    "        self.mean_diff = torch.zeros(num_inputs).share_memory_()\n",
    "        self.var = torch.zeros(num_inputs).share_memory_()\n",
    "\n",
    "    def observes(self, obs):\n",
    "        # observation mean var updates\n",
    "        x = obs.data.squeeze()\n",
    "        self.n += 1.\n",
    "        last_mean = self.mean.clone()\n",
    "        self.mean += (x-self.mean)/self.n\n",
    "        self.mean_diff += (x-last_mean)*(x-self.mean)\n",
    "        self.var = torch.clamp(self.mean_diff/self.n, min=1e-2)\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = Variable(self.mean.unsqueeze(0).expand_as(inputs))\n",
    "        obs_std = Variable(torch.sqrt(self.var).unsqueeze(0).expand_as(inputs))\n",
    "        return torch.clamp((inputs-obs_mean)/obs_std, -5., 5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:04.040170Z",
     "start_time": "2017-08-06T07:15:03.999178Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(x-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*np.pi).sqrt()\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:04.078583Z",
     "start_time": "2017-08-06T07:15:04.041842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenericSharedModel (\n",
       "  (conv1): Conv2d(3, 2, kernel_size=(1, 3), stride=(1, 1))\n",
       "  (bn_conv1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv2): Conv2d(2, 20, kernel_size=(1, 48), stride=(1, 1))\n",
       "  (bn_conv2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc1): Linear (100 -> 100)\n",
       "  (bn_fc1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc2): Linear (100 -> 64)\n",
       "  (bn_fc2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (mu): Linear (64 -> 6)\n",
       "  (fc1b): Linear (100 -> 100)\n",
       "  (bn_fcb1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc2b): Linear (100 -> 64)\n",
       "  (bn_fcb2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (v): Linear (64 -> 1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cuda = False\n",
    "torch.manual_seed(params.seed)\n",
    "work_dir = mkdir('exp', 'ppo')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "\n",
    "# env = gym.make(params.env_name)\n",
    "#env = wrappers.Monitor(env, monitor_dir, force=True)\n",
    "\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_outputs = env.action_space.shape[0]\n",
    "\n",
    "\n",
    "#initialize network and optimizer\n",
    "Model = GenericSharedModel\n",
    "model = Model(env.observation_space.shape, env.action_space.shape)\n",
    "if cuda: model.cuda()\n",
    "\n",
    "# shared_obs_stats = Shared_obs_stats(num_inputs)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.lr)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T07:15:04.138104Z",
     "start_time": "2017-08-06T07:15:04.080202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenericSharedModel (\n",
      "  conv1           [     20]: Conv2d(3, 2, kernel_size=(1, 3), stride=(1, 1))\n",
      "  bn_conv1        [      4]: BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True)\n",
      "  conv2           [   1940]: Conv2d(2, 20, kernel_size=(1, 48), stride=(1, 1))\n",
      "  bn_conv2        [     40]: BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)\n",
      "  fc1             [  10100]: Linear (100 -> 100)\n",
      "  bn_fc1          [    200]: BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True)\n",
      "  fc2             [   6464]: Linear (100 -> 64)\n",
      "  bn_fc2          [    128]: BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  mu              [    390]: Linear (64 -> 6)\n",
      "  fc1b            [  10100]: Linear (100 -> 100)\n",
      "  bn_fcb1         [    200]: BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True)\n",
      "  fc2b            [   6464]: Linear (100 -> 64)\n",
      "  bn_fcb2         [    128]: BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  v               [     65]: Linear (64 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.modules.module import _addindent\n",
    "import torch\n",
    "import numpy as np\n",
    "def torch_summarize(model, show_weights=False, show_parameters=True):\n",
    "    \"\"\"Summarizes torch model by showing trainable parameters and weights\"\"\"\n",
    "    tmpstr = model.__class__.__name__ + ' (\\n'\n",
    "    for key, module in model._modules.items():\n",
    "        # if it contains layers let call it recurvisvly to get params and weights\n",
    "        if type(module) in [\n",
    "            torch.nn.modules.container.Container,\n",
    "            torch.nn.modules.container.Sequential\n",
    "        ]:\n",
    "            modstr = torch_summarize(module)\n",
    "        else:\n",
    "            modstr = module.__repr__()\n",
    "        modstr = _addindent(modstr, 2)\n",
    "        \n",
    "        params = sum([np.prod(p.size()) for p in module.parameters()])\n",
    "        weights = tuple([tuple(p.size()) for p in module.parameters()])\n",
    "        \n",
    "        tmpstr += '  {:15.15} '.format(key) + '[{: 7.7g}]: '.format(params) + modstr \n",
    "        if show_weights:\n",
    "            tmpstr += ', weights={}'.format(weights)\n",
    "        tmpstr += '\\n'   \n",
    "\n",
    "    tmpstr = tmpstr + ')'\n",
    "    return tmpstr\n",
    "\n",
    "print(torch_summarize(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-06T07:15:06.348Z"
    }
   },
   "outputs": [],
   "source": [
    "memory = ReplayMemory(params.num_steps)\n",
    "# memory = PrioritisedReplayMemory(params.num_steps)\n",
    "\n",
    "num_inputs = int(np.prod(env.observation_space.shape))\n",
    "num_outputs = int(np.prod(env.action_space.shape))\n",
    "\n",
    "state = env.reset()\n",
    "state = Variable(torch.Tensor(state).unsqueeze(0))\n",
    "done = True\n",
    "episode_length = 0\n",
    "reports = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-06T07:15:06.352Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108950214b744228a5fb384f1bdddf12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=63, loss=0.004864, market_value=1.048, cash_bias=0.1248, reward=-3.866e-06, portfolio_value=1.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wassname/.pyenv/versions/3.5.3/envs/jupyter3/lib/python3.5/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type GenericSharedModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=127, loss=0.00192, market_value=1.038, cash_bias=0.1695, reward=2.815e-06, portfolio_value=1.041\n",
      "16384/|/av_reward= 0.00000326  1%|| 16384/2000000 [02:30<4:03:17, 135.89steps/s]episode=191, loss=0.002165, market_value=1.028, cash_bias=0.2029, reward=7.439e-06, portfolio_value=1.037\n",
      "episode=255, loss=0.002224, market_value=1.034, cash_bias=0.2108, reward=4.079e-06, portfolio_value=1.052\n",
      "40960/|/av_reward= 0.00057724  2%|| 40960/2000000 [06:20<4:13:53, 128.60steps/s]episode=319, loss=0.002281, market_value=1.042, cash_bias=0.2069, reward=2.316e-06, portfolio_value=1.032\n",
      "49152/|/av_reward=-0.00002268  2%|| 49152/2000000 [07:40<3:59:28, 135.77steps/s]episode=383, loss=0.002313, market_value=1.054, cash_bias=0.2293, reward=6.876e-06, portfolio_value=1.047\n",
      "episode=447, loss=0.003132, market_value=1.04, cash_bias=0.1413, reward=1.032e-05, portfolio_value=1.027\n",
      "65536/|/av_reward=-0.00140124  3%|| 65536/2000000 [10:10<4:00:17, 134.17steps/s]episode=511, loss=0.00504, market_value=1.041, cash_bias=0.165, reward=2.664e-07, portfolio_value=1.015\n",
      "episode=575, loss=0.01815, market_value=1.029, cash_bias=0.2184, reward=5.11e-06, portfolio_value=1.027\n",
      "81920/|/av_reward= 0.00094801  4%|| 81920/2000000 [12:40<3:56:19, 135.27steps/s]episode=639, loss=0.02324, market_value=1.032, cash_bias=0.1785, reward=-1.139e-05, portfolio_value=1.023\n",
      "90112/|/av_reward= 0.00013254  5%|| 90112/2000000 [14:20<5:48:07, 91.44steps/s]episode=703, loss=0.03878, market_value=1.039, cash_bias=0.2411, reward=2.13e-06, portfolio_value=1.044\n",
      "98304/|/av_reward= 0.00021603  5%|| 98304/2000000 [16:01<5:02:10, 104.89steps/s]episode=767, loss=0.07314, market_value=1.021, cash_bias=0.1886, reward=1.007e-05, portfolio_value=1.024\n",
      "106496/|/av_reward=-0.00005326  5%|| 106496/2000000 [17:41<5:23:23, 97.58steps/s]episode=831, loss=0.1157, market_value=1.022, cash_bias=0.2444, reward=-6.468e-06, portfolio_value=1.033\n",
      "114688/|/av_reward=-0.00043878  6%|| 114688/2000000 [19:01<3:56:44, 132.73steps/s]episode=895, loss=0.2048, market_value=1.045, cash_bias=0.2167, reward=1.045e-05, portfolio_value=1.049\n",
      "122880/|/av_reward= 0.00089497  6%|| 122880/2000000 [20:21<3:54:27, 133.44steps/s]episode=959, loss=0.3454, market_value=1.06, cash_bias=0.2495, reward=-5.462e-06, portfolio_value=1.063\n",
      "episode=1023, loss=0.4507, market_value=1.044, cash_bias=0.2098, reward=3.602e-06, portfolio_value=1.054\n",
      "139264/|/av_reward=-0.00036039  7%|| 139264/2000000 [22:51<3:52:04, 133.63steps/s]episode=1087, loss=0.5545, market_value=1.036, cash_bias=0.2722, reward=-3.685e-06, portfolio_value=1.04\n",
      "episode=1151, loss=0.5516, market_value=1.029, cash_bias=0.1674, reward=1.566e-07, portfolio_value=1.028\n",
      "147456/|/av_reward= 0.00095968  7%|| 147456/2000000 [24:11<3:48:54, 134.88steps/s]episode=1215, loss=0.5942, market_value=1.023, cash_bias=0.1906, reward=-8.688e-06, portfolio_value=1.005\n",
      "episode=1279, loss=0.6513, market_value=1.021, cash_bias=0.1861, reward=2.197e-05, portfolio_value=1.032\n",
      "episode=1343, loss=0.6651, market_value=1.024, cash_bias=0.1387, reward=-1.033e-06, portfolio_value=1.02\n",
      "180224/|/av_reward= 0.00111041  9%|| 180224/2000000 [29:32<3:54:44, 129.20steps/s]episode=1407, loss=0.899, market_value=1.034, cash_bias=0.134, reward=-2.297e-06, portfolio_value=1.04\n",
      "188416/|/av_reward= 0.00009518  9%|| 188416/2000000 [30:52<3:43:05, 135.34steps/s]episode=1471, loss=0.8522, market_value=1.03, cash_bias=0.2707, reward=3.024e-06, portfolio_value=1.034\n",
      "episode=1535, loss=0.8296, market_value=1.022, cash_bias=0.2185, reward=-1.355e-07, portfolio_value=1.013\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with tqdm(total=params.time_horizon, mininterval=2, unit='steps') as p:\n",
    "    episode = -1    \n",
    "    steps = 0\n",
    "    # horizon loop\n",
    "    while steps < params.time_horizon:\n",
    "        infos = []\n",
    "        episode_length = 0\n",
    "        # Sample data from the policy\n",
    "        while (len(memory.memory) < params.num_steps):\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            values = []\n",
    "            returns = []\n",
    "            advantages = []\n",
    "            av_reward = 0\n",
    "            cum_reward = 0\n",
    "            cum_done = 0\n",
    "            # n steps loops\n",
    "            for step in range(params.num_steps):\n",
    "                #                 shared_obs_stats.observes(state)\n",
    "                #                 state = shared_obs_stats.normalize(state)\n",
    "                states.append(state)\n",
    "                \n",
    "                mu, sigma_sq, v = model(state)\n",
    "                eps = torch.randn(mu.size())\n",
    "                action = (mu + sigma_sq.sqrt() * Variable(eps))\n",
    "                env_action = action.data.squeeze().numpy()\n",
    "                state, reward, done, info = env.step(env_action)\n",
    "                done = (done or episode_length >= params.max_episode_length)\n",
    "                \n",
    "                cum_reward += reward\n",
    "                reward = max(min(reward, 1), -1)\n",
    "                rewards.append(reward)\n",
    "                actions.append(action)\n",
    "                values.append(v)\n",
    "                \n",
    "                steps+=1  \n",
    "                p.update(1)\n",
    "                if done:\n",
    "                    episode += 1\n",
    "                    cum_done += 1\n",
    "                    av_reward += cum_reward\n",
    "                    p.desc='av_reward={: 2.8f}'.format(av_reward / float(cum_done))\n",
    "                    cum_reward = 0\n",
    "                    episode_length = 0\n",
    "                    infos.append(info)\n",
    "                    state = env.reset()\n",
    "                \n",
    "                state = Variable(torch.Tensor(state).unsqueeze(0))\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # one last step\n",
    "            R = torch.zeros(1, 1)\n",
    "            if not done:\n",
    "                _, _, v = model(state)\n",
    "                R = v.data\n",
    "            \n",
    "            # compute returns and GAE(lambda) advantages:\n",
    "            values.append(Variable(R))\n",
    "            R = Variable(R)\n",
    "            A = Variable(torch.zeros(1, 1))\n",
    "            for i in reversed(range(len(rewards))):\n",
    "                td = rewards[i] + params.gamma*values[i+1].data[0,0] - values[i].data[0,0]\n",
    "                A = float(td) + params.gamma * params.gae_param * A\n",
    "                advantages.insert(0, A)\n",
    "                R = A + values[i]\n",
    "                returns.insert(0, R)\n",
    "            \n",
    "            # store useful info:\n",
    "            memory.push([states, actions, returns, advantages])\n",
    "            \n",
    "\n",
    "        # perform several epochs of optimization on the sampled data\n",
    "        model_old = Model(env.observation_space.shape,\n",
    "                             env.action_space.shape)\n",
    "        model_old.load_state_dict(model.state_dict())\n",
    "        if cuda: model_old.cuda()\n",
    "        av_loss = 0\n",
    "        for k in range(params.num_epoch):\n",
    "            # cf https://github.com/openai/baselines/blob/master/baselines/pposgd/pposgd_simple.py\n",
    "            batch_states, batch_actions, batch_returns, batch_advantages = memory.sample(\n",
    "                params.batch_size)\n",
    "            \n",
    "            # old probas\n",
    "            mu_old, sigma_sq_old, v_pred_old = model_old(batch_states.detach())\n",
    "            probs_old = normal(batch_actions, mu_old, sigma_sq_old)\n",
    "            \n",
    "            # new probas\n",
    "            mu, sigma_sq, v_pred = model(batch_states)\n",
    "            probs = normal(batch_actions, mu, sigma_sq)\n",
    "            \n",
    "            # ratio\n",
    "            ratio = probs / (1e-15 + probs_old)\n",
    "            \n",
    "            # surrogate clip loss\n",
    "            surr1 = ratio * torch.cat([batch_advantages]*num_outputs,1) # surrogate from conservative policy iteration\n",
    "            surr2 = ratio.clamp(1-params.clip, 1+params.clip) * torch.cat([batch_advantages]*num_outputs,1)\n",
    "            loss_clip = -torch.mean(torch.min(surr1, surr2))\n",
    "            # should this be a mean along axis 0?\n",
    "            \n",
    "            # state-value function loss, do we even need this if they don't share params?\n",
    "            vfloss1 = (v_pred - batch_returns)**2\n",
    "            v_pred_clipped = v_pred_old + (v_pred - v_pred_old).clamp(-params.clip, params.clip)\n",
    "            vfloss2 = (v_pred_clipped - batch_returns)**2\n",
    "            loss_value = 0.5 * torch.mean(torch.max(vfloss1, vfloss2))\n",
    "            # should this be a mean along axis 0?\n",
    "            \n",
    "            # loss on entropy bonus to ensure sufficient exploration\n",
    "            loss_ent = -params.ent_coeff*torch.mean(probs*torch.log(probs+1e-5))\n",
    "            \n",
    "            # total\n",
    "            total_loss = (loss_clip + loss_value + loss_ent)\n",
    "#             total_loss = (loss_clip - loss_value + loss_ent)\n",
    "            av_loss += loss_value.data[0] / float(params.num_epoch)\n",
    "            \n",
    "            # before step, update old_model:\n",
    "            model_old.load_state_dict(model.state_dict())\n",
    "            \n",
    "            # step\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward(retain_variables=True)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # t finish, print:\n",
    "        df_infos = pd.DataFrame(infos)\n",
    "        \n",
    "        report=OrderedDict(\n",
    "            episode=episode,\n",
    "#             reward=av_reward / float(cum_done),\n",
    "            loss=av_loss,\n",
    "            cash_bias=df_infos.cash_bias.mean(),\n",
    "            market_value=df_infos.market_value.mean(),\n",
    "            portfolio_value=df_infos.portfolio_value.mean(),\n",
    "            reward=df_infos.reward.mean()\n",
    "        )\n",
    "        \n",
    "        s = ', '.join(['{}={:2.4g}'.format(key,value) for key,value in report.items()])\n",
    "        print(s)\n",
    "        \n",
    "        reports.append(report)\n",
    "        \n",
    "        memory.clear()\n",
    "        torch.save(model_old, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T03:48:54.472250Z",
     "start_time": "2017-08-06T03:48:54.442267Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-06T07:15:06.359Z"
    }
   },
   "outputs": [],
   "source": [
    "# show progress\n",
    "df=pd.DataFrame(reports)\n",
    "g = sns.jointplot(x=\"episode\", y=\"loss\", data=df, kind=\"reg\", size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-06T07:15:06.364Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# env_test = env\n",
    "\n",
    "# Test\n",
    "for i in range(10):\n",
    "    model.train(False)\n",
    "    state = env_test.reset()\n",
    "    for i in range(250):\n",
    "        state = Variable(torch.Tensor(state).unsqueeze(0))\n",
    "        mu, sigma_sq, v = model(state)\n",
    "        eps = torch.randn(mu.size())\n",
    "        action = (mu + sigma_sq.sqrt() * Variable(eps))\n",
    "        env_action = action.data.squeeze().numpy()\n",
    "        state, reward, done, info = env_test.step(env_action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env_test.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-06T03:52:12.779572Z",
     "start_time": "2017-08-06T03:52:12.743767Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "85px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
