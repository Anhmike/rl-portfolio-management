{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:40:52.715453Z",
     "start_time": "2017-07-15T14:40:52.013115+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# numeric\n",
    "import quandl\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# util\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import time\n",
    "import tempfile\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:40:52.742035Z",
     "start_time": "2017-07-15T14:40:52.717286+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:40:52.763591Z",
     "start_time": "2017-07-15T14:40:52.743756+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:40:52.781632Z",
     "start_time": "2017-07-15T14:40:52.765378+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:51:47.252907Z",
     "start_time": "2017-07-15T15:51:47.232896+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnvWrapper(PortfolioEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def step(self, action):\n",
    "        # also it puts it in a list\n",
    "        if isinstance(action, list):\n",
    "            action = action[0]\n",
    "        \n",
    "        # we have to normalise for some reason softmax wont work\n",
    "        if isinstance(action, dict):\n",
    "            action = np.abs(list(action.values()))\n",
    "            action /= action.sum()        \n",
    "        \n",
    "        return super().step(action) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:51:48.159427Z",
     "start_time": "2017-07-15T15:51:47.706019+08:00"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_hdf('./data/poliniex_30m.hf',key='train')\n",
    "env = EnvWrapper(\n",
    "    df=df_train,\n",
    "    steps=128, \n",
    "    scale=True, \n",
    "    augument=0.0005    \n",
    ")\n",
    "env.seed = 0   \n",
    "\n",
    "df_test = pd.read_hdf('./data/poliniex_30m.hf',key='test')\n",
    "env_test = EnvWrapper(\n",
    "    df=df_test,\n",
    "    steps=128, \n",
    "    scale=True, \n",
    "    augument=0.00)\n",
    "env_test.seed = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:52:20.529303Z",
     "start_time": "2017-07-15T15:52:20.496639+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2017-07-15 15:52:20,523] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "from tensorforce.environments.openai_gym import OpenAIGym\n",
    "environment = OpenAIGym('CartPole-v0')\n",
    "environment.gym = env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:52:21.308296Z",
     "start_time": "2017-07-15T15:52:21.280189+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce import Configuration\n",
    "from tensorforce.agents import VPGAgent\n",
    "from tensorforce.core.networks import layered_network_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:52:21.788836Z",
     "start_time": "2017-07-15T15:52:21.767377+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# Define a network builder from an ordered list of layers\n",
    "# https://github.com/reinforceio/tensorforce/blob/0d07fadec03f76537a2431e17c51cd759d53b5e9/tensorforce/core/networks/layers.py\n",
    "layers = [\n",
    "    dict(type='flatten'),\n",
    "    dict(type='dense', size=32),\n",
    "    dict(type='dense', size=32),\n",
    "    dict(type='nonlinearity', name='sigmoid'),\n",
    "    dict(type='nonlinearity', name='relu'),\n",
    "    \n",
    "]\n",
    "network = layered_network_builder(layers_config=layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:52:23.148763Z",
     "start_time": "2017-07-15T15:52:22.415634+08:00"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "config = Configuration(\n",
    "    batch_size=32,\n",
    "    states=dict(shape=tuple(env.observation_space.shape), type='float'),\n",
    "#     states=dict(shape=env.observation_space.shape, type=float),\n",
    "#     states=dict([('action'+str(k),v) for k,v in enumerate([state1]*nb_actions)]),\n",
    "    # https://github.com/reinforceio/tensorforce/blob/master/tensorforce/environments/openai_gym.py#L84\n",
    "    actions={'action' + str(n): dict(continuous=True) for n in range(env.action_space.shape[0])},\n",
    "#     actions={'action' + str(n): dict(continuous=True) for n in range(len(env.action_space.shape))},\n",
    "#     exploration=dict(\n",
    "#         type='EpsilonDecay',\n",
    "#         kwargs=dict(epsilon=1, epsilon_final=0.01, epsilon_timesteps=1e4)),\n",
    "    network=network,\n",
    "    generalized_advantage_estimation=False,\n",
    "    normalize_advantage=False,\n",
    "    sample_actions=True\n",
    ")\n",
    "\n",
    "# Create a Trust Region Policy Optimization agent\n",
    "agent = VPGAgent(config=config)\n",
    "\n",
    "# for some reason these are not set?\n",
    "agent.next_internal = agent.current_internal = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:53:24.499468Z",
     "start_time": "2017-07-15T15:53:24.470539+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorforce.models.vpg_model.VPGModel at 0x7f71e0c00f28>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why does softmax not work, how to view this?\n",
    "agent.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:52:27.046370Z",
     "start_time": "2017-07-15T15:52:26.670119+08:00"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00438812733755\n",
      "-0.00437489420782\n",
      "-0.000807419859218\n",
      "0.00328737897099\n",
      "-0.00327510418118\n",
      "-0.000199139228671\n",
      "-0.00146120935013\n",
      "0.00510737838663\n",
      "-0.00378710474651\n",
      "-0.00524470238275\n",
      "0.00430609927006\n",
      "-0.0015289046655\n",
      "-0.00323449487063\n",
      "-0.0054726359256\n",
      "-0.000352332110157\n",
      "-0.00408013979925\n",
      "0.00551566769843\n",
      "-0.0084226191255\n",
      "-0.00160485939118\n",
      "0.00105002635982\n",
      "-0.0056811879153\n",
      "0.00385485701753\n",
      "-0.000928409106663\n",
      "0.00499031513451\n",
      "-0.00512698083541\n",
      "0.000862944651578\n",
      "-0.00036530181874\n",
      "0.00559280263081\n",
      "-0.000231807997225\n",
      "0.000179197844571\n",
      "-0.00923303106192\n",
      "0.00473967973338\n",
      "-0.00290978863007\n",
      "-0.00605129762295\n",
      "-0.00242724681271\n",
      "-0.00492676786765\n",
      "-0.0059147862221\n",
      "0.000651893696436\n",
      "-0.00294846142033\n",
      "-0.00419780257275\n",
      "-0.00440700486449\n",
      "3.92402988733e-05\n",
      "0.00776443210555\n",
      "-0.010186845306\n",
      "-0.013979203807\n",
      "-2.14082403378e-06\n",
      "-0.00968769598724\n",
      "-0.00152467765548\n",
      "0.0037114337242\n",
      "0.0090361527741\n",
      "-0.00210604388839\n",
      "-0.000353805128936\n",
      "-0.0012306018666\n",
      "-0.0050666996567\n",
      "-0.00814795538018\n",
      "-0.00133065396143\n",
      "0.0115994472541\n",
      "-0.00684541084388\n",
      "-0.0019048616836\n",
      "0.00661939566604\n",
      "-0.00248496752424\n",
      "-0.00759676390867\n",
      "-0.0081092439961\n",
      "0.00504351720355\n",
      "-0.00408509953252\n",
      "-0.00507587854694\n",
      "-0.00603671678008\n",
      "-0.00223730672128\n",
      "-0.000834311645513\n",
      "-0.00951808663269\n",
      "-0.000403510493128\n",
      "0.00615193065968\n",
      "0.0125012128268\n",
      "0.00143455059453\n",
      "-0.00674851138311\n",
      "0.00677752783525\n",
      "-0.000204816695917\n",
      "0.00145040019796\n",
      "0.00565103256127\n",
      "-0.00485706940073\n",
      "-0.00156462960975\n",
      "-0.00373219562804\n",
      "0.00171159136647\n",
      "-0.00553448293367\n",
      "0.00193265112789\n",
      "-0.00127880176211\n",
      "0.0100975961724\n",
      "0.00313059928804\n",
      "-0.00153091785139\n",
      "-0.00470388234627\n",
      "0.00319392921772\n",
      "0.00438743041554\n",
      "-0.0155267671141\n",
      "-0.00582528206123\n",
      "0.00143079070674\n",
      "-0.00970094615453\n",
      "-0.00623072553482\n",
      "-0.00031858068854\n",
      "0.0115999273917\n",
      "0.00637157948063\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Poll new state from client\n",
    "state = environment.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    # Get prediction from agent, execute\n",
    "    action = agent.act(state=state)\n",
    "\n",
    "    state, reward, done = environment.execute(action)\n",
    "\n",
    "    # Add experience, agent automatically updates model according to batch size\n",
    "    agent.observe(reward=reward, terminal=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:53:33.351079Z",
     "start_time": "2017-07-15T15:53:33.333261+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce.execution import Runner\n",
    "runner = Runner(agent=agent, environment=environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-15T07:56:24.216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 2 after 128 timesteps (reward: -0.2520) portfolio_value: 0.7863\n",
      "Finished episode 4 after 128 timesteps (reward: -0.1582) portfolio_value: 0.8575\n",
      "Finished episode 6 after 128 timesteps (reward: -0.2620) portfolio_value: 0.7717\n",
      "Finished episode 8 after 128 timesteps (reward: -0.2728) portfolio_value: 0.7714\n",
      "Finished episode 10 after 128 timesteps (reward: -0.3500) portfolio_value: 0.7079\n",
      "Finished episode 12 after 128 timesteps (reward: -0.1362) portfolio_value: 0.8819\n",
      "Finished episode 14 after 128 timesteps (reward: -0.1603) portfolio_value: 0.8622\n",
      "Finished episode 16 after 128 timesteps (reward: -0.2177) portfolio_value: 0.8139\n",
      "Finished episode 18 after 128 timesteps (reward: -0.1636) portfolio_value: 0.8601\n",
      "Finished episode 20 after 128 timesteps (reward: -0.2167) portfolio_value: 0.8276\n",
      "Finished episode 22 after 128 timesteps (reward: -0.2418) portfolio_value: 0.7707\n",
      "Finished episode 24 after 128 timesteps (reward: -0.2219) portfolio_value: 0.8071\n",
      "Finished episode 26 after 128 timesteps (reward: -0.1742) portfolio_value: 0.8585\n",
      "Finished episode 28 after 128 timesteps (reward: -0.2559) portfolio_value: 0.7827\n",
      "Finished episode 30 after 128 timesteps (reward: -0.1428) portfolio_value: 0.8640\n",
      "Finished episode 32 after 128 timesteps (reward: -0.2345) portfolio_value: 0.7846\n",
      "Finished episode 34 after 128 timesteps (reward: -0.2781) portfolio_value: 0.7586\n",
      "Finished episode 36 after 128 timesteps (reward: -0.2416) portfolio_value: 0.7918\n",
      "Finished episode 38 after 128 timesteps (reward: -0.2196) portfolio_value: 0.8101\n",
      "Finished episode 40 after 128 timesteps (reward: -0.2268) portfolio_value: 0.7787\n",
      "Finished episode 42 after 128 timesteps (reward: -0.1322) portfolio_value: 0.8838\n",
      "Finished episode 44 after 128 timesteps (reward: -0.2546) portfolio_value: 0.7935\n",
      "Finished episode 46 after 128 timesteps (reward: -0.1867) portfolio_value: 0.8434\n",
      "Finished episode 48 after 128 timesteps (reward: -0.3129) portfolio_value: 0.7326\n",
      "Finished episode 50 after 128 timesteps (reward: -0.2113) portfolio_value: 0.8143\n",
      "Finished episode 52 after 128 timesteps (reward: -0.2021) portfolio_value: 0.8289\n",
      "Finished episode 54 after 128 timesteps (reward: -0.1301) portfolio_value: 0.8759\n",
      "Finished episode 56 after 128 timesteps (reward: -0.2207) portfolio_value: 0.8021\n",
      "Finished episode 58 after 128 timesteps (reward: -0.2767) portfolio_value: 0.7697\n",
      "Finished episode 60 after 128 timesteps (reward: 2.9977) portfolio_value: 6.5323\n",
      "Finished episode 62 after 128 timesteps (reward: -0.2517) portfolio_value: 0.7861\n",
      "Finished episode 64 after 128 timesteps (reward: -0.2071) portfolio_value: 0.8153\n",
      "Finished episode 66 after 128 timesteps (reward: -0.2402) portfolio_value: 0.7965\n",
      "Finished episode 68 after 128 timesteps (reward: -0.1842) portfolio_value: 0.8405\n",
      "Finished episode 70 after 128 timesteps (reward: -0.3150) portfolio_value: 0.7565\n",
      "Finished episode 72 after 128 timesteps (reward: -0.2203) portfolio_value: 0.8126\n",
      "Finished episode 74 after 128 timesteps (reward: -0.2359) portfolio_value: 0.7965\n",
      "Finished episode 76 after 128 timesteps (reward: -0.2632) portfolio_value: 0.7752\n",
      "Finished episode 78 after 128 timesteps (reward: -0.2513) portfolio_value: 0.7807\n",
      "Finished episode 80 after 128 timesteps (reward: -0.1850) portfolio_value: 0.8450\n",
      "Finished episode 82 after 128 timesteps (reward: -0.3165) portfolio_value: 0.7301\n",
      "Finished episode 84 after 128 timesteps (reward: -0.3362) portfolio_value: 0.7133\n",
      "Finished episode 86 after 128 timesteps (reward: -0.2357) portfolio_value: 0.7903\n",
      "Finished episode 88 after 128 timesteps (reward: -0.2513) portfolio_value: 0.7844\n",
      "Finished episode 90 after 128 timesteps (reward: -0.2555) portfolio_value: 0.7819\n",
      "Finished episode 92 after 128 timesteps (reward: -0.2450) portfolio_value: 0.7884\n",
      "Finished episode 94 after 128 timesteps (reward: -0.2156) portfolio_value: 0.8161\n",
      "Finished episode 96 after 128 timesteps (reward: -0.3380) portfolio_value: 0.7243\n",
      "Finished episode 98 after 128 timesteps (reward: -0.2782) portfolio_value: 0.7611\n",
      "Finished episode 100 after 128 timesteps (reward: -0.3070) portfolio_value: 0.7263\n",
      "Finished episode 102 after 128 timesteps (reward: -0.1943) portfolio_value: 0.8277\n",
      "Finished episode 104 after 128 timesteps (reward: -0.2097) portfolio_value: 0.8205\n",
      "Finished episode 106 after 128 timesteps (reward: -0.2511) portfolio_value: 0.7818\n",
      "Finished episode 108 after 128 timesteps (reward: -0.2310) portfolio_value: 0.8016\n",
      "Finished episode 110 after 128 timesteps (reward: -0.2944) portfolio_value: 0.7505\n",
      "Finished episode 112 after 128 timesteps (reward: -0.2422) portfolio_value: 0.8008\n",
      "Finished episode 114 after 128 timesteps (reward: -0.2648) portfolio_value: 0.7754\n",
      "Finished episode 116 after 128 timesteps (reward: -0.3053) portfolio_value: 0.7447\n",
      "Finished episode 118 after 128 timesteps (reward: -0.2790) portfolio_value: 0.7646\n",
      "Finished episode 120 after 128 timesteps (reward: -0.2969) portfolio_value: 0.7626\n",
      "Finished episode 122 after 128 timesteps (reward: -0.2396) portfolio_value: 0.7728\n",
      "Finished episode 124 after 128 timesteps (reward: -0.2441) portfolio_value: 0.7998\n",
      "Finished episode 126 after 128 timesteps (reward: -0.2181) portfolio_value: 0.8094\n",
      "Finished episode 128 after 128 timesteps (reward: -0.2644) portfolio_value: 0.7784\n",
      "Finished episode 130 after 128 timesteps (reward: -0.1083) portfolio_value: 0.9203\n",
      "Finished episode 132 after 128 timesteps (reward: 0.5245) portfolio_value: 1.6991\n",
      "Finished episode 134 after 128 timesteps (reward: -0.2017) portfolio_value: 0.8245\n",
      "Finished episode 136 after 128 timesteps (reward: -0.3583) portfolio_value: 0.7122\n",
      "Finished episode 138 after 128 timesteps (reward: -0.2583) portfolio_value: 0.7754\n",
      "Finished episode 140 after 128 timesteps (reward: -0.2879) portfolio_value: 0.7555\n",
      "Finished episode 142 after 128 timesteps (reward: -0.3333) portfolio_value: 0.7217\n",
      "Finished episode 144 after 128 timesteps (reward: -0.3083) portfolio_value: 0.7582\n",
      "Finished episode 146 after 128 timesteps (reward: -0.1533) portfolio_value: 0.8578\n",
      "Finished episode 148 after 128 timesteps (reward: -0.2708) portfolio_value: 0.7731\n",
      "Finished episode 150 after 128 timesteps (reward: -0.1669) portfolio_value: 0.8580\n",
      "Finished episode 152 after 128 timesteps (reward: -0.2312) portfolio_value: 0.8007\n",
      "Finished episode 154 after 128 timesteps (reward: -0.2439) portfolio_value: 0.7858\n",
      "Finished episode 156 after 128 timesteps (reward: -0.2078) portfolio_value: 0.8195\n",
      "Finished episode 158 after 128 timesteps (reward: -0.2934) portfolio_value: 0.7514\n",
      "Finished episode 160 after 128 timesteps (reward: -0.1333) portfolio_value: 0.8821\n",
      "Finished episode 162 after 128 timesteps (reward: -0.2321) portfolio_value: 0.8066\n",
      "Finished episode 164 after 128 timesteps (reward: -0.3229) portfolio_value: 0.7314\n",
      "Finished episode 166 after 128 timesteps (reward: -0.2457) portfolio_value: 0.7889\n",
      "Finished episode 168 after 128 timesteps (reward: -0.3628) portfolio_value: 0.7047\n",
      "Finished episode 170 after 128 timesteps (reward: -0.2299) portfolio_value: 0.7974\n",
      "Finished episode 172 after 128 timesteps (reward: -0.2546) portfolio_value: 0.7843\n",
      "Finished episode 174 after 128 timesteps (reward: -0.2528) portfolio_value: 0.7807\n",
      "Finished episode 176 after 128 timesteps (reward: -0.2549) portfolio_value: 0.7794\n",
      "Finished episode 178 after 128 timesteps (reward: -0.2199) portfolio_value: 0.8197\n",
      "Finished episode 180 after 128 timesteps (reward: -0.2093) portfolio_value: 0.8085\n",
      "Finished episode 182 after 128 timesteps (reward: -0.1769) portfolio_value: 0.8572\n",
      "Finished episode 184 after 128 timesteps (reward: -0.2272) portfolio_value: 0.8026\n",
      "Finished episode 186 after 128 timesteps (reward: -0.2679) portfolio_value: 0.7771\n",
      "Finished episode 188 after 128 timesteps (reward: -0.2441) portfolio_value: 0.7949\n",
      "Finished episode 190 after 128 timesteps (reward: -0.2189) portfolio_value: 0.8109\n",
      "Finished episode 192 after 128 timesteps (reward: -0.2579) portfolio_value: 0.7802\n",
      "Finished episode 194 after 128 timesteps (reward: -0.2814) portfolio_value: 0.7625\n",
      "Finished episode 196 after 128 timesteps (reward: -0.2543) portfolio_value: 0.7928\n",
      "Finished episode 198 after 128 timesteps (reward: -0.1901) portfolio_value: 0.8303\n",
      "Finished episode 200 after 128 timesteps (reward: -0.2095) portfolio_value: 0.8173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 202 after 128 timesteps (reward: -0.1997) portfolio_value: 0.8215\n",
      "Finished episode 204 after 128 timesteps (reward: -0.2797) portfolio_value: 0.7639\n"
     ]
    }
   ],
   "source": [
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    if r.episode%2==0:\n",
    "        df = pd.DataFrame(env.sim.infos)\n",
    "#         df.log_reward[-10:].mean()\n",
    "#         df.portfolio_value[-10:].mean()\n",
    "        print(\"Finished episode {ep} after {ts} timesteps (reward: {reward:2.4f}) portfolio_value: {portfolio_value:2.4f}\".format(ep=r.episode, ts=r.timestep, reward=r.episode_rewards[-1], portfolio_value=df.portfolio_value[-10:].mean()))\n",
    "    return True\n",
    "runner.run(episodes=90000, max_timesteps=200, episode_finished=episode_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
