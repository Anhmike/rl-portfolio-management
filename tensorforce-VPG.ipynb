{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T15:05:22.574747Z",
     "start_time": "2017-07-15T23:05:21.709431+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# numeric\n",
    "import quandl\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# util\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import time\n",
    "import tempfile\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T15:05:22.603518Z",
     "start_time": "2017-07-15T23:05:22.576899+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T15:05:22.608494Z",
     "start_time": "2017-07-15T23:05:22.604934+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T15:05:22.627394Z",
     "start_time": "2017-07-15T23:05:22.611493+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T15:05:22.637472Z",
     "start_time": "2017-07-15T23:05:22.629021+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnvWrapper(PortfolioEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def step(self, action):\n",
    "        # also it puts it in a list\n",
    "        if isinstance(action, list):\n",
    "            action = action[0]\n",
    "        \n",
    "        # we have to normalise for some reason softmax wont work\n",
    "        if isinstance(action, dict):\n",
    "            action = np.abs(list(action.values()))\n",
    "            action /= action.sum()        \n",
    "        \n",
    "        return super().step(action) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T15:05:23.250314Z",
     "start_time": "2017-07-15T23:05:22.639258+08:00"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_hdf('./data/poliniex_30m.hf',key='train')\n",
    "env = EnvWrapper(\n",
    "    df=df_train,\n",
    "    steps=128, \n",
    "    scale=True, \n",
    "    augument=0.0005    \n",
    ")\n",
    "env.seed = 0   \n",
    "\n",
    "df_test = pd.read_hdf('./data/poliniex_30m.hf',key='test')\n",
    "env_test = EnvWrapper(\n",
    "    df=df_test,\n",
    "    steps=128, \n",
    "    scale=True, \n",
    "    augument=0.00)\n",
    "env_test.seed = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T15:05:23.263124Z",
     "start_time": "2017-07-15T23:05:23.251679+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2017-07-15 23:05:23,255] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "from tensorforce.environments.openai_gym import OpenAIGym\n",
    "environment = OpenAIGym('CartPole-v0')\n",
    "environment.gym = env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T22:10:10.500095Z",
     "start_time": "2017-07-16T06:10:10.493474+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2017-07-16 06:10:10,494] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "environment_test = OpenAIGym('CartPole-v0')\n",
    "environment_test.gym = env_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T15:05:24.369337Z",
     "start_time": "2017-07-15T23:05:23.264894+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce import Configuration\n",
    "from tensorforce.agents import VPGAgent\n",
    "from tensorforce.core.networks import layered_network_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T15:05:24.377898Z",
     "start_time": "2017-07-15T23:05:24.371339+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# Define a network builder from an ordered list of layers\n",
    "# https://github.com/reinforceio/tensorforce/blob/0d07fadec03f76537a2431e17c51cd759d53b5e9/tensorforce/core/networks/layers.py\n",
    "layers = [\n",
    "    dict(type='flatten'),\n",
    "    dict(type='dense', size=32, l2_regularization=1e-8, activation='selu'),\n",
    "    dict(type='dense', size=32, l2_regularization=1e-8, activation='selu'),    \n",
    "    dict(type='dense', size=32, l2_regularization=1e-8, activation='selu'), \n",
    "]\n",
    "# act will it add's it's own head so we can't add a softmax at the end\n",
    "network = layered_network_builder(layers_config=layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T15:07:07.783756Z",
     "start_time": "2017-07-15T23:07:06.830267+08:00"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "config = Configuration(   \n",
    "    # Each agent requires the following ``Configuration`` parameters:\n",
    "    network=network,\n",
    "    states=dict(shape=tuple(env.observation_space.shape), type='float'),\n",
    "    actions={'action' + str(n): dict(continuous=True) for n in range(env.action_space.shape[0])},\n",
    "    preprocessing = None,# dict or list containing state preprocessing configuration.\n",
    "    exploration = dict(\n",
    "        type='EpsilonDecay',\n",
    "        kwargs=dict(epsilon=1, epsilon_final=0.01, epsilon_timesteps=1e4)\n",
    "    ),\n",
    "\n",
    "    # The `BatchAgent` class additionally requires the following parameters:\n",
    "    batch_size = 32,# integer of the batch size.\n",
    "\n",
    "    # A Policy Gradient Model expects the following additional configuration parameters:\n",
    "    sample_actions= True,# boolean of whether to sample actions.\n",
    "#     baseline='mlp' ,# string indicating the baseline value function (currently 'linear' or 'mlp').\n",
    "    baseline_args=dict(size=100, repeat_update=100) ,# list of arguments for the baseline value function.\n",
    "    override_line_search=False,\n",
    "    \n",
    "#     baseline_kwargs= ,# dict of keyword arguments for the baseline value function.\n",
    "    generalized_advantage_estimation= True ,# boolean indicating whether to use GAE.\n",
    "    gae_lambda= 0.97,# float of the Generalized Advantage Estimation lambda.\n",
    "    normalize_advantage= False,# boolean indicating whether to normalize the advantage or not.\n",
    "    cg_iterations=20,\n",
    "    max_kl_divergence=0.005,\n",
    "    cg_damping=0.001,\n",
    "    line_search_steps=20,\n",
    "    loglevel=\"info\",\n",
    ")\n",
    "\n",
    "# Create a Trust Region Policy Optimization agent\n",
    "agent = VPGAgent(config=config)\n",
    "\n",
    "# for some reason these are not set?\n",
    "agent.next_internal = agent.current_internal = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T15:07:21.441785Z",
     "start_time": "2017-07-15T23:07:21.438647+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce.execution import Runner\n",
    "runner = Runner(agent=agent, environment=environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T22:34:20.277027Z",
     "start_time": "2017-07-16T06:34:20.267667+08:00"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Callback function printing episode statistics\n",
    "class EpisodeFinished(object):\n",
    "    \n",
    "    def __init__(self, log_intv):\n",
    "        self.log_intv = log_intv\n",
    "        self.portfolio_values = [] \n",
    "    \n",
    "    def episode_finished(self, r):\n",
    "        if len(runner_test.environment.gym.sim.infos):\n",
    "            self.portfolio_values.append( r.environment.gym.sim.infos[-1]['portfolio_value'] )\n",
    "        if r.episode % self.log_intv == 0:\n",
    "#             df = pd.DataFrame(r.environment.gym.infos)\n",
    "            print(\n",
    "                \"Finished episode {ep} after {ts} timesteps (reward: {reward: 2.4f} [{rewards_min: 2.4f}, {rewards_max: 2.4f}]) portfolio_value: {portfolio_value: 2.4f} [{portfolio_value_min: 2.4f}, {portfolio_value_max: 2.4f}]\".\n",
    "                format(\n",
    "                    ep=r.episode,\n",
    "                    ts=r.timestep,\n",
    "                    reward=np.mean(r.episode_rewards[-self.log_intv:]),\n",
    "                    rewards_min=np.min(r.episode_rewards[-self.log_intv:]),\n",
    "                    rewards_max=np.max(r.episode_rewards[-self.log_intv:]),\n",
    "                    portfolio_value=np.mean(self.portfolio_values[-self.log_intv:]),\n",
    "                    portfolio_value_min=np.min(self.portfolio_values[-self.log_intv:]),\n",
    "                    portfolio_value_max=np.max(self.portfolio_values[-self.log_intv:])\n",
    "                )\n",
    "            )\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T22:10:09.465349Z",
     "start_time": "2017-07-15T23:08:36.070712+08:00"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 100 after 128 timesteps (reward: -0.0277 [-0.3963,  6.4502]) portfolio_value:  0.8600\n",
      "Finished episode 200 after 128 timesteps (reward: -0.1461 [-0.2775,  0.2785]) portfolio_value:  0.8632\n",
      "Finished episode 300 after 128 timesteps (reward: -0.1395 [-0.2846,  0.5663]) portfolio_value:  0.8751\n",
      "Finished episode 400 after 128 timesteps (reward: -0.1587 [-0.3149,  0.0365]) portfolio_value:  0.8685\n",
      "Finished episode 500 after 128 timesteps (reward: -0.1402 [-0.2792,  0.2533]) portfolio_value:  0.8543\n",
      "Finished episode 600 after 128 timesteps (reward: -0.0491 [-0.2891,  10.2761]) portfolio_value:  0.8590\n",
      "Finished episode 700 after 128 timesteps (reward: -0.1251 [-0.2834,  0.9922]) portfolio_value:  0.9553\n",
      "Finished episode 800 after 128 timesteps (reward: -0.0755 [-0.3038,  3.1840]) portfolio_value:  11.6895\n",
      "Finished episode 900 after 128 timesteps (reward:  0.1300 [-0.3031,  8.8457]) portfolio_value:  0.9336\n",
      "Finished episode 1000 after 128 timesteps (reward: -0.1311 [-0.3085,  0.1533]) portfolio_value:  0.8383\n",
      "Finished episode 1100 after 128 timesteps (reward:  0.0300 [-0.2736,  8.7409]) portfolio_value:  0.7606\n",
      "Finished episode 1200 after 128 timesteps (reward: -0.1310 [-0.2448,  0.0912]) portfolio_value:  0.8669\n",
      "Finished episode 1300 after 128 timesteps (reward:  0.0766 [-0.3323,  9.8042]) portfolio_value:  0.8328\n",
      "Finished episode 1400 after 128 timesteps (reward:  0.1647 [-0.2679,  10.3014]) portfolio_value:  0.9233\n",
      "Finished episode 1500 after 128 timesteps (reward:  0.0468 [-0.3091,  10.5940]) portfolio_value:  0.9915\n",
      "Finished episode 1600 after 128 timesteps (reward: -0.0363 [-0.2963,  5.6543]) portfolio_value:  0.8734\n",
      "Finished episode 1700 after 128 timesteps (reward: -0.0713 [-0.3319,  5.5885]) portfolio_value:  0.8956\n",
      "Finished episode 1800 after 128 timesteps (reward: -0.0348 [-0.2538,  8.6423]) portfolio_value:  0.9790\n",
      "Finished episode 1900 after 128 timesteps (reward: -0.0607 [-0.2882,  6.4941]) portfolio_value:  0.8587\n",
      "Finished episode 2000 after 128 timesteps (reward: -0.0385 [-0.3222,  9.5126]) portfolio_value:  0.9532\n",
      "Finished episode 2100 after 128 timesteps (reward: -0.0986 [-0.2837,  3.4855]) portfolio_value:  0.9773\n",
      "Finished episode 2200 after 128 timesteps (reward: -0.0886 [-0.2739,  3.7619]) portfolio_value:  0.8758\n",
      "Finished episode 2300 after 128 timesteps (reward: -0.0815 [-0.2682,  6.1439]) portfolio_value:  0.8386\n",
      "Finished episode 2400 after 128 timesteps (reward: -0.1450 [-0.3127,  0.1752]) portfolio_value:  0.8488\n",
      "Finished episode 2500 after 128 timesteps (reward: -0.0836 [-0.3768,  3.4749]) portfolio_value:  0.8195\n",
      "Finished episode 2600 after 128 timesteps (reward: -0.0999 [-0.2465,  2.1612]) portfolio_value:  0.8394\n",
      "Finished episode 2700 after 128 timesteps (reward: -0.0678 [-0.3110,  6.3977]) portfolio_value:  1.0252\n",
      "Finished episode 2800 after 128 timesteps (reward: -0.0595 [-0.2428,  6.1439]) portfolio_value:  0.9027\n",
      "Finished episode 2900 after 128 timesteps (reward:  0.0335 [-0.3724,  6.2781]) portfolio_value:  0.8928\n",
      "Finished episode 3000 after 128 timesteps (reward: -0.1229 [-0.2465,  0.3698]) portfolio_value:  0.8890\n",
      "Finished episode 3100 after 128 timesteps (reward: -0.0702 [-0.2728,  3.6985]) portfolio_value:  0.9089\n",
      "Finished episode 3200 after 128 timesteps (reward: -0.0745 [-0.3007,  6.0075]) portfolio_value:  0.8465\n",
      "Finished episode 3300 after 128 timesteps (reward: -0.0605 [-0.3138,  5.8562]) portfolio_value:  0.8663\n",
      "Finished episode 3400 after 128 timesteps (reward: -0.1236 [-0.2530,  0.0970]) portfolio_value:  0.9022\n",
      "Finished episode 3500 after 128 timesteps (reward:  0.0033 [-0.2643,  8.2606]) portfolio_value:  0.9030\n",
      "Finished episode 3600 after 128 timesteps (reward: -0.1033 [-0.2635,  3.3932]) portfolio_value:  0.8874\n",
      "Finished episode 3700 after 128 timesteps (reward: -0.0640 [-0.3475,  6.5140]) portfolio_value:  0.8525\n",
      "Finished episode 3800 after 128 timesteps (reward: -0.0827 [-0.2489,  3.5648]) portfolio_value:  0.8690\n",
      "Finished episode 3900 after 128 timesteps (reward: -0.0391 [-0.2851,  10.4442]) portfolio_value:  0.8528\n",
      "Finished episode 4000 after 128 timesteps (reward: -0.1445 [-0.3273,  0.3702]) portfolio_value:  0.8633\n",
      "Finished episode 4100 after 128 timesteps (reward: -0.1519 [-0.3223,  0.2029]) portfolio_value:  0.8999\n",
      "Finished episode 4200 after 128 timesteps (reward: -0.1449 [-0.3025,  0.2216]) portfolio_value:  0.9026\n",
      "Finished episode 4300 after 128 timesteps (reward: -0.0357 [-0.2471,  9.9713]) portfolio_value:  0.8441\n",
      "Finished episode 4400 after 128 timesteps (reward: -0.0624 [-0.2934,  9.6578]) portfolio_value:  0.7457\n",
      "Finished episode 4500 after 128 timesteps (reward: -0.1519 [-0.3024,  0.1360]) portfolio_value:  0.8951\n",
      "Finished episode 4600 after 128 timesteps (reward: -0.0877 [-0.2830,  3.8847]) portfolio_value:  1.2784\n",
      "Finished episode 4700 after 128 timesteps (reward: -0.0130 [-0.2456,  9.9424]) portfolio_value:  0.8510\n",
      "Finished episode 4800 after 128 timesteps (reward: -0.1214 [-0.3121,  0.1469]) portfolio_value:  0.9693\n",
      "Finished episode 4900 after 128 timesteps (reward: -0.0075 [-0.2460,  9.6372]) portfolio_value:  0.8756\n",
      "Finished episode 5000 after 128 timesteps (reward: -0.0887 [-0.3791,  5.0274]) portfolio_value:  0.8686\n",
      "Finished episode 5100 after 128 timesteps (reward: -0.0270 [-0.2798,  10.4659]) portfolio_value:  0.8710\n",
      "Finished episode 5200 after 128 timesteps (reward: -0.0227 [-0.2348,  4.9952]) portfolio_value:  0.8436\n",
      "Finished episode 5300 after 128 timesteps (reward: -0.1230 [-0.2610,  0.5880]) portfolio_value:  0.8164\n",
      "Finished episode 5400 after 128 timesteps (reward: -0.1050 [-0.3008,  2.5795]) portfolio_value:  0.9345\n",
      "Finished episode 5500 after 128 timesteps (reward: -0.1088 [-0.3211,  1.5146]) portfolio_value:  1.0569\n",
      "Finished episode 5600 after 128 timesteps (reward: -0.1137 [-0.2790,  2.7854]) portfolio_value:  0.8689\n",
      "Finished episode 5700 after 128 timesteps (reward: -0.0985 [-0.2648,  3.1268]) portfolio_value:  0.9544\n",
      "Finished episode 5800 after 128 timesteps (reward: -0.0705 [-0.3290,  5.9992]) portfolio_value:  0.9086\n",
      "Finished episode 5900 after 128 timesteps (reward: -0.1313 [-0.3043,  0.0613]) portfolio_value:  0.8567\n",
      "Finished episode 6000 after 128 timesteps (reward: -0.0808 [-0.2126,  3.1063]) portfolio_value:  0.9064\n",
      "Finished episode 6100 after 128 timesteps (reward: -0.1271 [-0.2781,  0.1992]) portfolio_value:  0.8971\n",
      "Finished episode 6200 after 128 timesteps (reward: -0.1226 [-0.2753,  0.2446]) portfolio_value:  0.8676\n",
      "Finished episode 6300 after 128 timesteps (reward: -0.0170 [-0.2378,  10.1614]) portfolio_value:  0.9482\n",
      "Finished episode 6400 after 128 timesteps (reward: -0.0289 [-0.2952,  9.6591]) portfolio_value:  0.8919\n",
      "Finished episode 6500 after 128 timesteps (reward: -0.0718 [-0.2651,  3.0931]) portfolio_value:  0.8803\n",
      "Finished episode 6600 after 128 timesteps (reward: -0.1248 [-0.2729,  0.0046]) portfolio_value:  0.7756\n",
      "Finished episode 6700 after 128 timesteps (reward:  0.0713 [-0.2207,  10.9494]) portfolio_value:  0.9171\n",
      "Finished episode 6800 after 128 timesteps (reward: -0.1186 [-0.2480,  0.0122]) portfolio_value:  0.8895\n",
      "Finished episode 6900 after 128 timesteps (reward: -0.1091 [-0.2475,  0.1600]) portfolio_value:  0.8902\n",
      "Finished episode 7000 after 128 timesteps (reward:  0.0124 [-0.2314,  8.2683]) portfolio_value:  0.9075\n",
      "Finished episode 7100 after 128 timesteps (reward: -0.0967 [-0.2698,  1.9864]) portfolio_value:  0.9029\n",
      "Finished episode 7200 after 128 timesteps (reward: -0.1121 [-0.2800,  0.1942]) portfolio_value:  0.9023\n",
      "Finished episode 7300 after 128 timesteps (reward: -0.0796 [-0.2639,  3.4162]) portfolio_value:  0.8953\n",
      "Finished episode 7400 after 128 timesteps (reward: -0.0518 [-0.2528,  3.5405]) portfolio_value:  0.8890\n",
      "Finished episode 7500 after 128 timesteps (reward: -0.1030 [-0.2755,  0.2248]) portfolio_value:  0.9237\n",
      "Finished episode 7600 after 128 timesteps (reward: -0.0305 [-0.2565,  8.5583]) portfolio_value:  0.8980\n",
      "Finished episode 7700 after 128 timesteps (reward: -0.0749 [-0.2220,  3.4613]) portfolio_value:  0.8816\n",
      "Finished episode 7800 after 128 timesteps (reward: -0.0821 [-0.2332,  3.3757]) portfolio_value:  0.8526\n",
      "Finished episode 7900 after 128 timesteps (reward: -0.1158 [-0.2592,  0.4808]) portfolio_value:  0.8863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 8000 after 128 timesteps (reward:  0.0940 [-0.2359,  9.9235]) portfolio_value:  0.8973\n",
      "Finished episode 8100 after 128 timesteps (reward:  0.0445 [-0.2746,  9.1373]) portfolio_value:  0.9688\n",
      "Finished episode 8200 after 128 timesteps (reward:  0.0336 [-0.2681,  9.3756]) portfolio_value:  0.8286\n",
      "Finished episode 8300 after 128 timesteps (reward: -0.0329 [-0.3058,  6.2936]) portfolio_value:  0.8534\n",
      "Finished episode 8400 after 128 timesteps (reward: -0.0831 [-0.4355,  3.3074]) portfolio_value:  1.0300\n",
      "Finished episode 8500 after 128 timesteps (reward: -0.0926 [-0.2438,  1.5461]) portfolio_value:  0.8139\n",
      "Finished episode 8600 after 128 timesteps (reward: -0.1100 [-0.2115,  0.2249]) portfolio_value:  0.8125\n",
      "Finished episode 8700 after 128 timesteps (reward: -0.0809 [-0.3003,  1.3677]) portfolio_value:  0.9518\n",
      "Finished episode 8800 after 128 timesteps (reward: -0.0171 [-0.2636,  9.0493]) portfolio_value:  0.9517\n",
      "Finished episode 8900 after 128 timesteps (reward: -0.0712 [-0.2493,  3.1933]) portfolio_value:  0.9016\n",
      "Finished episode 9000 after 128 timesteps (reward: -0.0344 [-0.2356,  5.6777]) portfolio_value:  0.8016\n",
      "Finished episode 9100 after 128 timesteps (reward: -0.1171 [-0.4239,  0.1454]) portfolio_value:  0.8993\n",
      "Finished episode 9200 after 128 timesteps (reward: -0.1092 [-0.2977,  0.1602]) portfolio_value:  0.8439\n",
      "Finished episode 9300 after 128 timesteps (reward: -0.0894 [-0.2150,  0.4402]) portfolio_value:  0.8881\n",
      "Finished episode 9400 after 128 timesteps (reward: -0.1105 [-0.2060,  0.2093]) portfolio_value:  0.9077\n",
      "Finished episode 9500 after 128 timesteps (reward: -0.0933 [-0.2563,  1.7104]) portfolio_value:  0.9206\n",
      "Finished episode 9600 after 128 timesteps (reward: -0.0916 [-0.2640,  0.7023]) portfolio_value:  0.8727\n",
      "Finished episode 9700 after 128 timesteps (reward: -0.0450 [-0.2471,  5.7349]) portfolio_value:  0.8556\n",
      "Finished episode 9800 after 128 timesteps (reward: -0.0886 [-0.2571,  0.3572]) portfolio_value:  0.9046\n",
      "Finished episode 9900 after 128 timesteps (reward: -0.0482 [-0.2679,  3.2403]) portfolio_value:  0.8834\n",
      "Finished episode 10000 after 128 timesteps (reward: -0.1114 [-0.3402,  0.0205]) portfolio_value:  0.9296\n",
      "Finished episode 10100 after 128 timesteps (reward:  0.0571 [-0.2305,  9.9486]) portfolio_value:  0.8464\n",
      "Finished episode 10200 after 128 timesteps (reward: -0.0951 [-0.2394,  0.1764]) portfolio_value:  0.8356\n",
      "Finished episode 10300 after 128 timesteps (reward: -0.0332 [-0.2166,  6.0989]) portfolio_value:  0.8844\n",
      "Finished episode 10400 after 128 timesteps (reward: -0.0553 [-0.2488,  2.8318]) portfolio_value:  0.9645\n",
      "Finished episode 10500 after 128 timesteps (reward: -0.0826 [-0.2829,  1.5616]) portfolio_value:  0.8702\n",
      "Finished episode 10600 after 128 timesteps (reward: -0.0152 [-0.4002,  7.1914]) portfolio_value:  0.8220\n",
      "Finished episode 10700 after 128 timesteps (reward: -0.0992 [-0.2913,  0.4331]) portfolio_value:  0.8546\n",
      "Finished episode 10800 after 128 timesteps (reward: -0.0843 [-0.2924,  1.5547]) portfolio_value:  0.8515\n",
      "Finished episode 10900 after 128 timesteps (reward: -0.1062 [-0.2588,  0.3085]) portfolio_value:  1.0176\n",
      "Finished episode 11000 after 128 timesteps (reward: -0.1120 [-0.2300,  0.2239]) portfolio_value:  0.9000\n",
      "Finished episode 11100 after 128 timesteps (reward:  0.0190 [-0.2183,  10.2651]) portfolio_value:  1.0705\n",
      "Finished episode 11200 after 128 timesteps (reward:  0.0173 [-0.2267,  6.3965]) portfolio_value:  0.9108\n",
      "Finished episode 11300 after 128 timesteps (reward: -0.1068 [-0.2909,  0.2992]) portfolio_value:  0.9646\n",
      "Finished episode 11400 after 128 timesteps (reward: -0.0706 [-0.2450,  3.0825]) portfolio_value:  0.8603\n",
      "Finished episode 11500 after 128 timesteps (reward: -0.0318 [-0.2867,  6.3900]) portfolio_value:  0.8505\n",
      "Finished episode 11600 after 128 timesteps (reward: -0.0128 [-0.2770,  9.9489]) portfolio_value:  1.2195\n",
      "Finished episode 11700 after 128 timesteps (reward: -0.0921 [-0.2411,  1.4679]) portfolio_value:  0.8585\n",
      "Finished episode 11800 after 128 timesteps (reward:  0.0076 [-0.3039,  8.9534]) portfolio_value:  0.9793\n",
      "Finished episode 11900 after 128 timesteps (reward: -0.0401 [-0.2755,  5.7382]) portfolio_value:  0.9038\n",
      "Finished episode 12000 after 128 timesteps (reward: -0.0785 [-0.3002,  2.7366]) portfolio_value:  0.8749\n",
      "Finished episode 12100 after 128 timesteps (reward:  0.0111 [-0.2453,  9.7758]) portfolio_value:  0.9417\n",
      "Finished episode 12200 after 128 timesteps (reward:  0.0318 [-0.2324,  6.5055]) portfolio_value:  0.9388\n",
      "Finished episode 12300 after 128 timesteps (reward: -0.0699 [-0.2442,  2.6069]) portfolio_value:  0.8871\n",
      "Finished episode 12400 after 128 timesteps (reward: -0.0782 [-0.2136,  0.1967]) portfolio_value:  0.9167\n",
      "Finished episode 12500 after 128 timesteps (reward: -0.0146 [-0.2700,  5.9013]) portfolio_value:  0.9012\n",
      "Finished episode 12600 after 128 timesteps (reward: -0.0776 [-0.1977,  0.3490]) portfolio_value:  0.9638\n",
      "Finished episode 12700 after 128 timesteps (reward: -0.0872 [-0.2552,  0.2251]) portfolio_value:  0.9186\n",
      "Finished episode 12800 after 128 timesteps (reward: -0.0778 [-0.2444,  0.4268]) portfolio_value:  0.9646\n",
      "Finished episode 12900 after 128 timesteps (reward: -0.0808 [-0.2628,  0.6232]) portfolio_value:  0.9244\n",
      "Finished episode 13000 after 128 timesteps (reward: -0.0467 [-0.2722,  3.1553]) portfolio_value:  0.9527\n",
      "Finished episode 13100 after 128 timesteps (reward: -0.0683 [-0.2107,  1.6788]) portfolio_value:  0.9574\n",
      "Finished episode 13200 after 128 timesteps (reward:  0.0891 [-0.2417,  10.0993]) portfolio_value:  0.8630\n",
      "Finished episode 13300 after 128 timesteps (reward: -0.0051 [-0.2049,  3.2371]) portfolio_value:  0.9047\n",
      "Finished episode 13400 after 128 timesteps (reward: -0.0415 [-0.2035,  2.0798]) portfolio_value:  0.8495\n",
      "Finished episode 13500 after 128 timesteps (reward: -0.0686 [-0.3089,  0.4971]) portfolio_value:  0.9900\n",
      "Finished episode 13600 after 128 timesteps (reward: -0.0810 [-0.2071,  0.1602]) portfolio_value:  0.9010\n",
      "Finished episode 13700 after 128 timesteps (reward:  0.0675 [-0.1781,  11.3295]) portfolio_value:  1.0886\n",
      "Finished episode 13800 after 128 timesteps (reward: -0.0655 [-0.1672,  0.2722]) portfolio_value:  0.9362\n",
      "Finished episode 13900 after 128 timesteps (reward: -0.0339 [-0.2026,  3.1442]) portfolio_value:  0.9072\n",
      "Finished episode 14000 after 128 timesteps (reward:  0.0515 [-0.2585,  9.9949]) portfolio_value:  0.9362\n",
      "Finished episode 14100 after 128 timesteps (reward: -0.0198 [-0.2306,  3.1890]) portfolio_value:  0.9567\n",
      "Finished episode 14200 after 128 timesteps (reward:  0.0011 [-0.2253,  6.0114]) portfolio_value:  0.9083\n",
      "Finished episode 14300 after 128 timesteps (reward: -0.0601 [-0.2775,  1.5381]) portfolio_value:  0.9218\n",
      "Finished episode 14400 after 128 timesteps (reward: -0.0252 [-0.2103,  3.0121]) portfolio_value:  0.9692\n",
      "Finished episode 14500 after 128 timesteps (reward: -0.0046 [-0.2021,  2.6432]) portfolio_value:  1.0234\n",
      "Finished episode 14600 after 128 timesteps (reward: -0.0616 [-0.1984,  0.2016]) portfolio_value:  0.8206\n",
      "Finished episode 14700 after 128 timesteps (reward: -0.0616 [-0.2020,  0.1733]) portfolio_value:  0.9282\n",
      "Finished episode 14800 after 128 timesteps (reward: -0.0565 [-0.1766,  0.1768]) portfolio_value:  0.9481\n",
      "Finished episode 14900 after 128 timesteps (reward: -0.0383 [-0.2885,  2.8846]) portfolio_value:  0.9308\n",
      "Finished episode 15000 after 128 timesteps (reward: -0.0330 [-0.2428,  2.8608]) portfolio_value:  0.9902\n",
      "Finished episode 15100 after 128 timesteps (reward: -0.0369 [-0.1946,  1.8101]) portfolio_value:  0.9305\n",
      "Finished episode 15200 after 128 timesteps (reward: -0.0041 [-0.2416,  5.2153]) portfolio_value:  0.9853\n",
      "Finished episode 15300 after 128 timesteps (reward:  0.0012 [-0.2193,  5.8369]) portfolio_value:  0.9443\n",
      "Finished episode 15400 after 128 timesteps (reward: -0.0264 [-0.2939,  2.7526]) portfolio_value:  0.9137\n",
      "Finished episode 15500 after 128 timesteps (reward: -0.0610 [-0.1702,  0.2847]) portfolio_value:  0.9403\n",
      "Finished episode 15600 after 128 timesteps (reward: -0.0183 [-0.2255,  5.6368]) portfolio_value:  1.0164\n",
      "Finished episode 15700 after 128 timesteps (reward:  0.0312 [-0.2037,  10.1238]) portfolio_value:  0.8806\n",
      "Finished episode 15800 after 128 timesteps (reward: -0.0614 [-0.1892,  0.4259]) portfolio_value:  0.9287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 15900 after 128 timesteps (reward: -0.0584 [-0.2113,  0.2950]) portfolio_value:  0.9158\n",
      "Finished episode 16000 after 128 timesteps (reward:  0.0079 [-0.2693,  5.7978]) portfolio_value:  0.9216\n",
      "Finished episode 16100 after 128 timesteps (reward: -0.0215 [-0.2171,  2.9220]) portfolio_value:  0.8921\n",
      "Finished episode 16200 after 128 timesteps (reward:  0.0325 [-0.2619,  9.9790]) portfolio_value:  0.9671\n",
      "Finished episode 16300 after 128 timesteps (reward: -0.0655 [-0.2279,  0.1814]) portfolio_value:  0.9155\n",
      "Finished episode 16400 after 128 timesteps (reward: -0.0402 [-0.1723,  1.7292]) portfolio_value:  1.0363\n",
      "Finished episode 16500 after 128 timesteps (reward: -0.0646 [-0.2595,  0.4686]) portfolio_value:  0.9354\n",
      "Finished episode 16600 after 128 timesteps (reward: -0.0464 [-0.1752,  2.5830]) portfolio_value:  0.8681\n",
      "Finished episode 16700 after 128 timesteps (reward: -0.0503 [-0.2720,  1.6566]) portfolio_value:  1.0262\n",
      "Finished episode 16800 after 128 timesteps (reward:  0.0495 [-0.2092,  10.7998]) portfolio_value:  0.9426\n",
      "Finished episode 16900 after 128 timesteps (reward: -0.0412 [-0.2180,  2.7732]) portfolio_value:  0.9513\n",
      "Finished episode 17000 after 128 timesteps (reward:  0.0376 [-0.1855,  10.2757]) portfolio_value:  0.9146\n",
      "Finished episode 17100 after 128 timesteps (reward:  0.0323 [-0.2039,  8.9923]) portfolio_value:  0.9539\n",
      "Finished episode 17200 after 128 timesteps (reward:  0.0495 [-0.1989,  10.6885]) portfolio_value:  1.0766\n",
      "Finished episode 17300 after 128 timesteps (reward: -0.0424 [-0.2093,  1.7883]) portfolio_value:  0.9199\n",
      "Finished episode 17400 after 128 timesteps (reward:  0.0097 [-0.2492,  5.4707]) portfolio_value:  0.9102\n",
      "Finished episode 17500 after 128 timesteps (reward:  0.0112 [-0.2222,  6.2426]) portfolio_value:  0.9412\n",
      "Finished episode 17600 after 128 timesteps (reward: -0.0449 [-0.1706,  0.7726]) portfolio_value:  0.9529\n",
      "Finished episode 17700 after 128 timesteps (reward: -0.0724 [-0.2267,  0.1364]) portfolio_value:  0.9644\n",
      "Finished episode 17800 after 128 timesteps (reward:  0.2246 [-0.2331,  10.3421]) portfolio_value:  0.9653\n",
      "Finished episode 17900 after 128 timesteps (reward: -0.0615 [-0.1510,  0.0481]) portfolio_value:  0.9450\n",
      "Finished episode 18000 after 128 timesteps (reward: -0.0441 [-0.2153,  1.8126]) portfolio_value:  0.9082\n",
      "Finished episode 18100 after 128 timesteps (reward: -0.0621 [-0.1718,  0.1206]) portfolio_value:  0.9772\n",
      "Finished episode 18200 after 128 timesteps (reward: -0.0533 [-0.2180,  0.2041]) portfolio_value:  0.9531\n",
      "Finished episode 18300 after 128 timesteps (reward:  0.0687 [-0.2590,  9.9698]) portfolio_value:  0.9258\n",
      "Finished episode 18400 after 128 timesteps (reward: -0.0646 [-0.2053,  0.1338]) portfolio_value:  0.9322\n",
      "Finished episode 18500 after 128 timesteps (reward:  0.1496 [-0.2104,  10.2488]) portfolio_value:  0.9333\n",
      "Finished episode 18600 after 128 timesteps (reward: -0.0689 [-0.2022,  0.2864]) portfolio_value:  1.1480\n",
      "Finished episode 18700 after 128 timesteps (reward:  0.0994 [-0.2041,  9.7572]) portfolio_value:  0.8776\n",
      "Finished episode 18800 after 128 timesteps (reward: -0.0866 [-0.2137,  0.1724]) portfolio_value:  0.9094\n",
      "Finished episode 18900 after 128 timesteps (reward:  0.1745 [-0.2181,  10.1823]) portfolio_value:  0.9135\n",
      "Finished episode 19000 after 128 timesteps (reward:  0.0430 [-0.2395,  10.0337]) portfolio_value:  0.9409\n",
      "Finished episode 19100 after 128 timesteps (reward: -0.0627 [-0.2183,  2.0927]) portfolio_value:  0.9177\n",
      "Finished episode 19200 after 128 timesteps (reward: -0.0618 [-0.2157,  2.8660]) portfolio_value:  0.9353\n",
      "Finished episode 19300 after 128 timesteps (reward: -0.0822 [-0.2172,  0.3508]) portfolio_value:  0.9397\n",
      "Finished episode 19400 after 128 timesteps (reward: -0.0000 [-0.3351,  6.3847]) portfolio_value:  0.8717\n",
      "Finished episode 19500 after 128 timesteps (reward:  0.0236 [-0.1715,  9.3948]) portfolio_value:  12025.8678\n",
      "Finished episode 19600 after 128 timesteps (reward: -0.0568 [-0.2421,  2.7129]) portfolio_value:  0.9345\n",
      "Finished episode 19700 after 128 timesteps (reward: -0.0686 [-0.2060,  0.1489]) portfolio_value:  1.0192\n",
      "Finished episode 19800 after 128 timesteps (reward:  0.0316 [-0.2044,  9.6424]) portfolio_value:  0.8631\n",
      "Finished episode 19900 after 128 timesteps (reward: -0.0768 [-0.2191,  0.1825]) portfolio_value:  0.9037\n",
      "Finished episode 20000 after 128 timesteps (reward: -0.0116 [-0.2150,  2.9094]) portfolio_value:  18.3453\n",
      "Finished episode 20100 after 128 timesteps (reward: -0.0698 [-0.2438,  1.8971]) portfolio_value:  0.8430\n",
      "Finished episode 20200 after 128 timesteps (reward:  0.2504 [-0.2038,  9.8207]) portfolio_value:  0.9344\n",
      "Finished episode 20300 after 128 timesteps (reward:  0.0768 [-0.4039,  10.1607]) portfolio_value:  0.8404\n",
      "Finished episode 20400 after 128 timesteps (reward: -0.0046 [-0.2414,  6.5501]) portfolio_value:  0.9199\n",
      "Finished episode 20500 after 128 timesteps (reward:  0.0820 [-0.2907,  6.9383]) portfolio_value:  0.9178\n",
      "Finished episode 20600 after 128 timesteps (reward: -0.0789 [-0.2095,  0.5282]) portfolio_value:  0.9152\n",
      "Finished episode 20700 after 128 timesteps (reward: -0.0836 [-0.2599,  0.2759]) portfolio_value:  1.0490\n",
      "Finished episode 20800 after 128 timesteps (reward: -0.0708 [-0.3346,  0.7847]) portfolio_value:  1.2198\n",
      "Finished episode 20900 after 128 timesteps (reward:  0.0206 [-0.4167,  9.8330]) portfolio_value:  0.8653\n",
      "Finished episode 21000 after 128 timesteps (reward: -0.0703 [-0.1852,  1.6099]) portfolio_value:  0.9383\n",
      "Finished episode 21100 after 128 timesteps (reward: -0.0335 [-0.2277,  6.4383]) portfolio_value:  0.9217\n",
      "Finished episode 21200 after 128 timesteps (reward: -0.0696 [-0.2267,  2.8157]) portfolio_value:  0.8867\n",
      "Finished episode 21300 after 128 timesteps (reward:  0.0152 [-0.2108,  9.6335]) portfolio_value:  0.8783\n",
      "Finished episode 21400 after 128 timesteps (reward: -0.0924 [-0.2657,  0.1990]) portfolio_value:  0.9019\n",
      "Finished episode 21500 after 128 timesteps (reward: -0.0969 [-0.3048,  0.2749]) portfolio_value:  0.9435\n",
      "Finished episode 21600 after 128 timesteps (reward: -0.0228 [-0.2761,  6.8419]) portfolio_value:  936.2517\n",
      "Finished episode 21700 after 128 timesteps (reward: -0.0256 [-0.2176,  7.2040]) portfolio_value:  0.8881\n",
      "Finished episode 21800 after 128 timesteps (reward: -0.0076 [-0.2206,  9.1057]) portfolio_value:  0.9277\n",
      "Finished episode 21900 after 128 timesteps (reward: -0.0861 [-0.2439,  0.3610]) portfolio_value:  0.9259\n",
      "Finished episode 22000 after 128 timesteps (reward: -0.0813 [-0.3107,  0.5396]) portfolio_value:  0.9636\n",
      "Finished episode 22100 after 128 timesteps (reward:  0.0335 [-0.2363,  9.0245]) portfolio_value:  0.8387\n",
      "Finished episode 22200 after 128 timesteps (reward: -0.0950 [-0.2861,  0.0953]) portfolio_value:  0.9273\n",
      "Finished episode 22300 after 128 timesteps (reward:  0.0039 [-0.2895,  9.7422]) portfolio_value:  0.8266\n",
      "Finished episode 22400 after 128 timesteps (reward: -0.0855 [-0.3399,  0.4147]) portfolio_value:  0.9056\n",
      "Finished episode 22500 after 128 timesteps (reward: -0.0007 [-0.2580,  9.4887]) portfolio_value:  0.9566\n",
      "Finished episode 22600 after 128 timesteps (reward: -0.0733 [-0.2658,  0.2759]) portfolio_value:  0.9243\n",
      "Finished episode 22700 after 128 timesteps (reward: -0.0317 [-0.1912,  2.6348]) portfolio_value:  0.9622\n",
      "Finished episode 22800 after 128 timesteps (reward: -0.0094 [-0.2495,  2.9644]) portfolio_value:  1.0033\n",
      "Finished episode 22900 after 128 timesteps (reward: -0.0754 [-0.2136,  0.1898]) portfolio_value:  0.8957\n",
      "Finished episode 23000 after 128 timesteps (reward: -0.0660 [-0.2488,  0.4823]) portfolio_value:  0.9049\n",
      "Finished episode 23100 after 128 timesteps (reward: -0.0541 [-0.1923,  1.9564]) portfolio_value:  0.9713\n",
      "Finished episode 23200 after 128 timesteps (reward: -0.0162 [-0.2219,  3.1999]) portfolio_value:  0.8897\n",
      "Finished episode 23300 after 128 timesteps (reward: -0.0099 [-0.2273,  6.3040]) portfolio_value:  0.9604\n",
      "Finished episode 23400 after 128 timesteps (reward: -0.0741 [-0.2388,  0.3423]) portfolio_value:  0.9802\n",
      "Finished episode 23500 after 128 timesteps (reward: -0.0628 [-0.1949,  0.1806]) portfolio_value:  0.9548\n",
      "Finished episode 23600 after 128 timesteps (reward: -0.0589 [-0.1982,  0.2444]) portfolio_value:  0.8743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 23700 after 128 timesteps (reward: -0.0037 [-0.2301,  2.9174]) portfolio_value:  0.9773\n",
      "Finished episode 23800 after 128 timesteps (reward:  0.0787 [-0.2581,  10.6802]) portfolio_value:  0.9820\n",
      "Finished episode 23900 after 128 timesteps (reward: -0.0643 [-0.2303,  0.2397]) portfolio_value:  0.9214\n",
      "Finished episode 24000 after 128 timesteps (reward:  0.1306 [-0.1972,  9.7999]) portfolio_value:  1.0619\n",
      "Finished episode 24100 after 128 timesteps (reward:  0.1119 [-0.1805,  9.7937]) portfolio_value:  1.0212\n",
      "Finished episode 24200 after 128 timesteps (reward: -0.0678 [-0.2066,  0.2152]) portfolio_value:  0.9856\n",
      "Finished episode 24300 after 128 timesteps (reward:  0.0591 [-0.2310,  10.3067]) portfolio_value:  0.9749\n",
      "Finished episode 24400 after 128 timesteps (reward:  0.0963 [-0.2124,  10.6423]) portfolio_value:  0.9247\n",
      "Finished episode 24500 after 128 timesteps (reward: -0.0663 [-0.2367,  0.3095]) portfolio_value:  0.9777\n",
      "Finished episode 24600 after 128 timesteps (reward: -0.0698 [-0.2194,  0.3299]) portfolio_value:  0.9401\n",
      "Finished episode 24700 after 128 timesteps (reward: -0.0205 [-0.1952,  2.1725]) portfolio_value:  0.9291\n",
      "Finished episode 24800 after 128 timesteps (reward: -0.0391 [-0.1966,  1.9957]) portfolio_value:  0.9310\n",
      "Finished episode 24900 after 128 timesteps (reward: -0.0615 [-0.2746,  0.4733]) portfolio_value:  0.7708\n",
      "Finished episode 25000 after 128 timesteps (reward: -0.0591 [-0.2184,  0.1799]) portfolio_value:  0.9486\n",
      "Finished episode 25100 after 128 timesteps (reward: -0.0467 [-0.1985,  0.3408]) portfolio_value:  1.1589\n",
      "Finished episode 25200 after 128 timesteps (reward: -0.0574 [-0.2662,  0.5367]) portfolio_value:  0.9770\n",
      "Finished episode 25300 after 128 timesteps (reward: -0.0384 [-0.1863,  1.9288]) portfolio_value:  0.9348\n",
      "Finished episode 25400 after 128 timesteps (reward: -0.0726 [-0.2371,  0.1781]) portfolio_value:  1.0115\n",
      "Finished episode 25500 after 128 timesteps (reward: -0.0288 [-0.1881,  2.1358]) portfolio_value:  0.9000\n",
      "Finished episode 25600 after 128 timesteps (reward:  0.1328 [-0.2294,  10.0272]) portfolio_value:  0.7950\n",
      "Finished episode 25700 after 128 timesteps (reward:  0.0468 [-0.2438,  10.6041]) portfolio_value:  0.9489\n",
      "Finished episode 25800 after 128 timesteps (reward:  0.0249 [-0.2006,  6.3496]) portfolio_value:  0.9521\n",
      "Finished episode 25900 after 128 timesteps (reward: -0.0675 [-0.2541,  0.2318]) portfolio_value:  1.0443\n",
      "Finished episode 26000 after 128 timesteps (reward: -0.0382 [-0.2920,  2.8968]) portfolio_value:  0.8577\n",
      "Finished episode 26100 after 128 timesteps (reward: -0.0568 [-0.1940,  0.3207]) portfolio_value:  0.8985\n",
      "Finished episode 26200 after 128 timesteps (reward: -0.0717 [-0.2246,  0.1381]) portfolio_value:  0.8860\n",
      "Finished episode 26300 after 128 timesteps (reward: -0.0529 [-0.1734,  0.4054]) portfolio_value:  0.9503\n",
      "Finished episode 26400 after 128 timesteps (reward:  0.0619 [-0.1919,  10.6036]) portfolio_value:  7.6828\n",
      "Finished episode 26500 after 128 timesteps (reward:  0.0054 [-0.2096,  6.5317]) portfolio_value:  0.9335\n",
      "Finished episode 26600 after 128 timesteps (reward:  0.0339 [-0.2242,  7.7442]) portfolio_value:  0.8442\n",
      "Finished episode 26700 after 128 timesteps (reward: -0.0481 [-0.2587,  2.0420]) portfolio_value:  0.9177\n",
      "Finished episode 26800 after 128 timesteps (reward:  0.0279 [-0.2422,  6.8113]) portfolio_value:  0.9374\n",
      "Finished episode 26900 after 128 timesteps (reward: -0.0742 [-0.2539,  0.3408]) portfolio_value:  0.9584\n",
      "Finished episode 27000 after 128 timesteps (reward: -0.0370 [-0.2756,  2.3992]) portfolio_value:  0.9104\n",
      "Finished episode 27100 after 128 timesteps (reward:  0.0767 [-0.1794,  10.4750]) portfolio_value:  0.9766\n",
      "Finished episode 27200 after 128 timesteps (reward: -0.0559 [-0.2289,  0.4812]) portfolio_value:  0.9656\n",
      "Finished episode 27300 after 128 timesteps (reward:  0.1147 [-0.1975,  9.8899]) portfolio_value:  0.8946\n",
      "Finished episode 27400 after 128 timesteps (reward: -0.0477 [-0.2654,  2.4786]) portfolio_value:  0.9489\n",
      "Finished episode 27500 after 128 timesteps (reward: -0.0738 [-0.1885,  0.1684]) portfolio_value:  0.8327\n",
      "Finished episode 27600 after 128 timesteps (reward: -0.0384 [-0.2105,  2.0215]) portfolio_value:  0.9189\n",
      "Finished episode 27700 after 128 timesteps (reward: -0.0760 [-0.2309,  0.5770]) portfolio_value:  1.0049\n",
      "Finished episode 27800 after 128 timesteps (reward: -0.0041 [-0.2327,  6.6596]) portfolio_value:  0.8652\n",
      "Finished episode 27900 after 128 timesteps (reward: -0.0449 [-0.2053,  1.9416]) portfolio_value:  0.8795\n",
      "Finished episode 28000 after 128 timesteps (reward: -0.0811 [-0.2911,  0.0998]) portfolio_value:  0.8918\n",
      "Finished episode 28100 after 128 timesteps (reward:  0.1578 [-0.2071,  9.2471]) portfolio_value:  0.9449\n",
      "Finished episode 28200 after 128 timesteps (reward: -0.0705 [-0.2854,  0.2351]) portfolio_value:  0.9857\n",
      "Finished episode 28300 after 128 timesteps (reward:  0.0103 [-0.2882,  6.8732]) portfolio_value:  0.9589\n",
      "Finished episode 28400 after 128 timesteps (reward: -0.0558 [-0.2977,  2.0211]) portfolio_value:  0.9239\n",
      "Finished episode 28500 after 128 timesteps (reward: -0.0461 [-0.2440,  1.6883]) portfolio_value:  0.9048\n",
      "Finished episode 28600 after 128 timesteps (reward:  0.0356 [-0.3082,  10.2701]) portfolio_value:  0.9071\n",
      "Finished episode 28700 after 128 timesteps (reward: -0.0633 [-0.2243,  0.3781]) portfolio_value:  0.8873\n",
      "Finished episode 28800 after 128 timesteps (reward:  0.2468 [-0.2407,  10.7406]) portfolio_value:  0.9353\n",
      "Finished episode 28900 after 128 timesteps (reward: -0.0750 [-0.2589,  0.1317]) portfolio_value:  0.8615\n",
      "Finished episode 29000 after 128 timesteps (reward:  0.0693 [-0.2180,  6.8484]) portfolio_value:  0.9728\n",
      "Finished episode 29100 after 128 timesteps (reward:  0.1847 [-0.2335,  10.1581]) portfolio_value:  0.9077\n",
      "Finished episode 29200 after 128 timesteps (reward: -0.0164 [-0.3096,  2.0024]) portfolio_value:  0.9292\n",
      "Finished episode 29300 after 128 timesteps (reward:  0.0125 [-0.2358,  6.6860]) portfolio_value:  0.9101\n",
      "Finished episode 29400 after 128 timesteps (reward: -0.0675 [-0.2701,  0.2266]) portfolio_value:  0.8692\n",
      "Finished episode 29500 after 128 timesteps (reward: -0.0260 [-0.2253,  2.2749]) portfolio_value:  0.9456\n",
      "Finished episode 29600 after 128 timesteps (reward:  0.2383 [-0.2189,  10.3001]) portfolio_value:  0.9076\n",
      "Finished episode 29700 after 128 timesteps (reward: -0.0683 [-0.2706,  0.3864]) portfolio_value:  0.8990\n",
      "Finished episode 29800 after 128 timesteps (reward: -0.0667 [-0.2049,  0.2837]) portfolio_value:  0.9395\n",
      "Finished episode 29900 after 128 timesteps (reward:  0.1177 [-0.1831,  6.5327]) portfolio_value:  0.9763\n",
      "Finished episode 30000 after 128 timesteps (reward:  0.1768 [-0.1867,  10.2599]) portfolio_value:  0.9580\n",
      "Finished episode 30100 after 128 timesteps (reward:  0.0118 [-0.2332,  6.5022]) portfolio_value:  0.9348\n",
      "Finished episode 30200 after 128 timesteps (reward:  0.0500 [-0.2323,  9.7038]) portfolio_value:  0.9635\n",
      "Finished episode 30300 after 128 timesteps (reward: -0.0595 [-0.2414,  0.2030]) portfolio_value:  0.9317\n",
      "Finished episode 30400 after 128 timesteps (reward: -0.0624 [-0.2212,  0.1510]) portfolio_value:  0.9643\n",
      "Finished episode 30500 after 128 timesteps (reward:  0.0426 [-0.2060,  10.1321]) portfolio_value:  0.9497\n",
      "Finished episode 30600 after 128 timesteps (reward: -0.0114 [-0.1656,  2.1132]) portfolio_value:  0.9414\n",
      "Finished episode 30700 after 128 timesteps (reward: -0.0551 [-0.2423,  0.5883]) portfolio_value:  0.9587\n",
      "Finished episode 30800 after 128 timesteps (reward: -0.0383 [-0.2125,  2.2272]) portfolio_value:  0.9875\n",
      "Finished episode 30900 after 128 timesteps (reward:  0.1450 [-0.2564,  10.8220]) portfolio_value:  0.9411\n",
      "Finished episode 31000 after 128 timesteps (reward:  0.0607 [-0.1792,  9.8119]) portfolio_value:  0.9727\n",
      "Finished episode 31100 after 128 timesteps (reward: -0.0361 [-0.2537,  2.5705]) portfolio_value:  1.1116\n",
      "Finished episode 31200 after 128 timesteps (reward: -0.0580 [-0.2129,  0.2477]) portfolio_value:  1.0008\n",
      "Finished episode 31300 after 128 timesteps (reward:  0.0053 [-0.2579,  6.5957]) portfolio_value:  0.9607\n",
      "Finished episode 31400 after 128 timesteps (reward: -0.0441 [-0.2417,  1.9023]) portfolio_value:  0.8800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 31500 after 128 timesteps (reward: -0.0169 [-0.2201,  2.1880]) portfolio_value:  0.9304\n",
      "Finished episode 31600 after 128 timesteps (reward:  0.1392 [-0.1843,  10.0764]) portfolio_value:  0.9312\n",
      "Finished episode 31700 after 128 timesteps (reward: -0.0606 [-0.2043,  0.3868]) portfolio_value:  0.9486\n",
      "Finished episode 31800 after 128 timesteps (reward: -0.0518 [-0.2449,  0.3028]) portfolio_value:  0.9872\n",
      "Finished episode 31900 after 128 timesteps (reward: -0.0610 [-0.2224,  0.1424]) portfolio_value:  0.9633\n",
      "Finished episode 32000 after 128 timesteps (reward:  0.0508 [-0.1997,  7.8416]) portfolio_value:  0.9180\n",
      "Finished episode 32100 after 128 timesteps (reward:  0.1824 [-0.2841,  10.2649]) portfolio_value:  1.0461\n",
      "Finished episode 32200 after 128 timesteps (reward: -0.0242 [-0.1888,  2.6837]) portfolio_value:  0.9509\n",
      "Finished episode 32300 after 128 timesteps (reward: -0.0343 [-0.1424,  0.5637]) portfolio_value:  0.8910\n",
      "Finished episode 32400 after 128 timesteps (reward: -0.0341 [-0.2231,  2.0608]) portfolio_value:  1.0248\n",
      "Finished episode 32500 after 128 timesteps (reward: -0.0433 [-0.1865,  0.4731]) portfolio_value:  0.8854\n",
      "Finished episode 32600 after 128 timesteps (reward:  0.0586 [-0.1653,  9.8247]) portfolio_value:  0.9105\n",
      "Finished episode 32700 after 128 timesteps (reward:  0.0577 [-0.1864,  6.4690]) portfolio_value:  1.0329\n",
      "Finished episode 32800 after 128 timesteps (reward:  0.0573 [-0.2211,  10.3554]) portfolio_value:  0.9589\n",
      "Finished episode 32900 after 128 timesteps (reward:  0.0704 [-0.1932,  10.3579]) portfolio_value:  0.9853\n",
      "Finished episode 33000 after 128 timesteps (reward:  0.0434 [-0.2022,  6.8086]) portfolio_value:  0.9744\n",
      "Finished episode 33100 after 128 timesteps (reward: -0.0445 [-0.2689,  0.2499]) portfolio_value:  0.9772\n",
      "Finished episode 33200 after 128 timesteps (reward: -0.0583 [-0.1946,  0.1784]) portfolio_value:  0.9391\n",
      "Finished episode 33300 after 128 timesteps (reward:  0.0659 [-0.2261,  10.0801]) portfolio_value:  0.9322\n",
      "Finished episode 33400 after 128 timesteps (reward:  0.0063 [-0.2306,  6.1941]) portfolio_value:  0.9833\n",
      "Finished episode 33500 after 128 timesteps (reward:  0.0199 [-0.1534,  6.4998]) portfolio_value:  0.9265\n",
      "Finished episode 33600 after 128 timesteps (reward: -0.0489 [-0.1727,  0.3687]) portfolio_value:  1.0080\n",
      "Finished episode 33700 after 128 timesteps (reward:  0.1341 [-0.1627,  9.9800]) portfolio_value:  0.9522\n",
      "Finished episode 33800 after 128 timesteps (reward: -0.0518 [-0.1915,  0.5306]) portfolio_value:  0.9138\n",
      "Finished episode 33900 after 128 timesteps (reward: -0.0472 [-0.2658,  0.2728]) portfolio_value:  0.8849\n",
      "Finished episode 34000 after 128 timesteps (reward: -0.0629 [-0.2013,  0.1331]) portfolio_value:  0.9428\n",
      "Finished episode 34100 after 128 timesteps (reward: -0.0135 [-0.2317,  2.1592]) portfolio_value:  0.9161\n",
      "Finished episode 34200 after 128 timesteps (reward:  0.0027 [-0.2454,  6.6078]) portfolio_value:  0.9321\n",
      "Finished episode 34300 after 128 timesteps (reward:  0.0579 [-0.1618,  10.8964]) portfolio_value:  0.9458\n",
      "Finished episode 34400 after 128 timesteps (reward: -0.0531 [-0.1572,  0.1096]) portfolio_value:  0.9342\n",
      "Finished episode 34500 after 128 timesteps (reward:  0.0392 [-0.2689,  10.1981]) portfolio_value:  0.9627\n",
      "Finished episode 34600 after 128 timesteps (reward:  0.0589 [-0.1807,  10.6667]) portfolio_value:  0.9158\n",
      "Finished episode 34700 after 128 timesteps (reward:  0.0301 [-0.2478,  6.0907]) portfolio_value:  1.0093\n",
      "Finished episode 34800 after 128 timesteps (reward:  0.1087 [-0.2375,  10.0036]) portfolio_value:  0.9035\n",
      "Finished episode 34900 after 128 timesteps (reward: -0.0030 [-0.3014,  6.3075]) portfolio_value:  0.9481\n",
      "Finished episode 35000 after 128 timesteps (reward: -0.0599 [-0.2302,  0.3002]) portfolio_value:  0.9034\n",
      "Finished episode 35100 after 128 timesteps (reward:  0.0761 [-0.3198,  11.2928]) portfolio_value:  0.9650\n",
      "Finished episode 35200 after 128 timesteps (reward: -0.0544 [-0.2255,  0.3631]) portfolio_value:  1.1260\n",
      "Finished episode 35300 after 128 timesteps (reward: -0.0575 [-0.2164,  0.2395]) portfolio_value:  0.8621\n",
      "Finished episode 35400 after 128 timesteps (reward:  0.0005 [-0.1880,  6.4790]) portfolio_value:  0.9961\n",
      "Finished episode 35500 after 128 timesteps (reward: -0.0022 [-0.3767,  2.9403]) portfolio_value:  0.9168\n",
      "Finished episode 35600 after 128 timesteps (reward:  0.0079 [-0.2045,  6.2870]) portfolio_value:  0.9547\n",
      "Finished episode 35700 after 128 timesteps (reward:  0.0046 [-0.2018,  5.9772]) portfolio_value:  0.9446\n",
      "Finished episode 35800 after 128 timesteps (reward:  0.0150 [-0.1722,  5.6088]) portfolio_value:  0.9135\n",
      "Finished episode 35900 after 128 timesteps (reward:  0.1594 [-0.1975,  10.8836]) portfolio_value:  0.9780\n",
      "Finished episode 36000 after 128 timesteps (reward: -0.0227 [-0.1447,  2.9880]) portfolio_value:  0.9330\n",
      "Finished episode 36100 after 128 timesteps (reward:  0.0189 [-0.2047,  6.0830]) portfolio_value:  0.9046\n",
      "Finished episode 36200 after 128 timesteps (reward: -0.0635 [-0.2484,  0.1424]) portfolio_value:  0.9377\n",
      "Finished episode 36300 after 128 timesteps (reward: -0.0622 [-0.1961,  0.1615]) portfolio_value:  0.9132\n",
      "Finished episode 36400 after 128 timesteps (reward: -0.0094 [-0.2104,  3.0168]) portfolio_value:  0.9401\n",
      "Finished episode 36500 after 128 timesteps (reward:  0.0068 [-0.3449,  6.1579]) portfolio_value:  0.9290\n",
      "Finished episode 36600 after 128 timesteps (reward: -0.0573 [-0.2296,  1.8101]) portfolio_value:  0.8844\n",
      "Finished episode 36700 after 128 timesteps (reward: -0.0242 [-0.1955,  3.2106]) portfolio_value:  0.9340\n",
      "Finished episode 36800 after 128 timesteps (reward:  0.0500 [-0.1878,  10.2898]) portfolio_value:  0.9969\n",
      "Finished episode 36900 after 128 timesteps (reward:  0.0687 [-0.2102,  10.8995]) portfolio_value:  1.0234\n",
      "Finished episode 37000 after 128 timesteps (reward: -0.0410 [-0.2652,  2.7790]) portfolio_value:  0.9000\n",
      "Finished episode 37100 after 128 timesteps (reward:  0.0313 [-0.2267,  6.2777]) portfolio_value:  0.8380\n",
      "Finished episode 37200 after 128 timesteps (reward: -0.0304 [-0.1783,  3.1879]) portfolio_value:  0.9233\n",
      "Finished episode 37300 after 128 timesteps (reward: -0.0573 [-0.2682,  1.9021]) portfolio_value:  0.8946\n",
      "Finished episode 37400 after 128 timesteps (reward:  0.0178 [-0.2461,  9.8986]) portfolio_value:  19901.8948\n",
      "Finished episode 37500 after 128 timesteps (reward: -0.0567 [-0.2102,  0.2787]) portfolio_value:  0.9998\n",
      "Finished episode 37600 after 128 timesteps (reward: -0.0403 [-0.2195,  3.0769]) portfolio_value:  0.9163\n",
      "Finished episode 37700 after 128 timesteps (reward:  0.0371 [-0.1953,  10.2529]) portfolio_value:  0.9086\n",
      "Finished episode 37800 after 128 timesteps (reward: -0.0451 [-0.1605,  0.6044]) portfolio_value:  0.9201\n",
      "Finished episode 37900 after 128 timesteps (reward: -0.0722 [-0.3094,  0.7186]) portfolio_value:  0.9379\n",
      "Finished episode 38000 after 128 timesteps (reward:  0.0106 [-0.2356,  6.1246]) portfolio_value:  0.8997\n",
      "Finished episode 38100 after 128 timesteps (reward:  0.1374 [-0.1837,  10.2372]) portfolio_value:  0.9589\n",
      "Finished episode 38200 after 128 timesteps (reward: -0.0128 [-0.2520,  5.7837]) portfolio_value:  0.9341\n",
      "Finished episode 38300 after 128 timesteps (reward: -0.0701 [-0.2136,  0.3224]) portfolio_value:  0.9313\n",
      "Finished episode 38400 after 128 timesteps (reward:  0.0256 [-0.2165,  5.8948]) portfolio_value:  0.8560\n",
      "Finished episode 38500 after 128 timesteps (reward:  0.1064 [-0.2611,  10.2693]) portfolio_value:  0.9330\n",
      "Finished episode 38600 after 128 timesteps (reward: -0.0021 [-0.3930,  6.5743]) portfolio_value:  0.9460\n",
      "Finished episode 38700 after 128 timesteps (reward:  0.0437 [-0.2462,  10.0273]) portfolio_value:  0.9455\n",
      "Finished episode 38800 after 128 timesteps (reward:  0.0176 [-0.2420,  10.0558]) portfolio_value:  0.8953\n",
      "Finished episode 38900 after 128 timesteps (reward: -0.0562 [-0.2110,  1.7394]) portfolio_value:  0.9069\n",
      "Finished episode 39000 after 128 timesteps (reward:  0.1185 [-0.2413,  10.6009]) portfolio_value:  226.2823\n",
      "Finished episode 39100 after 128 timesteps (reward:  0.0101 [-0.2148,  6.0048]) portfolio_value:  0.9196\n",
      "Finished episode 39200 after 128 timesteps (reward:  0.2557 [-0.2266,  10.6929]) portfolio_value:  0.9014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 39300 after 128 timesteps (reward: -0.0313 [-0.2140,  2.9956]) portfolio_value:  0.9097\n",
      "Finished episode 39400 after 128 timesteps (reward:  0.0210 [-0.1701,  5.9492]) portfolio_value:  1.0764\n",
      "Finished episode 39500 after 128 timesteps (reward:  0.0620 [-0.2022,  6.0985]) portfolio_value:  0.9509\n",
      "Finished episode 39600 after 128 timesteps (reward: -0.0539 [-0.2670,  0.1816]) portfolio_value:  0.9869\n",
      "Finished episode 39700 after 128 timesteps (reward: -0.0577 [-0.1985,  0.1957]) portfolio_value:  0.9258\n",
      "Finished episode 39800 after 128 timesteps (reward:  0.2214 [-0.2267,  11.2122]) portfolio_value:  0.9048\n",
      "Finished episode 39900 after 128 timesteps (reward:  0.0637 [-0.2336,  6.1916]) portfolio_value:  0.9032\n",
      "Finished episode 40000 after 128 timesteps (reward: -0.0633 [-0.1765,  0.2602]) portfolio_value:  0.9510\n",
      "Finished episode 40100 after 128 timesteps (reward:  0.1268 [-0.2109,  10.3349]) portfolio_value:  0.8718\n",
      "Finished episode 40200 after 128 timesteps (reward:  0.0022 [-0.2600,  4.5873]) portfolio_value:  0.8926\n",
      "Finished episode 40300 after 128 timesteps (reward: -0.0542 [-0.1884,  1.4602]) portfolio_value:  0.8823\n",
      "Finished episode 40400 after 128 timesteps (reward:  0.1088 [-0.2505,  10.3503]) portfolio_value:  0.9349\n",
      "Finished episode 40500 after 128 timesteps (reward:  0.0637 [-0.2878,  9.6697]) portfolio_value:  0.9196\n",
      "Finished episode 40600 after 128 timesteps (reward:  0.0124 [-0.3166,  6.4097]) portfolio_value:  0.8942\n",
      "Finished episode 40700 after 128 timesteps (reward: -0.0613 [-0.1826,  0.1826]) portfolio_value:  1.0256\n",
      "Finished episode 40800 after 128 timesteps (reward: -0.0691 [-0.2246,  0.2734]) portfolio_value:  0.8926\n",
      "Finished episode 40900 after 128 timesteps (reward: -0.0427 [-0.1827,  1.7901]) portfolio_value:  1.0354\n",
      "Finished episode 41000 after 128 timesteps (reward: -0.0243 [-0.2088,  3.1468]) portfolio_value:  0.9358\n",
      "Finished episode 41100 after 128 timesteps (reward:  0.0229 [-0.2088,  6.1015]) portfolio_value:  0.9327\n",
      "Finished episode 41200 after 128 timesteps (reward:  0.0639 [-0.1810,  9.9509]) portfolio_value:  0.9452\n",
      "Finished episode 41300 after 128 timesteps (reward: -0.0546 [-0.1907,  0.4022]) portfolio_value:  0.9422\n",
      "Finished episode 41400 after 128 timesteps (reward:  0.0787 [-0.1675,  10.4470]) portfolio_value:  0.9945\n",
      "Finished episode 41500 after 128 timesteps (reward:  0.0240 [-0.1898,  5.6177]) portfolio_value:  0.9278\n",
      "Finished episode 41600 after 128 timesteps (reward: -0.0742 [-0.2440,  0.1087]) portfolio_value:  0.9720\n",
      "Finished episode 41700 after 128 timesteps (reward:  0.0344 [-0.1908,  10.3891]) portfolio_value:  0.9588\n",
      "Finished episode 41800 after 128 timesteps (reward: -0.0604 [-0.1974,  0.3154]) portfolio_value:  0.8794\n",
      "Finished episode 41900 after 128 timesteps (reward:  0.0473 [-0.1772,  10.4499]) portfolio_value:  0.9234\n",
      "Finished episode 42000 after 128 timesteps (reward:  0.1495 [-0.1818,  10.1558]) portfolio_value:  0.9486\n",
      "Finished episode 42100 after 128 timesteps (reward: -0.0560 [-0.2151,  0.1536]) portfolio_value:  0.9930\n",
      "Finished episode 42200 after 128 timesteps (reward: -0.0516 [-0.1755,  0.2203]) portfolio_value:  0.9441\n",
      "Finished episode 42300 after 128 timesteps (reward:  0.1193 [-0.1993,  10.3559]) portfolio_value:  0.9571\n",
      "Finished episode 42400 after 128 timesteps (reward: -0.0667 [-0.2831,  0.0974]) portfolio_value:  0.9223\n",
      "Finished episode 42500 after 128 timesteps (reward:  0.0589 [-0.1682,  10.1972]) portfolio_value:  0.8747\n",
      "Finished episode 42600 after 128 timesteps (reward: -0.0121 [-0.1697,  2.8626]) portfolio_value:  0.9639\n",
      "Finished episode 42700 after 128 timesteps (reward: -0.0478 [-0.2322,  0.2086]) portfolio_value:  0.9099\n",
      "Finished episode 42800 after 128 timesteps (reward:  0.2617 [-0.2014,  10.6621]) portfolio_value:  0.9615\n",
      "Finished episode 42900 after 128 timesteps (reward:  0.0489 [-0.1929,  10.1778]) portfolio_value:  0.9143\n",
      "Finished episode 43000 after 128 timesteps (reward: -0.0501 [-0.1753,  0.1541]) portfolio_value:  0.9741\n",
      "Finished episode 43100 after 128 timesteps (reward: -0.0448 [-0.2003,  0.1320]) portfolio_value:  0.9277\n",
      "Finished episode 43200 after 128 timesteps (reward: -0.0523 [-0.1618,  0.0892]) portfolio_value:  0.9259\n",
      "Finished episode 43300 after 128 timesteps (reward: -0.0494 [-0.1538,  0.1893]) portfolio_value:  0.9116\n",
      "Finished episode 43400 after 128 timesteps (reward:  0.0897 [-0.1789,  10.3142]) portfolio_value:  30159.0548\n",
      "Finished episode 43500 after 128 timesteps (reward:  0.0544 [-0.2151,  9.8914]) portfolio_value:  0.9670\n",
      "Finished episode 43600 after 128 timesteps (reward: -0.0247 [-0.1379,  2.8733]) portfolio_value:  0.9433\n",
      "Finished episode 43700 after 128 timesteps (reward:  0.0131 [-0.1651,  6.1648]) portfolio_value:  0.9815\n",
      "Finished episode 43800 after 128 timesteps (reward:  0.0657 [-0.1941,  10.2949]) portfolio_value:  0.9274\n",
      "Finished episode 43900 after 128 timesteps (reward: -0.0475 [-0.2279,  0.1234]) portfolio_value:  0.9827\n",
      "Finished episode 44000 after 128 timesteps (reward:  0.0593 [-0.1769,  10.3122]) portfolio_value:  0.9104\n",
      "Finished episode 44100 after 128 timesteps (reward:  0.0275 [-0.1451,  5.7627]) portfolio_value:  0.9691\n",
      "Finished episode 44200 after 128 timesteps (reward: -0.0359 [-0.1871,  1.5321]) portfolio_value:  0.9284\n",
      "Finished episode 44300 after 128 timesteps (reward:  0.1566 [-0.1723,  10.2855]) portfolio_value:  1.0314\n",
      "Finished episode 44400 after 128 timesteps (reward: -0.0126 [-0.1793,  3.2622]) portfolio_value:  0.8921\n",
      "Finished episode 44500 after 128 timesteps (reward: -0.0309 [-0.1807,  1.5528]) portfolio_value:  0.9403\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e9de3f08d0b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m runner.run(\n\u001b[0;32m----> 2\u001b[0;31m     episodes=900000, max_timesteps=200, episode_finished=episode_finished)\n\u001b[0m",
      "\u001b[0;32m/media/isisilon/Data/linuxOpt/tensorforce/tensorforce/execution/runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, episodes, max_timesteps, episode_finished)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/linuxOpt/tensorforce/tensorforce/environments/openai_gym.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# some gym environments expect a list (f.i. Pendulum-v0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1495af273b5e>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/isisilon/.pyenv/versions/3.6.0/envs/jupyter3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/rl_keras_finance/portfolio-rl-jiang_2017/src/environments/portfolio.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# add dates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isisilon/.pyenv/versions/3.6.0/envs/jupyter3/lib/python3.6/site-packages/pandas/core/indexes/datetimelike.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_int\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isisilon/.pyenv/versions/3.6.0/envs/jupyter3/lib/python3.6/site-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_box_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_for_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runner.run(\n",
    "    episodes=900000, max_timesteps=200, episode_finished=episode_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T22:35:06.601218Z",
     "start_time": "2017-07-16T06:34:21.900110+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 10 after 128 timesteps (reward: -0.0305 [-0.1367,  0.0823]) portfolio_value:  0.9717 [ 0.8722  1.0857]\n",
      "Finished episode 20 after 128 timesteps (reward:  0.0000 [-0.1393,  0.1881]) portfolio_value:  1.0055 [ 0.8699  1.2070]\n",
      "Finished episode 30 after 128 timesteps (reward: -0.0473 [-0.1285, -0.0007]) portfolio_value:  0.9544 [ 0.8795  0.9993]\n",
      "Finished episode 40 after 128 timesteps (reward: -0.0712 [-0.2553,  0.0207]) portfolio_value:  0.9333 [ 0.7746  1.0209]\n",
      "Finished episode 50 after 128 timesteps (reward: -0.0003 [-0.1229,  0.2287]) portfolio_value:  1.0046 [ 0.8843  1.2570]\n",
      "Finished episode 60 after 128 timesteps (reward: -0.0200 [-0.0804,  0.1019]) portfolio_value:  0.9815 [ 0.9227  1.1073]\n",
      "Finished episode 70 after 128 timesteps (reward: -0.0592 [-0.1267,  0.0460]) portfolio_value:  0.9436 [ 0.8810  1.0471]\n",
      "Finished episode 80 after 128 timesteps (reward: -0.0313 [-0.1400,  0.1044]) portfolio_value:  0.9709 [ 0.8693  1.1100]\n",
      "Finished episode 90 after 128 timesteps (reward: -0.0111 [-0.1067,  0.0427]) portfolio_value:  0.9899 [ 0.8988  1.0436]\n",
      "Finished episode 100 after 128 timesteps (reward: -0.0400 [-0.1126,  0.0039]) portfolio_value:  0.9612 [ 0.8935  1.0039]\n"
     ]
    }
   ],
   "source": [
    "runner_test = Runner(agent=agent, environment=environment_test)\n",
    "runner_test.run(\n",
    "    episodes=100, max_timesteps=128, episode_finished=EpisodeFinished(10).episode_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T15:05:24.805273Z",
     "start_time": "2017-07-15T15:05:21.681Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Poll new state from client\n",
    "# state = environment.reset()\n",
    "\n",
    "# for i in range(10):\n",
    "#     # Get prediction from agent, execute\n",
    "#     action = agent.act(state=state)\n",
    "\n",
    "#     state, reward, done = environment.execute(action)\n",
    "\n",
    "#     # Add experience, agent automatically updates model according to batch size\n",
    "#     agent.observe(reward=reward, terminal=False)\n",
    "    \n",
    "#     a=np.array(list(action.values()))\n",
    "#     print(a, a.sum(),a.min(),a.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T22:32:07.581685Z",
     "start_time": "2017-07-16T06:31:07.329901+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/isisilon/.pyenv/versions/3.6.0/envs/jupyter3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m(29)\u001b[0;36m_amin\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     27 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 29 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     30 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     31 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/home/isisilon/.pyenv/versions/3.6.0/envs/jupyter3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m(2372)\u001b[0;36mamin\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   2370 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2371 \u001b[0;31m    return _methods._amin(a, axis=axis,\n",
      "\u001b[0m\u001b[0;32m-> 2372 \u001b[0;31m                          out=out, **kwargs)\n",
      "\u001b[0m\u001b[0;32m   2373 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   2374 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-63-b8b9589cd861>\u001b[0m(22)\u001b[0;36mepisode_finished\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     20 \u001b[0;31m                    \u001b[0mrewards_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_intv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0;31m                    \u001b[0mportfolio_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mportfolio_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_intv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 22 \u001b[0;31m                    \u001b[0mportfolio_value_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mportfolio_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_intv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m                    \u001b[0mportfolio_value_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mportfolio_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_intv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m                )\n",
      "\u001b[0m\n",
      "ipdb> self.portfolio_value\n",
      "*** AttributeError: 'EpisodeFinished' object has no attribute 'portfolio_value'\n",
      "ipdb> l\n",
      "\u001b[1;32m     17 \u001b[0m                    \u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     18 \u001b[0m                    \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_intv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     19 \u001b[0m                    \u001b[0mrewards_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_intv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     20 \u001b[0m                    \u001b[0mrewards_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_intv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     21 \u001b[0m                    \u001b[0mportfolio_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mportfolio_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_intv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 22 \u001b[0;31m                    \u001b[0mportfolio_value_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mportfolio_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_intv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     23 \u001b[0m                    \u001b[0mportfolio_value_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mportfolio_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_intv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     24 \u001b[0m                )\n",
      "\u001b[1;32m     25 \u001b[0m            )\n",
      "\u001b[1;32m     26 \u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "ipdb> print( r.environment.gym.sim.infos[-1])\n",
      "{'log_reward': 0.01143149661597161, 'portfolio_value': 0.99060096248542517, 'returns': array([ 1.        ,  1.04373831,  1.03125   ,  1.00095477,  0.99982759,\n",
      "        0.99714286]), 'rate_of_return': 0.011497087036856035, 'weights': array([ 0.13256454,  0.12632322,  0.2144572 ,  0.14042224,  0.21627754,\n",
      "        0.1699551 ], dtype=float32), 'cost': 0.0002871263081260535, 'index': Timestamp('2017-01-30 16:00:00', freq='30T'), 'steps': 127}\n",
      "ipdb> print( r.environment.gym.sim.infos[-1]['portfolio_value'])\n",
      "0.990600962485\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.portfolio_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
