{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:13:10.675092Z",
     "start_time": "2017-07-16T14:13:09.520365+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# numeric\n",
    "import quandl\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# util\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import time\n",
    "import tempfile\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:13:10.706445Z",
     "start_time": "2017-07-16T14:13:10.676727+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:13:10.743892Z",
     "start_time": "2017-07-16T14:13:10.709284+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:13:10.826545Z",
     "start_time": "2017-07-16T14:13:10.746387+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:13:10.848344Z",
     "start_time": "2017-07-16T14:13:10.828860+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:13:10.904558Z",
     "start_time": "2017-07-16T14:13:10.850006+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from rl.memory import  SequentialMemory, Memory\n",
    "from collections import deque\n",
    "\n",
    "class EnvWrapper(PortfolioEnv):\n",
    "    \"\"\"Wraps env to normalise and reshape action.\"\"\"\n",
    "    def __init__(self, window_length=50, *args, **kwargs):\n",
    "        self.memory = SequentialMemory(limit=window_length*2, window_length=window_length)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # also it puts it in a list\n",
    "        if isinstance(action, list):\n",
    "            action = action[0]\n",
    "        \n",
    "        # we have to normalise for some reason softmax wont work\n",
    "        if isinstance(action, dict):\n",
    "            action = np.abs(list(action.values()))\n",
    "            action /= action.sum()        \n",
    "        \n",
    "        return super().step(action) \n",
    "\n",
    "class MemoryWrapper(EnvWrapper):\n",
    "    \"\"\"Provides memory to env observations.\"\"\"\n",
    "    def __init__(self, window_length=50, *args, **kwargs):\n",
    "        self.memory = SequentialMemory(limit=window_length*2, window_length=window_length)\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = super().step(action)     \n",
    "        obs = np.array(self.memory.get_recent_state(obs))\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.memory.recent_terminals = deque(maxlen=window_length)\n",
    "        return super().reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:34:06.823632Z",
     "start_time": "2017-07-16T08:34:06.790201+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:13:11.506142Z",
     "start_time": "2017-07-16T14:13:10.906374+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2017-07-16 14:13:11,480] Making new env: CartPole-v0\n",
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2017-07-16 14:13:11,488] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_hdf('./data/poliniex_30m.hf',key='train')\n",
    "env = EnvWrapper(\n",
    "    window_length=window_length,\n",
    "    df=df_train,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.0005    \n",
    ")\n",
    "env.seed = 0   \n",
    "\n",
    "df_test = pd.read_hdf('./data/poliniex_30m.hf',key='test')\n",
    "env_test = EnvWrapper(\n",
    "    window_length=window_length,\n",
    "    df=df_test,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.00)\n",
    "env_test.seed = 0  \n",
    "\n",
    "from tensorforce.environments.openai_gym import OpenAIGym\n",
    "environment = OpenAIGym('CartPole-v0')\n",
    "environment.gym = env\n",
    "\n",
    "environment_test = OpenAIGym('CartPole-v0')\n",
    "environment_test.gym = env_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:41:21.116729Z",
     "start_time": "2017-07-16T12:41:21.086620+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:13:11.559528Z",
     "start_time": "2017-07-16T14:13:11.507656+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.8393800168e-05 False {'reward': -6.839380016796503e-05, 'log_return': -0.0020518140050389509, 'portfolio_value': 0.99795028907192707, 'returns': 0.99602074447926692, 'rate_of_return': -0.0020497106785853925, 'cost': 0.0020497106785853461, 'steps': 2}\n",
      "(5, 50, 3) (5, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "# check shapes\n",
    "obs1, reward, done, info=env.step(np.random.random(env.action_space.shape))\n",
    "print(reward, done, info)\n",
    "obs2 = env.reset()\n",
    "print(obs1.shape,obs2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:13:15.533748Z",
     "start_time": "2017-07-16T14:13:11.561029+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce import Configuration\n",
    "from tensorforce.agents import VPGAgent\n",
    "from tensorforce.core.networks import layered_network_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:13:15.771557Z",
     "start_time": "2017-07-16T14:13:15.535305+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/reinforceio/tensorforce/blob/0d07fadec03f76537a2431e17c51cd759d53b5e9/tensorforce/core/networks/layers.py#L90\n",
    "import tensorflow as tf\n",
    "from math import sqrt\n",
    "from tensorforce import util\n",
    "from tensorforce import TensorForceError\n",
    "\n",
    "def linear(x, size, bias=True, l2_regularization=0.0):\n",
    "    if util.rank(x) != 2:\n",
    "        raise TensorForceError('Invalid input rank for linear layer.')\n",
    "    with tf.variable_scope('linear'):\n",
    "        weights = tf.Variable(initial_value=tf.random_normal(shape=(x.get_shape()[1].value, size), stddev=sqrt(2.0 / (x.get_shape()[1].value + size))))\n",
    "        if l2_regularization > 0.0:\n",
    "            tf.losses.add_loss(l2_regularization * tf.nn.l2_loss(t=weights))\n",
    "        x = tf.matmul(a=x, b=weights)\n",
    "        if bias:\n",
    "            bias = tf.Variable(initial_value=tf.zeros(shape=(size,)))\n",
    "            if l2_regularization > 0.0:\n",
    "                tf.losses.add_loss(l2_regularization * tf.nn.l2_loss(t=bias))\n",
    "            x = tf.nn.bias_add(value=x, bias=bias)\n",
    "    return x\n",
    "\n",
    "def nonlinearity(x, name='relu'):\n",
    "    with tf.variable_scope('nonlinearity'):\n",
    "        if name == 'elu':\n",
    "            x = tf.nn.elu(features=x)\n",
    "        elif name == 'relu':\n",
    "            x = tf.nn.relu(features=x)\n",
    "        elif name == 'selu':\n",
    "            # https://arxiv.org/pdf/1706.02515.pdf\n",
    "            alpha = 1.6732632423543772848170429916717\n",
    "            scale = 1.0507009873554804934193349852946\n",
    "            negative = alpha * tf.nn.elu(features=x)\n",
    "            x = scale * tf.where(condition=(x >= 0.0), x=x, y=negative)\n",
    "        elif name == 'sigmoid':\n",
    "            x = tf.sigmoid(x=x)\n",
    "        elif name == 'softmax':\n",
    "            x = tf.nn.softmax(logits=x)\n",
    "        elif name == 'tanh':\n",
    "            x = tf.nn.tanh(x=x)\n",
    "        else:\n",
    "            raise TensorForceError('Invalid nonlinearity.')\n",
    "    return x\n",
    "\n",
    "def dense(x, size, bias=True, activation='relu', l2_regularization=0.0):\n",
    "    if util.rank(x) != 2:\n",
    "        raise TensorForceError('Invalid input rank for dense layer.')\n",
    "    with tf.variable_scope('dense'):\n",
    "        x = linear(x=x, size=size, bias=bias, l2_regularization=l2_regularization)\n",
    "        x = nonlinearity(x=x, name=activation)\n",
    "    return x\n",
    "\n",
    "def flatten(x):\n",
    "    with tf.variable_scope('flatten'):\n",
    "        x = tf.reshape(tensor=x, shape=(-1, util.prod(x.get_shape().as_list()[1:])))\n",
    "    return x\n",
    "\n",
    "def conv2d(x, size, window=(3,3), stride=(1,1), bias=False, activation='relu', l2_regularization=0.0, padding='SAME'):\n",
    "    if util.rank(x) != 4:\n",
    "        raise TensorForceError('Invalid input rank for conv2d layer.')\n",
    "    with tf.variable_scope('conv2d'):\n",
    "        filters = tf.Variable(initial_value=tf.random_normal(shape=(window[0], window[1], x.get_shape()[3].value, size), stddev=sqrt(2.0 / size)))\n",
    "        if l2_regularization > 0.0:\n",
    "            tf.losses.add_loss(l2_regularization * tf.nn.l2_loss(t=filters))\n",
    "        x = tf.nn.conv2d(input=x, filter=filters, strides=(1, stride[0], stride[1], 1), padding=padding)\n",
    "        if bias:\n",
    "            bias = tf.Variable(initial_value=tf.zeros(shape=(size,)))\n",
    "            if l2_regularization > 0.0:\n",
    "                tf.losses.add_loss(l2_regularization * tf.nn.l2_loss(t=bias))\n",
    "            x = tf.nn.bias_add(value=x, bias=bias)\n",
    "        x = nonlinearity(x=x, name=activation)\n",
    "    return x\n",
    "\n",
    "def network_builder(inputs):\n",
    "    if len(inputs) != 1:\n",
    "        raise TensorForceError('Layered network must have only one input.')\n",
    "    x = next(iter(inputs.values()))\n",
    "    \n",
    "    x = conv2d(x=x, size=2, window=(1,3), bias=True, activation='relu', l2_regularization=1e-8, padding='VALID')\n",
    "    x = conv2d(x=x, size=20, window=(1,window_length-2), bias=True, activation='relu', l2_regularization=1e-8, padding='VALID')\n",
    "    x = conv2d(x=x, size=1, window=(1,1), bias=True, activation='relu', l2_regularization=1e-8, padding='VALID')\n",
    "    x = flatten(x)\n",
    "    x = dense(x, size=env.action_space.shape[0],activation='relu', l2_regularization=1e-8)\n",
    "    x = nonlinearity(x,name='softmax')\n",
    "    \n",
    "    return x\n",
    "network=network_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:13:17.342036Z",
     "start_time": "2017-07-16T14:13:15.773267+08:00"
    }
   },
   "outputs": [],
   "source": [
    "config = Configuration(   \n",
    "    # Each agent requires the following ``Configuration`` parameters:\n",
    "    network=network,\n",
    "    states=dict(shape=tuple(env.observation_space.shape), type='float'),\n",
    "    actions={'action' + str(n): dict(continuous=True) for n in range(env.action_space.shape[0])},\n",
    "    preprocessing = None,# dict or list containing state preprocessing configuration.\n",
    "    exploration = dict(\n",
    "        type='EpsilonDecay',\n",
    "        kwargs=dict(epsilon=1, epsilon_final=0.01, epsilon_timesteps=1e4)\n",
    "    ),\n",
    "\n",
    "    # The `BatchAgent` class additionally requires the following parameters:\n",
    "    batch_size = 50,# integer of the batch size.\n",
    "\n",
    "    # A Policy Gradient Model expects the following additional configuration parameters:\n",
    "    sample_actions= True,# boolean of whether to sample actions.\n",
    "#     baseline='mlp' ,# string indicating the baseline value function (currently 'linear' or 'mlp').\n",
    "#     baseline_args=dict(size=100, repeat_update=100) ,# list of arguments for the baseline value function.\n",
    "    override_line_search=False,\n",
    "    \n",
    "#     baseline_kwargs= ,# dict of keyword arguments for the baseline value function.\n",
    "    generalized_advantage_estimation= True ,# boolean indicating whether to use GAE.\n",
    "    gae_lambda= 0.97,# float of the Generalized Advantage Estimation lambda.\n",
    "    normalize_advantage= False,# boolean indicating whether to normalize the advantage or not.\n",
    "    cg_iterations=20,\n",
    "    max_kl_divergence=0.005,\n",
    "    cg_damping=0.001,\n",
    "    line_search_steps=20,\n",
    "    loglevel=\"info\",\n",
    ")\n",
    "\n",
    "# Create a Trust Region Policy Optimization agent\n",
    "agent = VPGAgent(config=config)\n",
    "\n",
    "# for some reason these are not set?\n",
    "# agent.next_internal = agent.current_internal = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:13:17.486214Z",
     "start_time": "2017-07-16T14:13:17.343308+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from tensorforce.execution import Runner\n",
    "runner = Runner(agent=agent, environment=environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T06:13:09.479Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Callback function printing episode statistics\n",
    "class EpisodeFinished(object):\n",
    "    \"\"\"Logger callback for tensorforce runner.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_intv):\n",
    "        self.log_intv = log_intv\n",
    "        self.portfolio_values = [] \n",
    "    \n",
    "    def __call__(self, r):\n",
    "        if len(r.environment.gym.sim.infos):\n",
    "            self.portfolio_values.append( r.environment.gym.sim.infos[-1]['portfolio_value'] )\n",
    "        if r.episode % self.log_intv == 0:\n",
    "#             df = pd.DataFrame(r.environment.gym.infos)\n",
    "            print(\n",
    "                \"Finished episode {ep} after {ts} timesteps (reward: {reward: 2.4f} [{rewards_min: 2.4f}, {rewards_max: 2.4f}]) portfolio_value: {portfolio_value: 2.4f} [{portfolio_value_min: 2.4f}, {portfolio_value_max: 2.4f}]\".\n",
    "                format(\n",
    "                    ep=r.episode,\n",
    "                    ts=r.timestep,\n",
    "                    reward=np.mean(r.episode_rewards[-self.log_intv:]),\n",
    "                    rewards_min=np.min(r.episode_rewards[-self.log_intv:]),\n",
    "                    rewards_max=np.max(r.episode_rewards[-self.log_intv:]),\n",
    "                    portfolio_value=np.mean(self.portfolio_values[-self.log_intv:]),\n",
    "                    portfolio_value_min=np.min(self.portfolio_values[-self.log_intv:]),\n",
    "                    portfolio_value_max=np.max(self.portfolio_values[-self.log_intv:])\n",
    "                )\n",
    "            )\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T06:13:09.481Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0023]) portfolio_value:  0.9993 [ 0.9273,  1.0725]\n",
      "Finished episode 2000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0038]) portfolio_value:  0.9996 [ 0.9228,  1.1196]\n",
      "Finished episode 3000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0031]) portfolio_value:  0.9988 [ 0.9267,  1.0970]\n",
      "Finished episode 4000 after 30 timesteps (reward: -0.0001 [-0.0032,  0.0036]) portfolio_value:  0.9983 [ 0.9073,  1.1124]\n",
      "Finished episode 5000 after 30 timesteps (reward: -0.0000 [-0.0020,  0.0035]) portfolio_value:  0.9995 [ 0.9415,  1.1103]\n",
      "Finished episode 6000 after 30 timesteps (reward: -0.0000 [-0.0031,  0.0033]) portfolio_value:  0.9990 [ 0.9125,  1.1029]\n",
      "Finished episode 7000 after 30 timesteps (reward: -0.0000 [-0.0031,  0.0025]) portfolio_value:  0.9995 [ 0.9102,  1.0789]\n",
      "Finished episode 8000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0030]) portfolio_value:  0.9991 [ 0.9148,  1.0937]\n",
      "Finished episode 9000 after 30 timesteps (reward: -0.0000 [-0.0054,  0.0028]) portfolio_value:  0.9996 [ 0.8493,  1.0872]\n",
      "Finished episode 10000 after 30 timesteps (reward: -0.0001 [-0.0036,  0.0035]) portfolio_value:  0.9985 [ 0.8989,  1.1104]\n",
      "Finished episode 11000 after 30 timesteps (reward: -0.0000 [-0.0042,  0.0029]) portfolio_value:  0.9996 [ 0.8813,  1.0895]\n",
      "Finished episode 12000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0040]) portfolio_value:  0.9989 [ 0.9177,  1.1274]\n",
      "Finished episode 13000 after 30 timesteps (reward: -0.0000 [-0.0046,  0.0026]) portfolio_value:  0.9988 [ 0.8708,  1.0801]\n",
      "Finished episode 14000 after 30 timesteps (reward: -0.0001 [-0.0079,  0.0026]) portfolio_value:  0.9986 [ 0.7891,  1.0808]\n",
      "Finished episode 15000 after 30 timesteps (reward: -0.0000 [-0.0032,  0.0027]) portfolio_value:  0.9994 [ 0.9078,  1.0860]\n",
      "Finished episode 16000 after 30 timesteps (reward: -0.0000 [-0.0041,  0.0031]) portfolio_value:  1.0002 [ 0.8841,  1.0986]\n",
      "Finished episode 17000 after 30 timesteps (reward: -0.0000 [-0.0024,  0.0022]) portfolio_value:  0.9995 [ 0.9292,  1.0692]\n",
      "Finished episode 18000 after 30 timesteps (reward: -0.0000 [-0.0020,  0.0038]) portfolio_value:  0.9998 [ 0.9408,  1.1193]\n",
      "Finished episode 19000 after 30 timesteps (reward: -0.0001 [-0.0027,  0.0034]) portfolio_value:  0.9981 [ 0.9232,  1.1069]\n",
      "Finished episode 20000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0028]) portfolio_value:  0.9992 [ 0.9251,  1.0868]\n",
      "Finished episode 21000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0022]) portfolio_value:  0.9994 [ 0.9157,  1.0696]\n",
      "Finished episode 22000 after 30 timesteps (reward: -0.0000 [-0.0051,  0.0024]) portfolio_value:  0.9991 [ 0.8593,  1.0754]\n",
      "Finished episode 23000 after 30 timesteps (reward: -0.0000 [-0.0067,  0.0043]) portfolio_value:  0.9991 [ 0.8171,  1.1376]\n",
      "Finished episode 24000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0028]) portfolio_value:  0.9994 [ 0.9183,  1.0888]\n",
      "Finished episode 25000 after 30 timesteps (reward: -0.0000 [-0.0042,  0.0034]) portfolio_value:  0.9995 [ 0.8829,  1.1064]\n",
      "Finished episode 26000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0031]) portfolio_value:  0.9994 [ 0.9225,  1.0983]\n",
      "Finished episode 27000 after 30 timesteps (reward: -0.0000 [-0.0031,  0.0032]) portfolio_value:  0.9993 [ 0.9112,  1.1005]\n",
      "Finished episode 28000 after 30 timesteps (reward: -0.0000 [-0.0021,  0.0031]) portfolio_value:  0.9988 [ 0.9400,  1.0961]\n",
      "Finished episode 29000 after 30 timesteps (reward: -0.0000 [-0.0060,  0.0029]) portfolio_value:  0.9999 [ 0.8344,  1.0907]\n",
      "Finished episode 30000 after 30 timesteps (reward: -0.0001 [-0.0035,  0.0029]) portfolio_value:  0.9986 [ 0.8992,  1.0904]\n",
      "Finished episode 31000 after 30 timesteps (reward: -0.0000 [-0.0024,  0.0027]) portfolio_value:  0.9987 [ 0.9318,  1.0835]\n",
      "Finished episode 32000 after 30 timesteps (reward: -0.0000 [-0.0045,  0.0023]) portfolio_value:  0.9996 [ 0.8740,  1.0711]\n",
      "Finished episode 33000 after 30 timesteps (reward:  0.0000 [-0.0032,  0.0038]) portfolio_value:  1.0004 [ 0.9080,  1.1191]\n",
      "Finished episode 34000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0025]) portfolio_value:  0.9988 [ 0.9153,  1.0782]\n",
      "Finished episode 35000 after 30 timesteps (reward: -0.0001 [-0.0032,  0.0039]) portfolio_value:  0.9981 [ 0.9085,  1.1257]\n",
      "Finished episode 36000 after 30 timesteps (reward: -0.0001 [-0.0018,  0.0030]) portfolio_value:  0.9986 [ 0.9471,  1.0947]\n",
      "Finished episode 37000 after 30 timesteps (reward: -0.0000 [-0.0033,  0.0026]) portfolio_value:  0.9989 [ 0.9069,  1.0821]\n",
      "Finished episode 38000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0036]) portfolio_value:  0.9992 [ 0.9210,  1.1148]\n",
      "Finished episode 39000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0027]) portfolio_value:  0.9996 [ 0.9278,  1.0827]\n",
      "Finished episode 40000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0032]) portfolio_value:  0.9988 [ 0.9156,  1.1000]\n",
      "Finished episode 41000 after 30 timesteps (reward: -0.0000 [-0.0046,  0.0027]) portfolio_value:  0.9987 [ 0.8700,  1.0849]\n",
      "Finished episode 42000 after 30 timesteps (reward: -0.0001 [-0.0039,  0.0035]) portfolio_value:  0.9985 [ 0.8888,  1.1120]\n",
      "Finished episode 43000 after 30 timesteps (reward: -0.0000 [-0.0022,  0.0030]) portfolio_value:  0.9992 [ 0.9375,  1.0935]\n",
      "Finished episode 44000 after 30 timesteps (reward: -0.0000 [-0.0032,  0.0028]) portfolio_value:  0.9996 [ 0.9074,  1.0873]\n",
      "Finished episode 45000 after 30 timesteps (reward:  0.0000 [-0.0019,  0.0032]) portfolio_value:  1.0002 [ 0.9446,  1.1015]\n",
      "Finished episode 46000 after 30 timesteps (reward: -0.0001 [-0.0050,  0.0033]) portfolio_value:  0.9987 [ 0.8616,  1.1025]\n",
      "Finished episode 47000 after 30 timesteps (reward: -0.0000 [-0.0039,  0.0028]) portfolio_value:  0.9994 [ 0.8888,  1.0863]\n",
      "Finished episode 48000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0031]) portfolio_value:  0.9989 [ 0.9281,  1.0990]\n",
      "Finished episode 49000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0033]) portfolio_value:  1.0000 [ 0.9150,  1.1054]\n",
      "Finished episode 50000 after 30 timesteps (reward: -0.0000 [-0.0031,  0.0032]) portfolio_value:  0.9990 [ 0.9101,  1.1019]\n",
      "Finished episode 51000 after 30 timesteps (reward: -0.0000 [-0.0081,  0.0039]) portfolio_value:  0.9999 [ 0.7848,  1.1226]\n",
      "Finished episode 52000 after 30 timesteps (reward: -0.0001 [-0.0036,  0.0024]) portfolio_value:  0.9986 [ 0.8986,  1.0735]\n",
      "Finished episode 53000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0024]) portfolio_value:  1.0001 [ 0.9291,  1.0751]\n",
      "Finished episode 54000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0037]) portfolio_value:  1.0001 [ 0.9230,  1.1176]\n",
      "Finished episode 55000 after 30 timesteps (reward: -0.0000 [-0.0053,  0.0040]) portfolio_value:  0.9991 [ 0.8537,  1.1271]\n",
      "Finished episode 56000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0023]) portfolio_value:  0.9990 [ 0.9255,  1.0708]\n",
      "Finished episode 57000 after 30 timesteps (reward: -0.0001 [-0.0031,  0.0030]) portfolio_value:  0.9987 [ 0.9125,  1.0958]\n",
      "Finished episode 58000 after 30 timesteps (reward: -0.0001 [-0.0068,  0.0032]) portfolio_value:  0.9986 [ 0.8165,  1.0993]\n",
      "Finished episode 59000 after 30 timesteps (reward: -0.0001 [-0.0065,  0.0021]) portfolio_value:  0.9980 [ 0.8223,  1.0648]\n",
      "Finished episode 60000 after 30 timesteps (reward: -0.0000 [-0.0060,  0.0026]) portfolio_value:  0.9994 [ 0.8345,  1.0800]\n",
      "Finished episode 61000 after 30 timesteps (reward: -0.0000 [-0.0031,  0.0027]) portfolio_value:  0.9990 [ 0.9110,  1.0854]\n",
      "Finished episode 62000 after 30 timesteps (reward: -0.0001 [-0.0025,  0.0037]) portfolio_value:  0.9987 [ 0.9285,  1.1166]\n",
      "Finished episode 63000 after 30 timesteps (reward: -0.0000 [-0.0036,  0.0031]) portfolio_value:  0.9993 [ 0.8981,  1.0973]\n",
      "Finished episode 64000 after 30 timesteps (reward: -0.0000 [-0.0022,  0.0036]) portfolio_value:  0.9990 [ 0.9356,  1.1139]\n",
      "Finished episode 65000 after 30 timesteps (reward:  0.0000 [-0.0029,  0.0032]) portfolio_value:  1.0002 [ 0.9177,  1.1004]\n",
      "Finished episode 66000 after 30 timesteps (reward: -0.0000 [-0.0036,  0.0025]) portfolio_value:  0.9995 [ 0.8968,  1.0775]\n",
      "Finished episode 67000 after 30 timesteps (reward: -0.0000 [-0.0023,  0.0023]) portfolio_value:  0.9992 [ 0.9331,  1.0716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 68000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0035]) portfolio_value:  0.9997 [ 0.9234,  1.1103]\n",
      "Finished episode 69000 after 30 timesteps (reward: -0.0000 [-0.0056,  0.0037]) portfolio_value:  0.9993 [ 0.8450,  1.1189]\n",
      "Finished episode 70000 after 30 timesteps (reward: -0.0001 [-0.0032,  0.0027]) portfolio_value:  0.9984 [ 0.9097,  1.0837]\n",
      "Finished episode 71000 after 30 timesteps (reward: -0.0000 [-0.0064,  0.0022]) portfolio_value:  0.9990 [ 0.8253,  1.0668]\n",
      "Finished episode 72000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0033]) portfolio_value:  0.9988 [ 0.9199,  1.1036]\n",
      "Finished episode 73000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0029]) portfolio_value:  0.9989 [ 0.9230,  1.0918]\n",
      "Finished episode 74000 after 30 timesteps (reward: -0.0000 [-0.0040,  0.0025]) portfolio_value:  0.9995 [ 0.8876,  1.0789]\n",
      "Finished episode 75000 after 30 timesteps (reward: -0.0000 [-0.0032,  0.0026]) portfolio_value:  0.9994 [ 0.9092,  1.0801]\n",
      "Finished episode 76000 after 30 timesteps (reward: -0.0000 [-0.0023,  0.0021]) portfolio_value:  0.9995 [ 0.9345,  1.0661]\n",
      "Finished episode 77000 after 30 timesteps (reward: -0.0000 [-0.0031,  0.0026]) portfolio_value:  0.9998 [ 0.9102,  1.0796]\n",
      "Finished episode 78000 after 30 timesteps (reward:  0.0000 [-0.0054,  0.0028]) portfolio_value:  1.0006 [ 0.8511,  1.0875]\n",
      "Finished episode 79000 after 30 timesteps (reward: -0.0000 [-0.0019,  0.0020]) portfolio_value:  0.9989 [ 0.9459,  1.0609]\n",
      "Finished episode 80000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0025]) portfolio_value:  0.9996 [ 0.9247,  1.0777]\n",
      "Finished episode 81000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0027]) portfolio_value:  0.9995 [ 0.9145,  1.0844]\n",
      "Finished episode 82000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0021]) portfolio_value:  0.9989 [ 0.9174,  1.0652]\n",
      "Finished episode 83000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0023]) portfolio_value:  0.9992 [ 0.9200,  1.0707]\n",
      "Finished episode 84000 after 30 timesteps (reward: -0.0000 [-0.0022,  0.0024]) portfolio_value:  0.9987 [ 0.9359,  1.0747]\n",
      "Finished episode 85000 after 30 timesteps (reward: -0.0000 [-0.0049,  0.0026]) portfolio_value:  0.9989 [ 0.8636,  1.0805]\n",
      "Finished episode 86000 after 30 timesteps (reward: -0.0000 [-0.0059,  0.0025]) portfolio_value:  0.9989 [ 0.8382,  1.0769]\n",
      "Finished episode 87000 after 30 timesteps (reward: -0.0000 [-0.0042,  0.0026]) portfolio_value:  0.9992 [ 0.8806,  1.0797]\n",
      "Finished episode 88000 after 30 timesteps (reward: -0.0000 [-0.0023,  0.0032]) portfolio_value:  0.9997 [ 0.9340,  1.0992]\n",
      "Finished episode 89000 after 30 timesteps (reward: -0.0000 [-0.0031,  0.0029]) portfolio_value:  0.9997 [ 0.9101,  1.0913]\n",
      "Finished episode 90000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0030]) portfolio_value:  0.9991 [ 0.9153,  1.0926]\n",
      "Finished episode 91000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0021]) portfolio_value:  0.9995 [ 0.9291,  1.0643]\n",
      "Finished episode 92000 after 30 timesteps (reward: -0.0000 [-0.0059,  0.0038]) portfolio_value:  0.9999 [ 0.8366,  1.1222]\n",
      "Finished episode 93000 after 30 timesteps (reward: -0.0000 [-0.0050,  0.0029]) portfolio_value:  0.9998 [ 0.8599,  1.0902]\n",
      "Finished episode 94000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0030]) portfolio_value:  1.0000 [ 0.9148,  1.0945]\n",
      "Finished episode 95000 after 30 timesteps (reward: -0.0001 [-0.0035,  0.0025]) portfolio_value:  0.9978 [ 0.9016,  1.0764]\n",
      "Finished episode 96000 after 30 timesteps (reward: -0.0001 [-0.0055,  0.0026]) portfolio_value:  0.9985 [ 0.8479,  1.0815]\n",
      "Finished episode 97000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0031]) portfolio_value:  0.9993 [ 0.9165,  1.0980]\n",
      "Finished episode 98000 after 30 timesteps (reward: -0.0000 [-0.0023,  0.0030]) portfolio_value:  0.9994 [ 0.9321,  1.0933]\n",
      "Finished episode 99000 after 30 timesteps (reward: -0.0000 [-0.0022,  0.0030]) portfolio_value:  0.9993 [ 0.9362,  1.0952]\n",
      "Finished episode 100000 after 30 timesteps (reward: -0.0000 [-0.0037,  0.0032]) portfolio_value:  0.9992 [ 0.8962,  1.1014]\n",
      "Finished episode 101000 after 30 timesteps (reward: -0.0001 [-0.0029,  0.0025]) portfolio_value:  0.9983 [ 0.9176,  1.0766]\n",
      "Finished episode 102000 after 30 timesteps (reward: -0.0001 [-0.0029,  0.0027]) portfolio_value:  0.9984 [ 0.9169,  1.0848]\n",
      "Finished episode 103000 after 30 timesteps (reward: -0.0000 [-0.0043,  0.0056]) portfolio_value:  0.9992 [ 0.8800,  1.1819]\n",
      "Finished episode 104000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0025]) portfolio_value:  0.9995 [ 0.9260,  1.0776]\n",
      "Finished episode 105000 after 30 timesteps (reward: -0.0001 [-0.0032,  0.0021]) portfolio_value:  0.9984 [ 0.9079,  1.0639]\n",
      "Finished episode 106000 after 30 timesteps (reward: -0.0000 [-0.0033,  0.0023]) portfolio_value:  1.0001 [ 0.9070,  1.0726]\n",
      "Finished episode 107000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0028]) portfolio_value:  0.9991 [ 0.9153,  1.0881]\n",
      "Finished episode 108000 after 30 timesteps (reward: -0.0001 [-0.0076,  0.0019]) portfolio_value:  0.9980 [ 0.7972,  1.0575]\n",
      "Finished episode 109000 after 30 timesteps (reward: -0.0000 [-0.0052,  0.0039]) portfolio_value:  0.9999 [ 0.8562,  1.1232]\n",
      "Finished episode 110000 after 30 timesteps (reward: -0.0000 [-0.0057,  0.0037]) portfolio_value:  0.9989 [ 0.8419,  1.1164]\n",
      "Finished episode 111000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0032]) portfolio_value:  0.9992 [ 0.9187,  1.1007]\n",
      "Finished episode 112000 after 30 timesteps (reward: -0.0000 [-0.0041,  0.0031]) portfolio_value:  0.9999 [ 0.8847,  1.0969]\n",
      "Finished episode 113000 after 30 timesteps (reward: -0.0000 [-0.0024,  0.0022]) portfolio_value:  0.9999 [ 0.9296,  1.0682]\n",
      "Finished episode 114000 after 30 timesteps (reward: -0.0000 [-0.0023,  0.0031]) portfolio_value:  0.9987 [ 0.9334,  1.0980]\n",
      "Finished episode 115000 after 30 timesteps (reward: -0.0000 [-0.0033,  0.0034]) portfolio_value:  0.9990 [ 0.9070,  1.1089]\n",
      "Finished episode 116000 after 30 timesteps (reward: -0.0001 [-0.0033,  0.0032]) portfolio_value:  0.9984 [ 0.9064,  1.1008]\n",
      "Finished episode 117000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0027]) portfolio_value:  0.9992 [ 0.9130,  1.0846]\n",
      "Finished episode 118000 after 30 timesteps (reward: -0.0001 [-0.0026,  0.0026]) portfolio_value:  0.9983 [ 0.9259,  1.0801]\n",
      "Finished episode 119000 after 30 timesteps (reward: -0.0001 [-0.0035,  0.0029]) portfolio_value:  0.9983 [ 0.8997,  1.0924]\n",
      "Finished episode 120000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0028]) portfolio_value:  0.9997 [ 0.9129,  1.0872]\n",
      "Finished episode 121000 after 30 timesteps (reward: -0.0000 [-0.0042,  0.0025]) portfolio_value:  0.9991 [ 0.8808,  1.0781]\n",
      "Finished episode 122000 after 30 timesteps (reward: -0.0001 [-0.0060,  0.0024]) portfolio_value:  0.9986 [ 0.8358,  1.0749]\n",
      "Finished episode 123000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0033]) portfolio_value:  0.9993 [ 0.9284,  1.1050]\n",
      "Finished episode 124000 after 30 timesteps (reward: -0.0000 [-0.0058,  0.0022]) portfolio_value:  0.9989 [ 0.8391,  1.0684]\n",
      "Finished episode 125000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0030]) portfolio_value:  0.9993 [ 0.9154,  1.0941]\n",
      "Finished episode 126000 after 30 timesteps (reward: -0.0000 [-0.0051,  0.0030]) portfolio_value:  0.9987 [ 0.8582,  1.0939]\n",
      "Finished episode 127000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0035]) portfolio_value:  0.9989 [ 0.9146,  1.1123]\n",
      "Finished episode 128000 after 30 timesteps (reward: -0.0000 [-0.0042,  0.0042]) portfolio_value:  1.0000 [ 0.8810,  1.1341]\n",
      "Finished episode 129000 after 30 timesteps (reward:  0.0000 [-0.0029,  0.0028]) portfolio_value:  1.0003 [ 0.9179,  1.0875]\n",
      "Finished episode 130000 after 30 timesteps (reward: -0.0001 [-0.0044,  0.0026]) portfolio_value:  0.9979 [ 0.8758,  1.0813]\n",
      "Finished episode 131000 after 30 timesteps (reward: -0.0000 [-0.0024,  0.0022]) portfolio_value:  1.0001 [ 0.9316,  1.0691]\n",
      "Finished episode 132000 after 30 timesteps (reward: -0.0000 [-0.0045,  0.0029]) portfolio_value:  0.9990 [ 0.8724,  1.0901]\n",
      "Finished episode 133000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0023]) portfolio_value:  0.9987 [ 0.9251,  1.0721]\n",
      "Finished episode 134000 after 30 timesteps (reward: -0.0000 [-0.0051,  0.0028]) portfolio_value:  0.9998 [ 0.8577,  1.0872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 135000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0022]) portfolio_value:  0.9995 [ 0.9242,  1.0681]\n",
      "Finished episode 136000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0030]) portfolio_value:  0.9999 [ 0.9258,  1.0948]\n",
      "Finished episode 137000 after 30 timesteps (reward: -0.0001 [-0.0054,  0.0025]) portfolio_value:  0.9982 [ 0.8511,  1.0768]\n",
      "Finished episode 138000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0026]) portfolio_value:  0.9989 [ 0.9153,  1.0800]\n",
      "Finished episode 139000 after 30 timesteps (reward: -0.0000 [-0.0058,  0.0022]) portfolio_value:  0.9991 [ 0.8397,  1.0678]\n",
      "Finished episode 140000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0035]) portfolio_value:  0.9999 [ 0.9194,  1.1120]\n",
      "Finished episode 141000 after 30 timesteps (reward: -0.0000 [-0.0031,  0.0027]) portfolio_value:  0.9993 [ 0.9122,  1.0828]\n",
      "Finished episode 142000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0032]) portfolio_value:  0.9992 [ 0.9214,  1.1019]\n",
      "Finished episode 143000 after 30 timesteps (reward: -0.0000 [-0.0035,  0.0027]) portfolio_value:  0.9993 [ 0.9015,  1.0859]\n",
      "Finished episode 144000 after 30 timesteps (reward: -0.0000 [-0.0033,  0.0027]) portfolio_value:  0.9988 [ 0.9050,  1.0849]\n",
      "Finished episode 145000 after 30 timesteps (reward: -0.0000 [-0.0048,  0.0035]) portfolio_value:  0.9991 [ 0.8651,  1.1107]\n",
      "Finished episode 146000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0032]) portfolio_value:  0.9997 [ 0.9131,  1.0999]\n",
      "Finished episode 147000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0029]) portfolio_value:  0.9993 [ 0.9288,  1.0916]\n",
      "Finished episode 148000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0030]) portfolio_value:  1.0001 [ 0.9216,  1.0936]\n",
      "Finished episode 149000 after 30 timesteps (reward: -0.0000 [-0.0039,  0.0023]) portfolio_value:  0.9989 [ 0.8892,  1.0723]\n",
      "Finished episode 150000 after 30 timesteps (reward: -0.0000 [-0.0024,  0.0026]) portfolio_value:  0.9996 [ 0.9299,  1.0804]\n",
      "Finished episode 151000 after 30 timesteps (reward: -0.0000 [-0.0065,  0.0032]) portfolio_value:  0.9987 [ 0.8240,  1.1011]\n",
      "Finished episode 152000 after 30 timesteps (reward: -0.0000 [-0.0042,  0.0033]) portfolio_value:  0.9989 [ 0.8813,  1.1036]\n",
      "Finished episode 153000 after 30 timesteps (reward: -0.0000 [-0.0035,  0.0027]) portfolio_value:  0.9995 [ 0.9012,  1.0853]\n",
      "Finished episode 154000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0032]) portfolio_value:  0.9995 [ 0.9239,  1.1001]\n",
      "Finished episode 155000 after 30 timesteps (reward: -0.0000 [-0.0064,  0.0027]) portfolio_value:  0.9992 [ 0.8260,  1.0843]\n",
      "Finished episode 156000 after 30 timesteps (reward: -0.0001 [-0.0033,  0.0029]) portfolio_value:  0.9980 [ 0.9063,  1.0896]\n",
      "Finished episode 157000 after 30 timesteps (reward: -0.0001 [-0.0055,  0.0024]) portfolio_value:  0.9984 [ 0.8471,  1.0752]\n",
      "Finished episode 158000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0024]) portfolio_value:  0.9998 [ 0.9260,  1.0749]\n",
      "Finished episode 159000 after 30 timesteps (reward: -0.0000 [-0.0032,  0.0023]) portfolio_value:  0.9998 [ 0.9091,  1.0700]\n",
      "Finished episode 160000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0023]) portfolio_value:  0.9996 [ 0.9156,  1.0700]\n",
      "Finished episode 161000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0026]) portfolio_value:  0.9994 [ 0.9273,  1.0812]\n",
      "Finished episode 162000 after 30 timesteps (reward: -0.0001 [-0.0043,  0.0027]) portfolio_value:  0.9984 [ 0.8796,  1.0841]\n",
      "Finished episode 163000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0032]) portfolio_value:  0.9999 [ 0.9236,  1.1021]\n",
      "Finished episode 164000 after 30 timesteps (reward: -0.0000 [-0.0031,  0.0024]) portfolio_value:  0.9989 [ 0.9121,  1.0740]\n",
      "Finished episode 165000 after 30 timesteps (reward: -0.0000 [-0.0053,  0.0032]) portfolio_value:  0.9991 [ 0.8519,  1.1019]\n",
      "Finished episode 166000 after 30 timesteps (reward: -0.0000 [-0.0033,  0.0041]) portfolio_value:  0.9995 [ 0.9057,  1.1306]\n",
      "Finished episode 167000 after 30 timesteps (reward: -0.0001 [-0.0053,  0.0028]) portfolio_value:  0.9982 [ 0.8533,  1.0882]\n",
      "Finished episode 168000 after 30 timesteps (reward: -0.0000 [-0.0033,  0.0029]) portfolio_value:  0.9990 [ 0.9066,  1.0923]\n",
      "Finished episode 169000 after 30 timesteps (reward: -0.0000 [-0.0034,  0.0026]) portfolio_value:  0.9995 [ 0.9031,  1.0812]\n",
      "Finished episode 170000 after 30 timesteps (reward: -0.0000 [-0.0041,  0.0035]) portfolio_value:  0.9996 [ 0.8845,  1.1091]\n",
      "Finished episode 171000 after 30 timesteps (reward: -0.0000 [-0.0033,  0.0022]) portfolio_value:  0.9991 [ 0.9062,  1.0691]\n",
      "Finished episode 172000 after 30 timesteps (reward: -0.0001 [-0.0042,  0.0028]) portfolio_value:  0.9982 [ 0.8817,  1.0875]\n",
      "Finished episode 173000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0022]) portfolio_value:  0.9990 [ 0.9190,  1.0690]\n",
      "Finished episode 174000 after 30 timesteps (reward: -0.0000 [-0.0058,  0.0035]) portfolio_value:  0.9994 [ 0.8414,  1.1118]\n",
      "Finished episode 175000 after 30 timesteps (reward: -0.0001 [-0.0026,  0.0026]) portfolio_value:  0.9985 [ 0.9236,  1.0822]\n",
      "Finished episode 176000 after 30 timesteps (reward: -0.0001 [-0.0050,  0.0027]) portfolio_value:  0.9987 [ 0.8606,  1.0850]\n",
      "Finished episode 177000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0024]) portfolio_value:  0.9993 [ 0.9229,  1.0758]\n",
      "Finished episode 178000 after 30 timesteps (reward: -0.0000 [-0.0040,  0.0036]) portfolio_value:  0.9987 [ 0.8870,  1.1153]\n",
      "Finished episode 179000 after 30 timesteps (reward:  0.0000 [-0.0028,  0.0031]) portfolio_value:  1.0002 [ 0.9200,  1.0961]\n",
      "Finished episode 180000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0058]) portfolio_value:  0.9994 [ 0.9199,  1.1897]\n",
      "Finished episode 181000 after 30 timesteps (reward: -0.0000 [-0.0023,  0.0031]) portfolio_value:  1.0000 [ 0.9337,  1.0959]\n",
      "Finished episode 182000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0027]) portfolio_value:  0.9996 [ 0.9156,  1.0843]\n",
      "Finished episode 183000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0028]) portfolio_value:  0.9996 [ 0.9245,  1.0864]\n",
      "Finished episode 184000 after 30 timesteps (reward: -0.0000 [-0.0033,  0.0027]) portfolio_value:  0.9994 [ 0.9046,  1.0830]\n",
      "Finished episode 185000 after 30 timesteps (reward: -0.0000 [-0.0069,  0.0039]) portfolio_value:  1.0001 [ 0.8132,  1.1234]\n",
      "Finished episode 186000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0024]) portfolio_value:  0.9996 [ 0.9130,  1.0743]\n",
      "Finished episode 187000 after 30 timesteps (reward: -0.0000 [-0.0058,  0.0030]) portfolio_value:  0.9995 [ 0.8394,  1.0941]\n",
      "Finished episode 188000 after 30 timesteps (reward: -0.0001 [-0.0031,  0.0033]) portfolio_value:  0.9986 [ 0.9117,  1.1043]\n",
      "Finished episode 189000 after 30 timesteps (reward: -0.0000 [-0.0054,  0.0028]) portfolio_value:  0.9990 [ 0.8517,  1.0880]\n",
      "Finished episode 190000 after 30 timesteps (reward: -0.0001 [-0.0039,  0.0022]) portfolio_value:  0.9983 [ 0.8897,  1.0698]\n",
      "Finished episode 191000 after 30 timesteps (reward: -0.0000 [-0.0020,  0.0034]) portfolio_value:  0.9999 [ 0.9420,  1.1062]\n",
      "Finished episode 192000 after 30 timesteps (reward: -0.0000 [-0.0059,  0.0034]) portfolio_value:  0.9987 [ 0.8374,  1.1066]\n",
      "Finished episode 193000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0026]) portfolio_value:  0.9996 [ 0.9224,  1.0803]\n",
      "Finished episode 194000 after 30 timesteps (reward: -0.0000 [-0.0023,  0.0032]) portfolio_value:  0.9988 [ 0.9341,  1.1007]\n",
      "Finished episode 195000 after 30 timesteps (reward: -0.0000 [-0.0048,  0.0020]) portfolio_value:  0.9987 [ 0.8653,  1.0626]\n",
      "Finished episode 196000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0023]) portfolio_value:  0.9992 [ 0.9272,  1.0705]\n",
      "Finished episode 197000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0030]) portfolio_value:  0.9988 [ 0.9165,  1.0934]\n",
      "Finished episode 198000 after 30 timesteps (reward: -0.0001 [-0.0030,  0.0022]) portfolio_value:  0.9983 [ 0.9131,  1.0677]\n",
      "Finished episode 199000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0040]) portfolio_value:  0.9996 [ 0.9243,  1.1275]\n",
      "Finished episode 200000 after 30 timesteps (reward: -0.0000 [-0.0041,  0.0025]) portfolio_value:  0.9987 [ 0.8848,  1.0766]\n",
      "Finished episode 201000 after 30 timesteps (reward: -0.0000 [-0.0046,  0.0032]) portfolio_value:  0.9993 [ 0.8698,  1.1021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 202000 after 30 timesteps (reward: -0.0000 [-0.0023,  0.0034]) portfolio_value:  1.0002 [ 0.9333,  1.1074]\n",
      "Finished episode 203000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0027]) portfolio_value:  0.9996 [ 0.9193,  1.0831]\n",
      "Finished episode 204000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0026]) portfolio_value:  0.9997 [ 0.9228,  1.0800]\n",
      "Finished episode 205000 after 30 timesteps (reward: -0.0000 [-0.0035,  0.0029]) portfolio_value:  0.9997 [ 0.9000,  1.0911]\n",
      "Finished episode 206000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0025]) portfolio_value:  0.9996 [ 0.9183,  1.0780]\n",
      "Finished episode 207000 after 30 timesteps (reward: -0.0000 [-0.0050,  0.0032]) portfolio_value:  0.9995 [ 0.8612,  1.0992]\n",
      "Finished episode 208000 after 30 timesteps (reward: -0.0000 [-0.0031,  0.0034]) portfolio_value:  0.9994 [ 0.9100,  1.1087]\n",
      "Finished episode 209000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0030]) portfolio_value:  0.9999 [ 0.9138,  1.0950]\n",
      "Finished episode 210000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0025]) portfolio_value:  0.9994 [ 0.9283,  1.0791]\n",
      "Finished episode 211000 after 30 timesteps (reward: -0.0000 [-0.0022,  0.0037]) portfolio_value:  0.9997 [ 0.9361,  1.1175]\n",
      "Finished episode 212000 after 30 timesteps (reward: -0.0000 [-0.0023,  0.0025]) portfolio_value:  0.9999 [ 0.9328,  1.0785]\n",
      "Finished episode 213000 after 30 timesteps (reward: -0.0001 [-0.0033,  0.0023]) portfolio_value:  0.9985 [ 0.9069,  1.0703]\n",
      "Finished episode 214000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0031]) portfolio_value:  0.9989 [ 0.9212,  1.0989]\n",
      "Finished episode 215000 after 30 timesteps (reward: -0.0000 [-0.0020,  0.0043]) portfolio_value:  0.9995 [ 0.9409,  1.1362]\n",
      "Finished episode 216000 after 30 timesteps (reward: -0.0000 [-0.0038,  0.0030]) portfolio_value:  0.9993 [ 0.8935,  1.0943]\n",
      "Finished episode 217000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0038]) portfolio_value:  0.9994 [ 0.9138,  1.1198]\n",
      "Finished episode 218000 after 30 timesteps (reward: -0.0000 [-0.0024,  0.0028]) portfolio_value:  0.9993 [ 0.9312,  1.0868]\n",
      "Finished episode 219000 after 30 timesteps (reward:  0.0000 [-0.0017,  0.0025]) portfolio_value:  1.0003 [ 0.9493,  1.0765]\n",
      "Finished episode 220000 after 30 timesteps (reward: -0.0000 [-0.0024,  0.0031]) portfolio_value:  0.9994 [ 0.9317,  1.0988]\n",
      "Finished episode 221000 after 30 timesteps (reward: -0.0000 [-0.0036,  0.0041]) portfolio_value:  0.9989 [ 0.8982,  1.1316]\n",
      "Finished episode 222000 after 30 timesteps (reward: -0.0000 [-0.0049,  0.0021]) portfolio_value:  0.9999 [ 0.8622,  1.0657]\n",
      "Finished episode 223000 after 30 timesteps (reward:  0.0000 [-0.0053,  0.0046]) portfolio_value:  1.0004 [ 0.8541,  1.1474]\n",
      "Finished episode 224000 after 30 timesteps (reward: -0.0000 [-0.0024,  0.0026]) portfolio_value:  0.9996 [ 0.9299,  1.0813]\n",
      "Finished episode 225000 after 30 timesteps (reward: -0.0000 [-0.0044,  0.0019]) portfolio_value:  0.9996 [ 0.8772,  1.0583]\n",
      "Finished episode 226000 after 30 timesteps (reward: -0.0000 [-0.0058,  0.0029]) portfolio_value:  0.9994 [ 0.8414,  1.0917]\n",
      "Finished episode 227000 after 30 timesteps (reward: -0.0000 [-0.0022,  0.0025]) portfolio_value:  0.9995 [ 0.9363,  1.0786]\n",
      "Finished episode 228000 after 30 timesteps (reward: -0.0001 [-0.0061,  0.0022]) portfolio_value:  0.9987 [ 0.8317,  1.0686]\n",
      "Finished episode 229000 after 30 timesteps (reward: -0.0000 [-0.0053,  0.0022]) portfolio_value:  0.9998 [ 0.8523,  1.0698]\n",
      "Finished episode 230000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0037]) portfolio_value:  1.0000 [ 0.9162,  1.1164]\n",
      "Finished episode 231000 after 30 timesteps (reward: -0.0000 [-0.0032,  0.0028]) portfolio_value:  1.0001 [ 0.9098,  1.0865]\n",
      "Finished episode 232000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0030]) portfolio_value:  0.9992 [ 0.9191,  1.0952]\n",
      "Finished episode 233000 after 30 timesteps (reward:  0.0000 [-0.0024,  0.0030]) portfolio_value:  1.0006 [ 0.9314,  1.0926]\n",
      "Finished episode 234000 after 30 timesteps (reward: -0.0000 [-0.0044,  0.0030]) portfolio_value:  0.9988 [ 0.8751,  1.0942]\n",
      "Finished episode 235000 after 30 timesteps (reward: -0.0000 [-0.0043,  0.0031]) portfolio_value:  0.9995 [ 0.8798,  1.0989]\n",
      "Finished episode 236000 after 30 timesteps (reward: -0.0000 [-0.0057,  0.0029]) portfolio_value:  0.9991 [ 0.8422,  1.0899]\n",
      "Finished episode 237000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0024]) portfolio_value:  0.9991 [ 0.9201,  1.0754]\n",
      "Finished episode 238000 after 30 timesteps (reward: -0.0000 [-0.0033,  0.0026]) portfolio_value:  0.9994 [ 0.9060,  1.0810]\n",
      "Finished episode 239000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0025]) portfolio_value:  0.9989 [ 0.9182,  1.0777]\n",
      "Finished episode 240000 after 30 timesteps (reward: -0.0000 [-0.0020,  0.0029]) portfolio_value:  0.9989 [ 0.9410,  1.0923]\n",
      "Finished episode 241000 after 30 timesteps (reward: -0.0000 [-0.0040,  0.0027]) portfolio_value:  0.9990 [ 0.8861,  1.0853]\n",
      "Finished episode 242000 after 30 timesteps (reward: -0.0000 [-0.0021,  0.0032]) portfolio_value:  0.9994 [ 0.9390,  1.1004]\n",
      "Finished episode 243000 after 30 timesteps (reward:  0.0000 [-0.0021,  0.0042]) portfolio_value:  1.0005 [ 0.9398,  1.1347]\n",
      "Finished episode 244000 after 30 timesteps (reward: -0.0000 [-0.0019,  0.0031]) portfolio_value:  0.9999 [ 0.9449,  1.0979]\n",
      "Finished episode 245000 after 30 timesteps (reward: -0.0000 [-0.0072,  0.0041]) portfolio_value:  0.9989 [ 0.8068,  1.1310]\n",
      "Finished episode 246000 after 30 timesteps (reward: -0.0000 [-0.0020,  0.0034]) portfolio_value:  0.9992 [ 0.9412,  1.1087]\n",
      "Finished episode 247000 after 30 timesteps (reward: -0.0000 [-0.0028,  0.0039]) portfolio_value:  0.9987 [ 0.9198,  1.1233]\n",
      "Finished episode 248000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0025]) portfolio_value:  0.9993 [ 0.9156,  1.0792]\n",
      "Finished episode 249000 after 30 timesteps (reward:  0.0000 [-0.0036,  0.0033]) portfolio_value:  1.0004 [ 0.8988,  1.1055]\n",
      "Finished episode 250000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0027]) portfolio_value:  0.9995 [ 0.9256,  1.0838]\n",
      "Finished episode 251000 after 30 timesteps (reward: -0.0001 [-0.0051,  0.0024]) portfolio_value:  0.9983 [ 0.8578,  1.0757]\n",
      "Finished episode 252000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0024]) portfolio_value:  0.9999 [ 0.9148,  1.0757]\n",
      "Finished episode 253000 after 30 timesteps (reward: -0.0000 [-0.0023,  0.0029]) portfolio_value:  1.0001 [ 0.9342,  1.0908]\n",
      "Finished episode 254000 after 30 timesteps (reward: -0.0001 [-0.0048,  0.0039]) portfolio_value:  0.9975 [ 0.8659,  1.1239]\n",
      "Finished episode 255000 after 30 timesteps (reward: -0.0000 [-0.0045,  0.0027]) portfolio_value:  0.9997 [ 0.8742,  1.0859]\n",
      "Finished episode 256000 after 30 timesteps (reward:  0.0000 [-0.0063,  0.0037]) portfolio_value:  1.0003 [ 0.8286,  1.1183]\n",
      "Finished episode 257000 after 30 timesteps (reward: -0.0000 [-0.0040,  0.0027]) portfolio_value:  0.9990 [ 0.8874,  1.0839]\n",
      "Finished episode 258000 after 30 timesteps (reward:  0.0000 [-0.0021,  0.0034]) portfolio_value:  1.0006 [ 0.9389,  1.1082]\n",
      "Finished episode 259000 after 30 timesteps (reward: -0.0000 [-0.0032,  0.0032]) portfolio_value:  1.0001 [ 0.9075,  1.1016]\n",
      "Finished episode 260000 after 30 timesteps (reward: -0.0000 [-0.0044,  0.0030]) portfolio_value:  0.9989 [ 0.8757,  1.0931]\n",
      "Finished episode 261000 after 30 timesteps (reward: -0.0000 [-0.0033,  0.0030]) portfolio_value:  0.9996 [ 0.9055,  1.0943]\n",
      "Finished episode 262000 after 30 timesteps (reward: -0.0000 [-0.0057,  0.0028]) portfolio_value:  0.9992 [ 0.8431,  1.0882]\n",
      "Finished episode 263000 after 30 timesteps (reward: -0.0000 [-0.0019,  0.0032]) portfolio_value:  0.9994 [ 0.9436,  1.1020]\n",
      "Finished episode 264000 after 30 timesteps (reward: -0.0000 [-0.0022,  0.0028]) portfolio_value:  0.9995 [ 0.9352,  1.0862]\n",
      "Finished episode 265000 after 30 timesteps (reward: -0.0000 [-0.0051,  0.0028]) portfolio_value:  0.9999 [ 0.8576,  1.0889]\n",
      "Finished episode 266000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0030]) portfolio_value:  1.0000 [ 0.9232,  1.0957]\n",
      "Finished episode 267000 after 30 timesteps (reward: -0.0000 [-0.0040,  0.0038]) portfolio_value:  0.9992 [ 0.8865,  1.1215]\n",
      "Finished episode 268000 after 30 timesteps (reward: -0.0000 [-0.0033,  0.0033]) portfolio_value:  0.9992 [ 0.9064,  1.1054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 269000 after 30 timesteps (reward: -0.0000 [-0.0043,  0.0034]) portfolio_value:  0.9987 [ 0.8786,  1.1064]\n",
      "Finished episode 270000 after 30 timesteps (reward: -0.0000 [-0.0044,  0.0026]) portfolio_value:  0.9990 [ 0.8776,  1.0807]\n",
      "Finished episode 271000 after 30 timesteps (reward:  0.0000 [-0.0034,  0.0024]) portfolio_value:  1.0003 [ 0.9021,  1.0737]\n",
      "Finished episode 272000 after 30 timesteps (reward:  0.0000 [-0.0020,  0.0032]) portfolio_value:  1.0010 [ 0.9410,  1.0993]\n",
      "Finished episode 273000 after 30 timesteps (reward: -0.0000 [-0.0053,  0.0026]) portfolio_value:  0.9989 [ 0.8537,  1.0817]\n",
      "Finished episode 274000 after 30 timesteps (reward: -0.0000 [-0.0033,  0.0032]) portfolio_value:  0.9999 [ 0.9051,  1.0992]\n",
      "Finished episode 275000 after 30 timesteps (reward: -0.0000 [-0.0031,  0.0029]) portfolio_value:  0.9987 [ 0.9119,  1.0895]\n",
      "Finished episode 276000 after 30 timesteps (reward: -0.0000 [-0.0030,  0.0039]) portfolio_value:  1.0002 [ 0.9135,  1.1228]\n",
      "Finished episode 277000 after 30 timesteps (reward: -0.0000 [-0.0022,  0.0037]) portfolio_value:  1.0000 [ 0.9366,  1.1160]\n",
      "Finished episode 278000 after 30 timesteps (reward: -0.0001 [-0.0066,  0.0029]) portfolio_value:  0.9982 [ 0.8207,  1.0895]\n",
      "Finished episode 279000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0042]) portfolio_value:  0.9996 [ 0.9223,  1.1357]\n",
      "Finished episode 280000 after 30 timesteps (reward: -0.0000 [-0.0031,  0.0036]) portfolio_value:  0.9998 [ 0.9110,  1.1144]\n",
      "Finished episode 281000 after 30 timesteps (reward: -0.0000 [-0.0027,  0.0025]) portfolio_value:  0.9993 [ 0.9229,  1.0765]\n",
      "Finished episode 282000 after 30 timesteps (reward: -0.0001 [-0.0051,  0.0026]) portfolio_value:  0.9985 [ 0.8588,  1.0827]\n",
      "Finished episode 283000 after 30 timesteps (reward: -0.0000 [-0.0018,  0.0025]) portfolio_value:  0.9995 [ 0.9476,  1.0774]\n",
      "Finished episode 284000 after 30 timesteps (reward:  0.0000 [-0.0020,  0.0027]) portfolio_value:  1.0011 [ 0.9426,  1.0851]\n",
      "Finished episode 285000 after 30 timesteps (reward:  0.0000 [-0.0027,  0.0031]) portfolio_value:  1.0005 [ 0.9213,  1.0981]\n",
      "Finished episode 286000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0036]) portfolio_value:  0.9989 [ 0.9271,  1.1137]\n",
      "Finished episode 287000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0027]) portfolio_value:  0.9988 [ 0.9261,  1.0840]\n",
      "Finished episode 288000 after 30 timesteps (reward: -0.0000 [-0.0021,  0.0024]) portfolio_value:  0.9995 [ 0.9378,  1.0752]\n",
      "Finished episode 289000 after 30 timesteps (reward:  0.0000 [-0.0062,  0.0034]) portfolio_value:  1.0007 [ 0.8290,  1.1061]\n",
      "Finished episode 290000 after 30 timesteps (reward: -0.0000 [-0.0026,  0.0023]) portfolio_value:  0.9993 [ 0.9252,  1.0721]\n",
      "Finished episode 291000 after 30 timesteps (reward:  0.0000 [-0.0018,  0.0026]) portfolio_value:  1.0005 [ 0.9462,  1.0820]\n",
      "Finished episode 292000 after 30 timesteps (reward: -0.0000 [-0.0025,  0.0025]) portfolio_value:  0.9999 [ 0.9284,  1.0790]\n",
      "Finished episode 293000 after 30 timesteps (reward: -0.0000 [-0.0029,  0.0050]) portfolio_value:  0.9990 [ 0.9169,  1.1627]\n",
      "Finished episode 294000 after 30 timesteps (reward: -0.0000 [-0.0060,  0.0027]) portfolio_value:  0.9994 [ 0.8358,  1.0847]\n"
     ]
    }
   ],
   "source": [
    "runner.run(\n",
    "    episodes=2e6, max_timesteps=200, episode_finished=EpisodeFinished(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T06:13:09.482Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO turn off learning during test\n",
    "runner_test = Runner(agent=agent, environment=environment_test)\n",
    "runner_test.run(\n",
    "    episodes=100, max_timesteps=128, episode_finished=EpisodeFinished(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:36:25.184976Z",
     "start_time": "2017-07-16T12:36:10.079463+08:00"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
