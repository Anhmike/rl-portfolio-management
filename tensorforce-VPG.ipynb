{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:40:52.715453Z",
     "start_time": "2017-07-15T14:40:52.013115+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# numeric\n",
    "import quandl\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# util\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import time\n",
    "import tempfile\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:40:52.742035Z",
     "start_time": "2017-07-15T14:40:52.717286+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:40:52.763591Z",
     "start_time": "2017-07-15T14:40:52.743756+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:40:52.781632Z",
     "start_time": "2017-07-15T14:40:52.765378+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:51:47.252907Z",
     "start_time": "2017-07-15T15:51:47.232896+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnvWrapper(PortfolioEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def step(self, action):\n",
    "        # also it puts it in a list\n",
    "        if isinstance(action, list):\n",
    "            action = action[0]\n",
    "        \n",
    "        # we have to normalise for some reason softmax wont work\n",
    "        if isinstance(action, dict):\n",
    "            action = np.abs(list(action.values()))\n",
    "            action /= action.sum()        \n",
    "        \n",
    "        return super().step(action) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:51:48.159427Z",
     "start_time": "2017-07-15T15:51:47.706019+08:00"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_hdf('./data/poliniex_30m.hf',key='train')\n",
    "env = EnvWrapper(\n",
    "    df=df_train,\n",
    "    steps=128, \n",
    "    scale=True, \n",
    "    augument=0.0005    \n",
    ")\n",
    "env.seed = 0   \n",
    "\n",
    "df_test = pd.read_hdf('./data/poliniex_30m.hf',key='test')\n",
    "env_test = EnvWrapper(\n",
    "    df=df_test,\n",
    "    steps=128, \n",
    "    scale=True, \n",
    "    augument=0.00)\n",
    "env_test.seed = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:52:20.529303Z",
     "start_time": "2017-07-15T15:52:20.496639+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2017-07-15 15:52:20,523] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "from tensorforce.environments.openai_gym import OpenAIGym\n",
    "environment = OpenAIGym('CartPole-v0')\n",
    "environment.gym = env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T07:52:21.308296Z",
     "start_time": "2017-07-15T15:52:21.280189+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce import Configuration\n",
    "from tensorforce.agents import VPGAgent\n",
    "from tensorforce.core.networks import layered_network_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T08:14:14.169844Z",
     "start_time": "2017-07-15T16:14:14.138796+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# Define a network builder from an ordered list of layers\n",
    "# https://github.com/reinforceio/tensorforce/blob/0d07fadec03f76537a2431e17c51cd759d53b5e9/tensorforce/core/networks/layers.py\n",
    "layers = [\n",
    "    dict(type='flatten'),\n",
    "    dict(type='dense', size=32, l2_regularization=1e-8, activation='relu'),\n",
    "    dict(type='dense', size=32, l2_regularization=1e-8, activation='relu'),    \n",
    "]\n",
    "# act will it add's it's own head so we can't add a softmax at the end\n",
    "network = layered_network_builder(layers_config=layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T08:39:29.264462Z",
     "start_time": "2017-07-15T16:39:28.591199+08:00"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "config = Configuration(   \n",
    "    # Each agent requires the following ``Configuration`` parameters:\n",
    "    network=network,\n",
    "    states=dict(shape=tuple(env.observation_space.shape), type='float'),\n",
    "    actions={'action' + str(n): dict(continuous=True) for n in range(env.action_space.shape[0])},\n",
    "    preprocessing = None,# dict or list containing state preprocessing configuration.\n",
    "    exploration = dict(\n",
    "        type='EpsilonDecay',\n",
    "        kwargs=dict(epsilon=1, epsilon_final=0.01, epsilon_timesteps=1e4)\n",
    "    ),\n",
    "\n",
    "    # The `BatchAgent` class additionally requires the following parameters:\n",
    "    batch_size = 32,# integer of the batch size.\n",
    "\n",
    "    # A Policy Gradient Model expects the following additional configuration parameters:\n",
    "    sample_actions= True,# boolean of whether to sample actions.\n",
    "#     baseline= ,# string indicating the baseline value function (currently 'linear' or 'mlp').\n",
    "#     baseline_args= ,# list of arguments for the baseline value function.\n",
    "#     baseline_kwargs= ,# dict of keyword arguments for the baseline value function.\n",
    "    generalized_advantage_estimation= True ,# boolean indicating whether to use GAE.\n",
    "#     gae_lambda= ,# float of the Generalized Advantage Estimation lambda.\n",
    "    normalize_advantage= True,# boolean indicating whether to normalize the advantage or not.\n",
    ")\n",
    "\n",
    "# Create a Trust Region Policy Optimization agent\n",
    "agent = VPGAgent(config=config)\n",
    "\n",
    "# for some reason these are not set?\n",
    "agent.next_internal = agent.current_internal = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T08:39:29.283051Z",
     "start_time": "2017-07-15T16:39:29.266099+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# why does softmax not work, how to view this?\n",
    "# agent.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T08:39:29.855634Z",
     "start_time": "2017-07-15T16:39:29.833258+08:00"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Poll new state from client\n",
    "# state = environment.reset()\n",
    "\n",
    "# for i in range(10):\n",
    "#     # Get prediction from agent, execute\n",
    "#     action = agent.act(state=state)\n",
    "\n",
    "#     state, reward, done = environment.execute(action)\n",
    "\n",
    "#     # Add experience, agent automatically updates model according to batch size\n",
    "#     agent.observe(reward=reward, terminal=False)\n",
    "    \n",
    "#     a=np.array(list(action.values()))\n",
    "#     print(a, a.sum(),a.min(),a.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T08:39:30.921673Z",
     "start_time": "2017-07-15T16:39:30.894702+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce.execution import Runner\n",
    "runner = Runner(agent=agent, environment=environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T08:39:31.375755Z",
     "start_time": "2017-07-15T16:39:31.342883+08:00"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    log_intv = 200\n",
    "    if r.episode % log_intv == 0:\n",
    "        df = pd.DataFrame(env.sim.infos)\n",
    "        print(\n",
    "            \"Finished episode {ep} after {ts} timesteps (reward: {reward: 2.4f}) portfolio_value: {portfolio_value: 2.4f}\".\n",
    "            format(\n",
    "                ep=r.episode,\n",
    "                ts=r.timestep,\n",
    "                reward=r.episode_rewards[-log_intv],\n",
    "                portfolio_value=df.portfolio_value[-1:].mean()\n",
    "            )\n",
    "        )\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-15T08:39:32.094Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "runner.run(\n",
    "    episodes=90000, max_timesteps=200, episode_finished=episode_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T08:06:18.789315Z",
     "start_time": "2017-07-15T16:05:19.669576+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
