{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:55:19.963521Z",
     "start_time": "2017-07-16T08:55:19.123482+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# numeric\n",
    "import quandl\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# util\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import time\n",
    "import tempfile\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:55:20.002083Z",
     "start_time": "2017-07-16T08:55:19.966233+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:55:20.008438Z",
     "start_time": "2017-07-16T08:55:20.004281+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:55:20.021381Z",
     "start_time": "2017-07-16T08:55:20.011131+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:55:20.034469Z",
     "start_time": "2017-07-16T08:55:20.024002+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:55:33.508004Z",
     "start_time": "2017-07-16T08:55:33.481012+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from rl.memory import  SequentialMemory, Memory\n",
    "from collections import deque\n",
    "\n",
    "class EnvWrapper(PortfolioEnv):\n",
    "    \"\"\"Wraps env to normalise and reshape action.\"\"\"\n",
    "    def __init__(self, window_length=50, *args, **kwargs):\n",
    "        self.memory = SequentialMemory(limit=window_length*2, window_length=window_length)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # also it puts it in a list\n",
    "        if isinstance(action, list):\n",
    "            action = action[0]\n",
    "        \n",
    "        # we have to normalise for some reason softmax wont work\n",
    "        if isinstance(action, dict):\n",
    "            action = np.abs(list(action.values()))\n",
    "            action /= action.sum()        \n",
    "        \n",
    "        return super().step(action) \n",
    "\n",
    "class MemoryWrapper(EnvWrapper):\n",
    "    \"\"\"Provides memory to env observations.\"\"\"\n",
    "    def __init__(self, window_length=50, *args, **kwargs):\n",
    "        self.memory = SequentialMemory(limit=window_length*2, window_length=window_length)\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = super().step(action)     \n",
    "        obs = np.array(self.memory.get_recent_state(obs))\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.memory.recent_terminals = deque(maxlen=window_length)\n",
    "        return super().reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:34:06.823632Z",
     "start_time": "2017-07-16T08:34:06.790201+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:56:26.738211Z",
     "start_time": "2017-07-16T08:56:26.143103+08:00"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_hdf('./data/poliniex_30m.hf',key='train')\n",
    "env = MemoryWrapper(\n",
    "    window_length=window_length,\n",
    "    df=df_train,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.0005    \n",
    ")\n",
    "env.seed = 0   \n",
    "\n",
    "df_test = pd.read_hdf('./data/poliniex_30m.hf',key='test')\n",
    "env_test = MemoryWrapper(\n",
    "    window_length=window_length,\n",
    "    df=df_test,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.00)\n",
    "env_test.seed = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:56:26.751501Z",
     "start_time": "2017-07-16T08:56:26.739698+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2017-07-16 08:56:26,741] Making new env: CartPole-v0\n",
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2017-07-16 08:56:26,746] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "from tensorforce.environments.openai_gym import OpenAIGym\n",
    "environment = OpenAIGym('CartPole-v0')\n",
    "environment.gym = env\n",
    "\n",
    "environment_test = OpenAIGym('CartPole-v0')\n",
    "environment_test.gym = env_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:56:27.331113Z",
     "start_time": "2017-07-16T08:56:27.270458+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.000166183773606 False {'reward': -0.00016618377360567651, 'log_return': -0.0049855132081702957, 'portfolio_value': 0.99502689333843652, 'returns': 0.99883330332329823, 'rate_of_return': -0.0049731066615634756, 'cost': 0.0049730071588740838, 'steps': 2}\n",
      "(50, 6, 8) (50, 6, 8)\n"
     ]
    }
   ],
   "source": [
    "# check shapes\n",
    "obs1, reward, done, info=env.step(np.random.random(env.action_space.shape))\n",
    "print(reward, done, info)\n",
    "obs2 = env.reset()\n",
    "print(obs1.shape,obs2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:56:27.475816Z",
     "start_time": "2017-07-16T08:56:27.472395+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce import Configuration\n",
    "from tensorforce.agents import VPGAgent\n",
    "from tensorforce.core.networks import layered_network_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:56:27.722668Z",
     "start_time": "2017-07-16T08:56:27.664183+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/reinforceio/tensorforce/blob/0d07fadec03f76537a2431e17c51cd759d53b5e9/tensorforce/core/networks/layers.py#L90\n",
    "import tensorflow as tf\n",
    "from math import sqrt\n",
    "from tensorforce import util\n",
    "from tensorforce import TensorForceError\n",
    "def nonlinearity(x, name='relu'):\n",
    "    with tf.variable_scope('nonlinearity'):\n",
    "        if name == 'elu':\n",
    "            x = tf.nn.elu(features=x)\n",
    "        elif name == 'relu':\n",
    "            x = tf.nn.relu(features=x)\n",
    "        elif name == 'selu':\n",
    "            # https://arxiv.org/pdf/1706.02515.pdf\n",
    "            alpha = 1.6732632423543772848170429916717\n",
    "            scale = 1.0507009873554804934193349852946\n",
    "            negative = alpha * tf.nn.elu(features=x)\n",
    "            x = scale * tf.where(condition=(x >= 0.0), x=x, y=negative)\n",
    "        elif name == 'sigmoid':\n",
    "            x = tf.sigmoid(x=x)\n",
    "        elif name == 'softmax':\n",
    "            x = tf.nn.softmax(logits=x)\n",
    "        elif name == 'tanh':\n",
    "            x = tf.nn.tanh(x=x)\n",
    "        else:\n",
    "            raise TensorForceError('Invalid nonlinearity.')\n",
    "    return x\n",
    "\n",
    "def flatten(x):\n",
    "    with tf.variable_scope('flatten'):\n",
    "        x = tf.reshape(tensor=x, shape=(-1, util.prod(x.get_shape().as_list()[1:])))\n",
    "    return x\n",
    "\n",
    "def conv2d(x, size, window=(3,3), stride=(1,1), bias=False, activation='relu', l2_regularization=0.0, padding='SAME'):\n",
    "    if util.rank(x) != 4:\n",
    "        raise TensorForceError('Invalid input rank for conv2d layer.')\n",
    "    with tf.variable_scope('conv2d'):\n",
    "        filters = tf.Variable(initial_value=tf.random_normal(shape=(window[0], window[1], x.get_shape()[3].value, size), stddev=sqrt(2.0 / size)))\n",
    "        if l2_regularization > 0.0:\n",
    "            tf.losses.add_loss(l2_regularization * tf.nn.l2_loss(t=filters))\n",
    "        x = tf.nn.conv2d(input=x, filter=filters, strides=(1, stride[0], stride[1], 1), padding=padding)\n",
    "        if bias:\n",
    "            bias = tf.Variable(initial_value=tf.zeros(shape=(size,)))\n",
    "            if l2_regularization > 0.0:\n",
    "                tf.losses.add_loss(l2_regularization * tf.nn.l2_loss(t=bias))\n",
    "            x = tf.nn.bias_add(value=x, bias=bias)\n",
    "        x = nonlinearity(x=x, name=activation)\n",
    "    return x\n",
    "\n",
    "def network_builder(inputs):\n",
    "    if len(inputs) != 1:\n",
    "        raise TensorForceError('Layered network must have only one input.')\n",
    "    x = next(iter(inputs.values()))\n",
    "    \n",
    "    x = conv2d(x=x, size=2, window=(3,1), bias=True, activation='relu', l2_regularization=1e-8, padding='VALID')\n",
    "    x = conv2d(x=x, size=20, window=(window_length-2,1), bias=True, activation='relu', l2_regularization=1e-8, padding='VALID')\n",
    "    x = conv2d(x=x, size=1, window=(1,1), bias=True, activation='relu', l2_regularization=1e-8, padding='VALID')\n",
    "    x = flatten(x)\n",
    "    x = nonlinearity(x,name='softmax')\n",
    "    \n",
    "    return x\n",
    "network=network_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:57:10.933323Z",
     "start_time": "2017-07-16T08:57:10.005079+08:00"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "config = Configuration(   \n",
    "    # Each agent requires the following ``Configuration`` parameters:\n",
    "    network=network,\n",
    "    states=dict(shape=(window_length,)+tuple(env.observation_space.shape), type='float'),\n",
    "    actions={'action' + str(n): dict(continuous=True) for n in range(env.action_space.shape[0])},\n",
    "    preprocessing = None,# dict or list containing state preprocessing configuration.\n",
    "    exploration = dict(\n",
    "        type='EpsilonDecay',\n",
    "        kwargs=dict(epsilon=1, epsilon_final=0.01, epsilon_timesteps=1e4)\n",
    "    ),\n",
    "\n",
    "    # The `BatchAgent` class additionally requires the following parameters:\n",
    "    batch_size = 50,# integer of the batch size.\n",
    "\n",
    "    # A Policy Gradient Model expects the following additional configuration parameters:\n",
    "    sample_actions= True,# boolean of whether to sample actions.\n",
    "#     baseline='mlp' ,# string indicating the baseline value function (currently 'linear' or 'mlp').\n",
    "#     baseline_args=dict(size=100, repeat_update=100) ,# list of arguments for the baseline value function.\n",
    "    override_line_search=False,\n",
    "    \n",
    "#     baseline_kwargs= ,# dict of keyword arguments for the baseline value function.\n",
    "    generalized_advantage_estimation= True ,# boolean indicating whether to use GAE.\n",
    "    gae_lambda= 0.97,# float of the Generalized Advantage Estimation lambda.\n",
    "    normalize_advantage= False,# boolean indicating whether to normalize the advantage or not.\n",
    "    cg_iterations=20,\n",
    "    max_kl_divergence=0.005,\n",
    "    cg_damping=0.001,\n",
    "    line_search_steps=20,\n",
    "    loglevel=\"info\",\n",
    ")\n",
    "\n",
    "# Create a Trust Region Policy Optimization agent\n",
    "agent = VPGAgent(config=config)\n",
    "\n",
    "# for some reason these are not set?\n",
    "# agent.next_internal = agent.current_internal = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:57:10.949394Z",
     "start_time": "2017-07-16T08:57:10.935950+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from tensorforce.execution import Runner\n",
    "runner = Runner(agent=agent, environment=environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:57:12.071792Z",
     "start_time": "2017-07-16T08:57:12.060702+08:00"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Callback function printing episode statistics\n",
    "class EpisodeFinished(object):\n",
    "    \"\"\"Logger callback for tensorforce runner.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_intv):\n",
    "        self.log_intv = log_intv\n",
    "        self.portfolio_values = [] \n",
    "    \n",
    "    def __call__(self, r):\n",
    "        if len(r.environment.gym.sim.infos):\n",
    "            self.portfolio_values.append( r.environment.gym.sim.infos[-1]['portfolio_value'] )\n",
    "        if r.episode % self.log_intv == 0:\n",
    "#             df = pd.DataFrame(r.environment.gym.infos)\n",
    "            print(\n",
    "                \"Finished episode {ep} after {ts} timesteps (reward: {reward: 2.4f} [{rewards_min: 2.4f}, {rewards_max: 2.4f}]) portfolio_value: {portfolio_value: 2.4f} [{portfolio_value_min: 2.4f}, {portfolio_value_max: 2.4f}]\".\n",
    "                format(\n",
    "                    ep=r.episode,\n",
    "                    ts=r.timestep,\n",
    "                    reward=np.mean(r.episode_rewards[-self.log_intv:]),\n",
    "                    rewards_min=np.min(r.episode_rewards[-self.log_intv:]),\n",
    "                    rewards_max=np.max(r.episode_rewards[-self.log_intv:]),\n",
    "                    portfolio_value=np.mean(self.portfolio_values[-self.log_intv:]),\n",
    "                    portfolio_value_min=np.min(self.portfolio_values[-self.log_intv:]),\n",
    "                    portfolio_value_max=np.max(self.portfolio_values[-self.log_intv:])\n",
    "                )\n",
    "            )\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T00:57:21.581Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner.run(\n",
    "    episodes=2e6, max_timesteps=200, episode_finished=EpisodeFinished(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T22:37:42.373009Z",
     "start_time": "2017-07-16T06:37:42.364106+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:55:17.508949Z",
     "start_time": "2017-07-16T08:55:17.458053+08:00"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Runner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-990b680b0306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO turn off learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrunner_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m runner_test.run(\n\u001b[1;32m      4\u001b[0m     episodes=100, max_timesteps=128, episode_finished=EpisodeFinished(10))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Runner' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO turn off learning during test\n",
    "runner_test = Runner(agent=agent, environment=environment_test)\n",
    "runner_test.run(\n",
    "    episodes=100, max_timesteps=128, episode_finished=EpisodeFinished(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
