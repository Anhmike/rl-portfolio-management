{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:22:53.400678Z",
     "start_time": "2017-07-17T07:22:52.382525+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# numeric\n",
    "import quandl\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# util\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import time\n",
    "import tempfile\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:22:53.425193Z",
     "start_time": "2017-07-17T07:22:53.402237+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:22:53.451341Z",
     "start_time": "2017-07-17T07:22:53.429403+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:34:06.823632Z",
     "start_time": "2017-07-16T08:34:06.790201+08:00"
    }
   },
   "outputs": [],
   "source": [
    "window_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T02:27:36.822Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:22:53.546840Z",
     "start_time": "2017-07-17T07:22:53.500566+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from rl.memory import  SequentialMemory, Memory\n",
    "from collections import deque\n",
    "\n",
    "class EnvWrapper(PortfolioEnv):\n",
    "    \"\"\"Wraps env to normalise and reshape action.\"\"\"\n",
    "    def __init__(self, window_length=50, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # also it puts it in a list\n",
    "        if isinstance(action, list):\n",
    "            action = action[0]\n",
    "        \n",
    "        # we have to normalise for some reason softmax wont work\n",
    "        if isinstance(action, dict):\n",
    "            action = np.abs(list(action.values()))\n",
    "            action /= action.sum()        \n",
    "        \n",
    "        return super().step(action) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:22:53.934023Z",
     "start_time": "2017-07-17T07:22:53.548783+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2017-07-17 07:22:53,913] Making new env: CartPole-v0\n",
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2017-07-17 07:22:53,924] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_hdf('./data/poliniex_30m.hf',key='train')\n",
    "env = PortfolioEnv(\n",
    "    df=df_train,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.0005,\n",
    "    trading_cost=0 # let just overfit first,\n",
    "    window_length = window_length,\n",
    "    \n",
    ")\n",
    "env.seed = 0   \n",
    "\n",
    "df_test = pd.read_hdf('./data/poliniex_30m.hf',key='test')\n",
    "env_test = PortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.00,\n",
    "    trading_cost=0, # let just overfit first\n",
    "    window_length=window_length,\n",
    ")\n",
    "env_test.seed = 0  \n",
    "\n",
    "from tensorforce.environments.openai_gym import OpenAIGym\n",
    "environment = OpenAIGym('CartPole-v0')\n",
    "environment.gym = env\n",
    "\n",
    "environment_test = OpenAIGym('CartPole-v0')\n",
    "environment_test.gym = env_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:22:54.001165Z",
     "start_time": "2017-07-17T07:22:53.935952+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.000125080439895 False {'reward': -0.00012508043989537139, 'log_return': -0.003752413196861142, 'portfolio_value': 0.99625461768415069, 'returns': 0.99563679959826168, 'rate_of_return': -0.0037453820667856519, 'cost': 0.0037453820667856302, 'steps': 2}\n",
      "(5, 50, 3) (5, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "# check shapes\n",
    "obs1, reward, done, info=env.step(np.random.random(env.action_space.shape))\n",
    "print(reward, done, info)\n",
    "obs2 = env.reset()\n",
    "print(obs1.shape,obs2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:41:21.116729Z",
     "start_time": "2017-07-16T12:41:21.086620+08:00"
    }
   },
   "source": [
    "# Model\n",
    "\n",
    "Derived from  https://github.com/reinforceio/tensorforce/blob/0d07fadec03f76537a2431e17c51cd759d53b5e9/tensorforce/core/networks/layers.py#L90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:22:56.075716Z",
     "start_time": "2017-07-17T07:22:54.003816+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce import Configuration\n",
    "from tensorforce.agents import VPGAgent\n",
    "from tensorforce.core.networks import layered_network_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:22:56.228822Z",
     "start_time": "2017-07-17T07:22:56.077535+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer helpers from:\n",
    "# https://github.com/reinforceio/tensorforce/blob/0d07fadec03f76537a2431e17c51cd759d53b5e9/tensorforce/core/networks/layers.py#L90\n",
    "import tensorflow as tf\n",
    "from math import sqrt\n",
    "from tensorforce import util\n",
    "from tensorforce import TensorForceError\n",
    "\n",
    "def linear(x, size, bias=True, l2_regularization=0.0):\n",
    "    if util.rank(x) != 2:\n",
    "        raise TensorForceError('Invalid input rank for linear layer.')\n",
    "    with tf.variable_scope('linear'):\n",
    "        weights = tf.Variable(initial_value=tf.random_normal(shape=(x.get_shape()[1].value, size), stddev=sqrt(2.0 / (x.get_shape()[1].value + size))))\n",
    "        if l2_regularization > 0.0:\n",
    "            tf.losses.add_loss(l2_regularization * tf.nn.l2_loss(t=weights))\n",
    "        x = tf.matmul(a=x, b=weights)\n",
    "        if bias:\n",
    "            bias = tf.Variable(initial_value=tf.zeros(shape=(size,)))\n",
    "            if l2_regularization > 0.0:\n",
    "                tf.losses.add_loss(l2_regularization * tf.nn.l2_loss(t=bias))\n",
    "            x = tf.nn.bias_add(value=x, bias=bias)\n",
    "    return x\n",
    "\n",
    "def nonlinearity(x, name='relu'):\n",
    "    with tf.variable_scope('nonlinearity'):\n",
    "        if name == 'elu':\n",
    "            x = tf.nn.elu(features=x)\n",
    "        elif name == 'relu':\n",
    "            x = tf.nn.relu(features=x)\n",
    "        elif name == 'selu':\n",
    "            # https://arxiv.org/pdf/1706.02515.pdf\n",
    "            alpha = 1.6732632423543772848170429916717\n",
    "            scale = 1.0507009873554804934193349852946\n",
    "            negative = alpha * tf.nn.elu(features=x)\n",
    "            x = scale * tf.where(condition=(x >= 0.0), x=x, y=negative)\n",
    "        elif name == 'sigmoid':\n",
    "            x = tf.sigmoid(x=x)\n",
    "        elif name == 'softmax':\n",
    "            x = tf.nn.softmax(logits=x)\n",
    "        elif name == 'tanh':\n",
    "            x = tf.nn.tanh(x=x)\n",
    "        else:\n",
    "            raise TensorForceError('Invalid nonlinearity.')\n",
    "    return x\n",
    "\n",
    "def dense(x, size, bias=True, activation='relu', l2_regularization=0.0):\n",
    "    if util.rank(x) != 2:\n",
    "        raise TensorForceError('Invalid input rank for dense layer.')\n",
    "    with tf.variable_scope('dense'):\n",
    "        x = linear(x=x, size=size, bias=bias, l2_regularization=l2_regularization)\n",
    "        x = nonlinearity(x=x, name=activation)\n",
    "    return x\n",
    "\n",
    "def flatten(x):\n",
    "    with tf.variable_scope('flatten'):\n",
    "        x = tf.reshape(tensor=x, shape=(-1, util.prod(x.get_shape().as_list()[1:])))\n",
    "    return x\n",
    "\n",
    "def conv2d(x, size, window=(3,3), stride=(1,1), bias=False, activation='relu', l2_regularization=0.0, padding='SAME'):\n",
    "    if util.rank(x) != 4:\n",
    "        raise TensorForceError('Invalid input rank for conv2d layer.')\n",
    "    with tf.variable_scope('conv2d'):\n",
    "        filters = tf.Variable(initial_value=tf.random_normal(shape=(window[0], window[1], x.get_shape()[3].value, size), stddev=sqrt(2.0 / size)))\n",
    "        if l2_regularization > 0.0:\n",
    "            tf.losses.add_loss(l2_regularization * tf.nn.l2_loss(t=filters))\n",
    "        x = tf.nn.conv2d(input=x, filter=filters, strides=(1, stride[0], stride[1], 1), padding=padding)\n",
    "        if bias:\n",
    "            bias = tf.Variable(initial_value=tf.zeros(shape=(size,)))\n",
    "            if l2_regularization > 0.0:\n",
    "                tf.losses.add_loss(l2_regularization * tf.nn.l2_loss(t=bias))\n",
    "            x = tf.nn.bias_add(value=x, bias=bias)\n",
    "        x = nonlinearity(x=x, name=activation)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a network for a given input\n",
    "def network_builder(inputs):\n",
    "    if len(inputs) != 1:\n",
    "        raise TensorForceError('Layered network must have only one input.')\n",
    "    x = next(iter(inputs.values()))\n",
    "    \n",
    "    x = conv2d(x=x, size=2, window=(1,3), bias=True, activation='relu', l2_regularization=1e-8, padding='VALID')\n",
    "    x = conv2d(x=x, size=20, window=(1,window_length-2), bias=True, activation='relu', l2_regularization=1e-8, padding='VALID')\n",
    "    x = conv2d(x=x, size=1, window=(1,1), bias=True, activation='relu', l2_regularization=1e-8, padding='VALID')\n",
    "    x = flatten(x)\n",
    "    x = dense(x, size=env.action_space.shape[0],activation='relu', l2_regularization=1e-8)\n",
    "    x = nonlinearity(x,name='softmax')\n",
    "    \n",
    "    return x\n",
    "network=network_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:22:57.260985Z",
     "start_time": "2017-07-17T07:22:56.230145+08:00"
    }
   },
   "outputs": [],
   "source": [
    "config = Configuration(   \n",
    "    # Each agent requires the following ``Configuration`` parameters:\n",
    "    network=network,\n",
    "    states=dict(shape=tuple(env.observation_space.shape), type='float'),\n",
    "    actions={'action' + str(n): dict(continuous=True) for n in range(env.action_space.shape[0])},\n",
    "    preprocessing = None,# dict or list containing state preprocessing configuration.\n",
    "    exploration = dict(\n",
    "        type='EpsilonDecay',\n",
    "        kwargs=dict(epsilon=1, epsilon_final=0.01, epsilon_timesteps=1e4)\n",
    "    ),\n",
    "\n",
    "    # The `BatchAgent` class additionally requires the following parameters:\n",
    "    batch_size = 50,# integer of the batch size.\n",
    "\n",
    "    # A Policy Gradient Model expects the following additional configuration parameters:\n",
    "    sample_actions= True,# boolean of whether to sample actions.\n",
    "#     baseline='mlp' ,# string indicating the baseline value function (currently 'linear' or 'mlp').\n",
    "#     baseline_args=dict(size=100, repeat_update=100) ,# list of arguments for the baseline value function.\n",
    "    override_line_search=False,\n",
    "    \n",
    "#     baseline_kwargs= ,# dict of keyword arguments for the baseline value function.\n",
    "    generalized_advantage_estimation= True ,# boolean indicating whether to use GAE.\n",
    "    gae_lambda= 0.97,# float of the Generalized Advantage Estimation lambda.\n",
    "    normalize_advantage= False,# boolean indicating whether to normalize the advantage or not.\n",
    "    cg_iterations=20,\n",
    "    max_kl_divergence=0.005,\n",
    "    cg_damping=0.001,\n",
    "    line_search_steps=20,\n",
    "    loglevel=\"info\",\n",
    ")\n",
    "\n",
    "# Create a Trust Region Policy Optimization agent\n",
    "agent = VPGAgent(config=config)\n",
    "\n",
    "# for some reason these are not set?\n",
    "# agent.next_internal = agent.current_internal = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T23:22:57.360238Z",
     "start_time": "2017-07-17T07:22:57.262925+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from tensorforce.execution import Runner\n",
    "runner = Runner(agent=agent, environment=environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T23:22:52.365Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Callback function printing episode statistics\n",
    "class EpisodeFinished(object):\n",
    "    \"\"\"Logger callback for tensorforce runner\"\"\"\n",
    "    \n",
    "    def __init__(self, log_intv):\n",
    "        self.log_intv = log_intv\n",
    "        self.portfolio_values = [] \n",
    "    \n",
    "    def __call__(self, r):\n",
    "        if len(r.environment.gym.sim.infos):\n",
    "            self.portfolio_values.append( r.environment.gym.sim.infos[-1]['portfolio_value'] )\n",
    "        if r.episode % self.log_intv == 0:\n",
    "#             df = pd.DataFrame(r.environment.gym.infos)\n",
    "            print(\n",
    "                \"Finished episode {ep} after {ts} timesteps (reward: {reward: 2.4f} [{rewards_min: 2.4f}, {rewards_max: 2.4f}]) portfolio_value: {portfolio_value: 2.4f} [{portfolio_value_min: 2.4f}, {portfolio_value_max: 2.4f}]\".\n",
    "                format(\n",
    "                    ep=r.episode,\n",
    "                    ts=r.timestep,\n",
    "                    reward=np.mean(r.episode_rewards[-self.log_intv:]),\n",
    "                    rewards_min=np.min(r.episode_rewards[-self.log_intv:]),\n",
    "                    rewards_max=np.max(r.episode_rewards[-self.log_intv:]),\n",
    "                    portfolio_value=np.mean(self.portfolio_values[-self.log_intv:]),\n",
    "                    portfolio_value_min=np.min(self.portfolio_values[-self.log_intv:]),\n",
    "                    portfolio_value_max=np.max(self.portfolio_values[-self.log_intv:])\n",
    "                )\n",
    "            )\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T23:22:52.365Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Callback EpisodeFinishedTQDM\n",
    "from tqdm import tqdm_notebook\n",
    "class EpisodeFinishedTQDM(object):\n",
    "    \"\"\"Logger for tensorforce using tqdm_notebook for jupyter-notebook.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_intv):\n",
    "        self.log_intv = log_intv\n",
    "        self.portfolio_values = [] \n",
    "        self.progbar = tqdm_notebook(desc='', total=self.params['nb_steps'], leave=True, mininterval=0.5)\n",
    "    \n",
    "    def __call__(self, r):\n",
    "        if len(r.environment.gym.sim.infos):\n",
    "            self.portfolio_values.append( r.environment.gym.sim.infos[-1]['portfolio_value'] )\n",
    "        if r.episode % self.log_intv == 0:\n",
    "#             df = pd.DataFrame(r.environment.gym.infos)\n",
    "            print(\n",
    "                \"Finished episode {ep} after {ts} timesteps (reward: {reward: 2.4f} [{rewards_min: 2.4f}, {rewards_max: 2.4f}]) portfolio_value: {portfolio_value: 2.4f} [{portfolio_value_min: 2.4f}, {portfolio_value_max: 2.4f}]\".\n",
    "                format(\n",
    "                    ep=r.episode,\n",
    "                    ts=r.timestep,\n",
    "                    reward=np.mean(r.episode_rewards[-self.log_intv:]),\n",
    "                    rewards_min=np.min(r.episode_rewards[-self.log_intv:]),\n",
    "                    rewards_max=np.max(r.episode_rewards[-self.log_intv:]),\n",
    "                    portfolio_value=np.mean(self.portfolio_values[-self.log_intv:]),\n",
    "                    portfolio_value_min=np.min(self.portfolio_values[-self.log_intv:]),\n",
    "                    portfolio_value_max=np.max(self.portfolio_values[-self.log_intv:])\n",
    "                )\n",
    "            )\n",
    "        desc = \"reward: {reward: 2.4f} [{rewards_min: 2.4f}, {rewards_max: 2.4f}], portfolio_value: {portfolio_value: 2.4f} [{portfolio_value_min: 2.4f}, {portfolio_value_max: 2.4f}]\".\n",
    "        format(\n",
    "            reward=np.mean(r.episode_rewards[-1:]),\n",
    "            rewards_min=np.min(r.episode_rewards[-1:]),\n",
    "            rewards_max=np.max(r.episode_rewards[-1:]),\n",
    "            portfolio_value=np.mean(self.portfolio_values[-1:]),\n",
    "            portfolio_value_min=np.min(self.portfolio_values[-1:]),\n",
    "            portfolio_value_max=np.max(self.portfolio_values[-1:])\n",
    "        )\n",
    "        self.progbar.desc = desc\n",
    "        self.progbar.update(1)  # update\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T23:22:52.366Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1000 after 30 timesteps (reward: -0.0017 [-0.0055,  0.0037]) portfolio_value:  0.9498 [ 0.8477,  1.1165]\n",
      "Finished episode 2000 after 30 timesteps (reward: -0.0018 [-0.0066,  0.0025]) portfolio_value:  0.9487 [ 0.8206,  1.0771]\n",
      "Finished episode 3000 after 30 timesteps (reward: -0.0018 [-0.0104,  0.0049]) portfolio_value:  0.9492 [ 0.7314,  1.1587]\n",
      "Finished episode 4000 after 30 timesteps (reward: -0.0018 [-0.0057,  0.0038]) portfolio_value:  0.9484 [ 0.8418,  1.1205]\n",
      "Finished episode 5000 after 30 timesteps (reward: -0.0017 [-0.0101,  0.0035]) portfolio_value:  0.9499 [ 0.7379,  1.1112]\n",
      "Finished episode 6000 after 30 timesteps (reward: -0.0018 [-0.0144,  0.0029]) portfolio_value:  0.9493 [ 0.6482,  1.0900]\n",
      "Finished episode 7000 after 30 timesteps (reward: -0.0018 [-0.0062,  0.0051]) portfolio_value:  0.9484 [ 0.8294,  1.1639]\n",
      "Finished episode 8000 after 30 timesteps (reward: -0.0018 [-0.0070,  0.0035]) portfolio_value:  0.9489 [ 0.8107,  1.1119]\n",
      "Finished episode 9000 after 30 timesteps (reward: -0.0018 [-0.0061,  0.0027]) portfolio_value:  0.9489 [ 0.8323,  1.0845]\n",
      "Finished episode 10000 after 30 timesteps (reward: -0.0018 [-0.0153,  0.0028]) portfolio_value:  0.9472 [ 0.6324,  1.0863]\n",
      "Finished episode 11000 after 30 timesteps (reward: -0.0018 [-0.0091,  0.0021]) portfolio_value:  0.9476 [ 0.7613,  1.0658]\n",
      "Finished episode 12000 after 30 timesteps (reward: -0.0018 [-0.0083,  0.0035]) portfolio_value:  0.9487 [ 0.7791,  1.1102]\n",
      "Finished episode 13000 after 30 timesteps (reward: -0.0018 [-0.0084,  0.0030]) portfolio_value:  0.9487 [ 0.7779,  1.0946]\n",
      "Finished episode 14000 after 30 timesteps (reward: -0.0018 [-0.0088,  0.0034]) portfolio_value:  0.9474 [ 0.7680,  1.1074]\n",
      "Finished episode 15000 after 30 timesteps (reward: -0.0018 [-0.0067,  0.0041]) portfolio_value:  0.9485 [ 0.8170,  1.1321]\n",
      "Finished episode 16000 after 30 timesteps (reward: -0.0018 [-0.0083,  0.0037]) portfolio_value:  0.9475 [ 0.7806,  1.1169]\n",
      "Finished episode 17000 after 30 timesteps (reward: -0.0018 [-0.0070,  0.0021]) portfolio_value:  0.9481 [ 0.8111,  1.0643]\n",
      "Finished episode 18000 after 30 timesteps (reward: -0.0018 [-0.0062,  0.0034]) portfolio_value:  0.9484 [ 0.8314,  1.1073]\n",
      "Finished episode 19000 after 30 timesteps (reward: -0.0018 [-0.0093,  0.0017]) portfolio_value:  0.9488 [ 0.7564,  1.0531]\n",
      "Finished episode 20000 after 30 timesteps (reward: -0.0018 [-0.0172,  0.0034]) portfolio_value:  0.9488 [ 0.5973,  1.1090]\n",
      "Finished episode 21000 after 30 timesteps (reward: -0.0017 [-0.0083,  0.0046]) portfolio_value:  0.9496 [ 0.7801,  1.1478]\n",
      "Finished episode 22000 after 30 timesteps (reward: -0.0018 [-0.0065,  0.0029]) portfolio_value:  0.9479 [ 0.8240,  1.0918]\n",
      "Finished episode 23000 after 30 timesteps (reward: -0.0017 [-0.0061,  0.0038]) portfolio_value:  0.9500 [ 0.8326,  1.1219]\n",
      "Finished episode 24000 after 30 timesteps (reward: -0.0017 [-0.0057,  0.0022]) portfolio_value:  0.9500 [ 0.8430,  1.0692]\n",
      "Finished episode 25000 after 30 timesteps (reward: -0.0017 [-0.0101,  0.0039]) portfolio_value:  0.9496 [ 0.7387,  1.1236]\n",
      "Finished episode 26000 after 30 timesteps (reward: -0.0017 [-0.0107,  0.0018]) portfolio_value:  0.9495 [ 0.7248,  1.0559]\n",
      "Finished episode 27000 after 30 timesteps (reward: -0.0018 [-0.0064,  0.0022]) portfolio_value:  0.9492 [ 0.8261,  1.0694]\n",
      "Finished episode 28000 after 30 timesteps (reward: -0.0018 [-0.0060,  0.0040]) portfolio_value:  0.9493 [ 0.8349,  1.1291]\n",
      "Finished episode 29000 after 30 timesteps (reward: -0.0018 [-0.0060,  0.0038]) portfolio_value:  0.9493 [ 0.8361,  1.1212]\n",
      "Finished episode 30000 after 30 timesteps (reward: -0.0018 [-0.0080,  0.0022]) portfolio_value:  0.9485 [ 0.7867,  1.0673]\n",
      "Finished episode 31000 after 30 timesteps (reward: -0.0017 [-0.0061,  0.0037]) portfolio_value:  0.9495 [ 0.8322,  1.1183]\n",
      "Finished episode 32000 after 30 timesteps (reward: -0.0018 [-0.0057,  0.0034]) portfolio_value:  0.9481 [ 0.8419,  1.1069]\n",
      "Finished episode 33000 after 30 timesteps (reward: -0.0018 [-0.0080,  0.0019]) portfolio_value:  0.9486 [ 0.7876,  1.0575]\n",
      "Finished episode 34000 after 30 timesteps (reward: -0.0018 [-0.0062,  0.0026]) portfolio_value:  0.9476 [ 0.8305,  1.0803]\n",
      "Finished episode 35000 after 30 timesteps (reward: -0.0018 [-0.0058,  0.0025]) portfolio_value:  0.9486 [ 0.8392,  1.0794]\n",
      "Finished episode 36000 after 30 timesteps (reward: -0.0018 [-0.0063,  0.0031]) portfolio_value:  0.9492 [ 0.8281,  1.0973]\n",
      "Finished episode 37000 after 30 timesteps (reward: -0.0018 [-0.0056,  0.0049]) portfolio_value:  0.9490 [ 0.8443,  1.1572]\n",
      "Finished episode 38000 after 30 timesteps (reward: -0.0018 [-0.0060,  0.0029]) portfolio_value:  0.9491 [ 0.8356,  1.0895]\n",
      "Finished episode 39000 after 30 timesteps (reward: -0.0018 [-0.0083,  0.0034]) portfolio_value:  0.9479 [ 0.7796,  1.1073]\n",
      "Finished episode 40000 after 30 timesteps (reward: -0.0018 [-0.0069,  0.0025]) portfolio_value:  0.9493 [ 0.8131,  1.0774]\n",
      "Finished episode 41000 after 30 timesteps (reward: -0.0018 [-0.0073,  0.0029]) portfolio_value:  0.9488 [ 0.8043,  1.0908]\n"
     ]
    }
   ],
   "source": [
    "runner.run(\n",
    "    episodes=2e6, max_timesteps=200, episode_finished=EpisodeFinished(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T23:22:52.367Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO turn off learning during test\n",
    "runner_test = Runner(agent=agent, environment=environment_test)\n",
    "runner_test.run(\n",
    "    episodes=100, max_timesteps=128, episode_finished=EpisodeFinished(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:36:25.184976Z",
     "start_time": "2017-07-16T12:36:10.079463+08:00"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
