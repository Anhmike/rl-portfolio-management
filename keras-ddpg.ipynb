{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:14.074124Z",
     "start_time": "2017-07-21T14:20:13.395440+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:__main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# numeric\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:29.894963Z",
     "start_time": "2017-07-21T14:20:14.075558+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# reinforcement learning\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization, Conv1D, InputLayer, Dropout, regularizers, Conv2D, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:29.923199Z",
     "start_time": "2017-07-21T14:20:29.896703+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:29.944942Z",
     "start_time": "2017-07-21T14:20:29.924916+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "You can see the base environment class [here](https://github.com/openai/gym/blob/master/gym/core.py#L13) and openai's nice docs [here](https://gym.openai.com/docs). My environment is in `src/environments/portfolio.py` and the PortfolioEnvironment load a datasource and simulation subclass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:29.992689Z",
     "start_time": "2017-07-21T14:20:29.946617+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:31.462782Z",
     "start_time": "2017-07-21T14:20:29.994448+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_hdf('./data/poloniex_30m.hf',key='train')\n",
    "env = PortfolioEnv(\n",
    "    df=df_train,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augment=0.0000, # let just overfit first,\n",
    "    trading_cost=0, # let just overfit first,\n",
    "    window_length = window_length,\n",
    "    \n",
    ")\n",
    "env.seed = 0   \n",
    "\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n",
    "env_test = PortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augment=0.00,\n",
    "    trading_cost=0, # let just overfit first\n",
    "    window_length=window_length,\n",
    ")\n",
    "env_test.seed = 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-04T01:42:37.345932Z",
     "start_time": "2017-07-04T09:42:37.328860+08:00"
    },
    "collapsed": true
   },
   "source": [
    "# Model\n",
    "\n",
    "arXiv:1612.01277 indicated that CNN's are just as effective. That's great because I like them, they are fast so I can try more things and see the results faster. So we will be using a CNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:31.875363Z",
     "start_time": "2017-07-21T14:20:31.464698+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1, 5, 50, 3)       0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 5, 50, 3)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 5, 48, 2)          20        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 5, 48, 2)          8         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 1, 20)          1940      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 5, 1, 20)          80        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 1, 1)           21        \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 5, 1, 1)           4         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 36        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 2,109\n",
      "Trainable params: 2,063\n",
      "Non-trainable params: 46\n",
      "_________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "observation_input (InputLayer)   (None, 1, 5, 50, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 5, 50, 3)      0           observation_input[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 5, 48, 2)      20          reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 5, 48, 2)      8           conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 5, 1, 20)      1940        batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 5, 1, 20)      80          conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "action_input (InputLayer)        (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 100)           0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 106)           0           action_input[0][0]               \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           13696       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 128)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 128)           512         activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 64)            8256        batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 64)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 64)            256         activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 32)            2080        batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 32)            0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 32)            128         activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             33          batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 1)             0           dense_5[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 27,009\n",
      "Trainable params: 26,517\n",
      "Non-trainable params: 492\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, merge, Reshape\n",
    "from keras.layers import concatenate, Conv2D\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.models import Model\n",
    "\n",
    "window_length=50\n",
    "nb_actions=env.action_space.shape[0]\n",
    "reg=1e-8\n",
    "\n",
    "# Simple CNN actor model\n",
    "actor = Sequential()\n",
    "actor.add(InputLayer(input_shape=(1,)+env.observation_space.shape))\n",
    "actor.add(Reshape(env.observation_space.shape))\n",
    "actor.add(Conv2D(\n",
    "    filters=2,\n",
    "    kernel_size=(1,3),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(BatchNormalization()) # lets add batch norm to decrease training time\n",
    "\n",
    "actor.add(Conv2D(\n",
    "    filters=20,\n",
    "    kernel_size=(1,window_length-2),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(BatchNormalization())\n",
    "\n",
    "actor.add(Conv2D(\n",
    "    filters=1,\n",
    "    kernel_size=(1,1),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(BatchNormalization())\n",
    "\n",
    "actor.add(Flatten())\n",
    "actor.add(Dense(\n",
    "    nb_actions, \n",
    "    kernel_regularizer=l2(reg)\n",
    ")) # this adds cash bias\n",
    "actor.add(Activation('softmax'))\n",
    "print(actor.summary())\n",
    "\n",
    "# Lets have nice flexible critic so it can approximate the Q-function\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "\n",
    "observation_input = Input(shape=(1,)+env.observation_space.shape, name='observation_input')\n",
    "y = Reshape(env.observation_space.shape)(observation_input)\n",
    "\n",
    "y = Conv2D(\n",
    "    filters=2,\n",
    "    kernel_size=(1,3),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    ")(y)\n",
    "y = BatchNormalization()(y) # lets add batch norm to decrease training time\n",
    "\n",
    "y = Conv2D(\n",
    "    filters=20,\n",
    "    kernel_size=(1,window_length-2),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    ")(y)\n",
    "y = BatchNormalization()(y)\n",
    "\n",
    "y = Flatten()(y)\n",
    "\n",
    "x = concatenate([action_input, y])\n",
    "x = Dense(128)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:34.184253Z",
     "start_time": "2017-07-21T14:20:31.876818+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rl.agents.ddpg.DDPGAgent at 0x7f52132b12e8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.agents.ddpg import DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "# We configure and compile our agent.\n",
    "\n",
    "# We are providing the last 50 steps so we don't need memory, use window_lenght=1 as placeholder\n",
    "memory = SequentialMemory(limit=1000, window_length=1)\n",
    "\n",
    "random_process = OrnsteinUhlenbeckProcess(\n",
    "    size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "\n",
    "agent = DDPGAgent(\n",
    "    nb_actions=nb_actions,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    critic_action_input=action_input,\n",
    "    random_process=random_process,\n",
    "    memory=memory,\n",
    "    batch_size=50,\n",
    "    nb_steps_warmup_critic=100,\n",
    "    nb_steps_warmup_actor=100,    \n",
    "    gamma=.00, # discounted factor of zero as per paper\n",
    "    target_model_update=1e-3\n",
    ")\n",
    "\n",
    "agent.compile(Adam(lr=3e-5), metrics=['mse'])\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:34.229358Z",
     "start_time": "2017-07-21T14:20:34.185771+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.callbacks.keras_rl_callbacks import TrainIntervalLoggerTQDMNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:34.257031Z",
     "start_time": "2017-07-21T14:20:34.231830+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sharpe(returns, freq=30, rfr=0):\n",
    "    \"\"\"Given a set of returns, calculates naive (rfr=0) sharpe (eq 28) \"\"\"\n",
    "    return (np.sqrt(freq) * np.mean(returns-rfr)) / np.std(returns - rfr)\n",
    "\n",
    "\n",
    "def MDD(returns):\n",
    "    \"\"\"Max drawdown.\"\"\"\n",
    "    peak = returns.max()\n",
    "    i = returns.argmax()\n",
    "    trough = returns[returns.argmax():].min()\n",
    "    return (trough-peak)/trough "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:34.294626Z",
     "start_time": "2017-07-21T14:20:34.258887+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # https://github.com/matthiasplappert/keras-rl/blob/master/rl/callbacks.py#L104\n",
    "from rl.callbacks import TrainEpisodeLogger\n",
    "class TrainEpisodeLoggerPortfolio(TrainEpisodeLogger):\n",
    "    \"\"\"Print custom stats every <log_intv> episode.\"\"\"\n",
    "    def __init__(self, log_intv):\n",
    "        super().__init__()\n",
    "        self.log_intv=log_intv\n",
    "        self.episode_metrics={} # custom metrics\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        \n",
    "        # save custom metrics\n",
    "        df = pd.DataFrame(self.env.infos)\n",
    "        self.episode_metrics[episode]=dict(\n",
    "            max_drawdown=MDD(df.rate_of_return+1), \n",
    "            sharpe=sharpe(df.rate_of_return+1), \n",
    "            accumulated_portfolio_value=df.portfolio_value.iloc[-1],\n",
    "            mean_market_return=df.mean_market_returns.cumprod().iloc[-1],\n",
    "            cash_bias=df.weights.apply(lambda x:x[0]).mean()\n",
    "        )\n",
    "        \n",
    "        if episode%self.log_intv==0:\n",
    "            # print normal metrics\n",
    "            super().on_episode_end(episode, logs)\n",
    "            \n",
    "            # print custom metrics for last N episodes\n",
    "            df = pd.DataFrame(self.episode_metrics).T[-self.log_intv:]          \n",
    "            for col in df.columns:\n",
    "                print('{name:25.25s}: {mean: 10.6f} [{min: 10.6f}, {max: 10.6f}]'.format(\n",
    "                    name=df[col].name, \n",
    "                    min=df[col].min(), \n",
    "                    mean=df[col].mean(), \n",
    "                    max=df[col].max(), \n",
    "                ))\n",
    "            print('')           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:34.317736Z",
     "start_time": "2017-07-21T14:20:34.296326+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rl.callbacks import ModelIntervalCheckpoint\n",
    "save_path= 'outputs/agent_portfolio-ddpg-keras/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:20:44.630741Z",
     "start_time": "2017-07-21T14:20:44.576162+08:00"
    }
   },
   "outputs": [],
   "source": [
    "agent.load_weights(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-20T15:35:52.662Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484571a671f24313a7600c226a79e277"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000.0 steps ...\n",
      "Training for 2000000.0 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "      30/2000000.0: episode: 1, duration: 0.252s, episode steps: 30, steps per second: 119, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.132 [-0.286, 0.445], mean observation: 0.986 [0.916, 1.049], loss: --, mean_squared_error: --, mean_q: --\n",
      "accumulated_portfolio_val:   1.007654 [  1.007654,   1.007654]\n",
      "cash_bias                :   0.182006 [  0.182006,   0.182006]\n",
      "max_drawdown             :  -0.016880 [ -0.016880,  -0.016880]\n",
      "mean_market_return       :   1.008074 [  1.008074,   1.008074]\n",
      "sharpe                   :  1373.525454 [ 1373.525454,  1373.525454]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.008] - loss: 0.069 - mean_squared_error: 0.138 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.006 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.159 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "Step 15000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "   15030/2000000.0: episode: 501, duration: 0.922s, episode steps: 30, steps per second: 33, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.126 [-0.125, 0.293], mean observation: 1.002 [0.909, 1.087], loss: 0.001158, mean_squared_error: 0.002316, mean_q: 0.000008\n",
      "accumulated_portfolio_val:   1.010805 [  0.867529,   1.252763]\n",
      "cash_bias                :   0.188445 [  0.040831,   0.439787]\n",
      "max_drawdown             :  -0.019891 [ -0.093274,   0.000000]\n",
      "mean_market_return       :   1.011159 [  0.920785,   1.164541]\n",
      "sharpe                   :  1295.038560 [ 284.439757,  3830.464276]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.003] - loss: 0.001 - mean_squared_error: 0.002 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.162 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "Step 30000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.176 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "   30030/2000000.0: episode: 1001, duration: 0.996s, episode steps: 30, steps per second: 30, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.175 [-0.078, 0.457], mean observation: 0.997 [0.882, 1.054], loss: 0.000027, mean_squared_error: 0.000055, mean_q: 0.000001\n",
      "accumulated_portfolio_val:   1.004821 [  0.907691,   1.162252]\n",
      "cash_bias                :   0.196355 [  0.040883,   0.670994]\n",
      "max_drawdown             :  -0.018098 [ -0.054011,   0.000000]\n",
      "mean_market_return       :   1.005097 [  0.910998,   1.126893]\n",
      "sharpe                   :  1396.453597 [ 427.941703,  4907.231563]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.007, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.167 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "Step 45000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "   45030/2000000.0: episode: 1501, duration: 0.915s, episode steps: 30, steps per second: 33, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.262 [0.044, 0.544], mean observation: 0.986 [0.898, 1.054], loss: 0.000005, mean_squared_error: 0.000010, mean_q: 0.000016\n",
      "accumulated_portfolio_val:   1.006281 [  0.805465,   1.211831]\n",
      "cash_bias                :   0.194952 [  0.042231,   0.548327]\n",
      "max_drawdown             :  -0.019103 [ -0.071371,   0.000000]\n",
      "mean_market_return       :   1.006237 [  0.854680,   1.097494]\n",
      "sharpe                   :  1341.234553 [ 362.310585,  3478.575770]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.169 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "Step 60000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.165 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "   60030/2000000.0: episode: 2001, duration: 1.000s, episode steps: 30, steps per second: 30, episode reward: -0.000, mean reward: -0.000 [-0.000, 0.000], mean action: 0.189 [-0.039, 0.403], mean observation: 0.978 [0.898, 1.047], loss: 0.000005, mean_squared_error: 0.000010, mean_q: 0.000021\n",
      "accumulated_portfolio_val:   1.007584 [  0.874287,   1.236916]\n",
      "cash_bias                :   0.192379 [  0.038380,   0.530991]\n",
      "max_drawdown             :  -0.019241 [ -0.181202,   0.000000]\n",
      "mean_market_return       :   1.008276 [  0.936294,   1.170414]\n",
      "sharpe                   :  1350.999421 [ 203.635855,  3450.242653]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.165 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "Step 75000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "   75030/2000000.0: episode: 2501, duration: 0.914s, episode steps: 30, steps per second: 33, episode reward: 0.000, mean reward: 0.000 [-0.001, 0.000], mean action: 0.146 [-0.091, 0.400], mean observation: 0.985 [0.782, 1.129], loss: 0.000002, mean_squared_error: 0.000003, mean_q: 0.000003\n",
      "accumulated_portfolio_val:   1.006712 [  0.912440,   1.112918]\n",
      "cash_bias                :   0.191069 [  0.045835,   0.574524]\n",
      "max_drawdown             :  -0.019952 [ -0.166899,   0.000000]\n",
      "mean_market_return       :   1.006451 [  0.915644,   1.108304]\n",
      "sharpe                   :  1349.316005 [ 230.262148,  3827.014771]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.161 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "Step 90000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.170 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "   90030/2000000.0: episode: 3001, duration: 1.012s, episode steps: 30, steps per second: 30, episode reward: -0.000, mean reward: -0.000 [-0.000, 0.000], mean action: 0.173 [-0.018, 0.368], mean observation: 0.990 [0.899, 1.060], loss: 0.000001, mean_squared_error: 0.000001, mean_q: 0.000014\n",
      "accumulated_portfolio_val:   1.008043 [  0.918122,   1.167036]\n",
      "cash_bias                :   0.193221 [  0.043254,   0.556918]\n",
      "max_drawdown             :  -0.020069 [ -0.145159,   0.000000]\n",
      "mean_market_return       :   1.008078 [  0.928626,   1.131092]\n",
      "sharpe                   :  1317.147486 [ 246.679350,  3031.928519]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.162 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "Step 105000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  105030/2000000.0: episode: 3501, duration: 0.920s, episode steps: 30, steps per second: 33, episode reward: -0.001, mean reward: -0.000 [-0.000, 0.000], mean action: 0.219 [0.012, 0.729], mean observation: 1.009 [0.943, 1.100], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000001\n",
      "accumulated_portfolio_val:   1.008412 [  0.908828,   1.213078]\n",
      "cash_bias                :   0.188269 [  0.039821,   0.434827]\n",
      "max_drawdown             :  -0.019754 [ -0.255824,   0.000000]\n",
      "mean_market_return       :   1.007514 [  0.919927,   1.132786]\n",
      "sharpe                   :  1333.031619 [ 168.770046,  2959.971925]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.165 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "Step 120000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "  120030/2000000.0: episode: 4001, duration: 1.014s, episode steps: 30, steps per second: 30, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.187 [-0.020, 0.449], mean observation: 1.001 [0.933, 1.053], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000005\n",
      "accumulated_portfolio_val:   1.008356 [  0.898947,   1.158031]\n",
      "cash_bias                :   0.194641 [  0.040598,   0.674194]\n",
      "max_drawdown             :  -0.019371 [ -0.164783,   0.000000]\n",
      "mean_market_return       :   1.008563 [  0.932309,   1.161902]\n",
      "sharpe                   :  1307.063941 [ 249.027962,  3010.936910]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.165 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "Step 135000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  135030/2000000.0: episode: 4501, duration: 0.923s, episode steps: 30, steps per second: 33, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.178 [-0.085, 0.536], mean observation: 1.011 [0.951, 1.088], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000001\n",
      "accumulated_portfolio_val:   1.007493 [  0.850394,   1.160440]\n",
      "cash_bias                :   0.194931 [  0.040976,   0.586744]\n",
      "max_drawdown             :  -0.019762 [ -0.100282,   0.000000]\n",
      "mean_market_return       :   1.006548 [  0.848026,   1.112565]\n",
      "sharpe                   :  1325.930468 [ 175.486334,  3594.258184]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "Step 150000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.164 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "  150030/2000000.0: episode: 5001, duration: 0.998s, episode steps: 30, steps per second: 30, episode reward: -0.001, mean reward: -0.000 [-0.000, 0.000], mean action: 0.189 [-0.036, 0.484], mean observation: 1.000 [0.924, 1.126], loss: 0.000000, mean_squared_error: 0.000001, mean_q: 0.000009\n",
      "accumulated_portfolio_val:   1.007342 [  0.903938,   1.157376]\n",
      "cash_bias                :   0.188125 [  0.041667,   0.447104]\n",
      "max_drawdown             :  -0.018805 [ -0.095430,   0.000000]\n",
      "mean_market_return       :   1.008176 [  0.848720,   1.158541]\n",
      "sharpe                   :  1334.575393 [ 257.570800,  3626.453777]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.169 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "Step 165000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  165030/2000000.0: episode: 5501, duration: 0.914s, episode steps: 30, steps per second: 33, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.146 [-0.103, 0.438], mean observation: 1.008 [0.898, 1.082], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000005\n",
      "accumulated_portfolio_val:   1.007678 [  0.891741,   1.154640]\n",
      "cash_bias                :   0.196190 [  0.039089,   0.440911]\n",
      "max_drawdown             :  -0.017945 [ -0.068435,   0.000000]\n",
      "mean_market_return       :   1.006815 [  0.848720,   1.106454]\n",
      "sharpe                   :  1363.236342 [ 243.336687,  3756.993404]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.008] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.172 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "Step 180000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.005, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.168 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "  180030/2000000.0: episode: 6001, duration: 1.022s, episode steps: 30, steps per second: 29, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.001], mean action: 0.150 [-0.112, 0.452], mean observation: 0.994 [0.817, 1.127], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000009\n",
      "accumulated_portfolio_val:   1.007610 [  0.872043,   1.277010]\n",
      "cash_bias                :   0.196453 [  0.041560,   0.490237]\n",
      "max_drawdown             :  -0.019182 [ -0.148882,   0.000000]\n",
      "mean_market_return       :   1.007779 [  0.928926,   1.129833]\n",
      "sharpe                   :  1337.126411 [ 210.247703,  3824.461918]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.167 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "Step 195000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  195030/2000000.0: episode: 6501, duration: 0.928s, episode steps: 30, steps per second: 32, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.149 [-0.133, 0.379], mean observation: 1.001 [0.962, 1.036], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000014\n",
      "accumulated_portfolio_val:   1.006088 [  0.895500,   1.166820]\n",
      "cash_bias                :   0.196826 [  0.045872,   0.641466]\n",
      "max_drawdown             :  -0.019151 [ -0.078875,   0.000000]\n",
      "mean_market_return       :   1.006168 [  0.910998,   1.163961]\n",
      "sharpe                   :  1350.801527 [ 295.398289,  3615.457546]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "Step 210000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.164 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 22 (210000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  210030/2000000.0: episode: 7001, duration: 1.019s, episode steps: 30, steps per second: 29, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.209 [0.071, 0.442], mean observation: 1.004 [0.908, 1.077], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000004\n",
      "accumulated_portfolio_val:   1.004450 [  0.886004,   1.140253]\n",
      "cash_bias                :   0.188195 [  0.042458,   0.426703]\n",
      "max_drawdown             :  -0.019503 [ -0.093260,   0.000000]\n",
      "mean_market_return       :   1.005047 [  0.881979,   1.157721]\n",
      "sharpe                   :  1311.064921 [ 316.101648,  3452.464668]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.161 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "Step 225000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  225030/2000000.0: episode: 7501, duration: 0.914s, episode steps: 30, steps per second: 33, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.169 [-0.100, 0.401], mean observation: 1.015 [0.917, 1.099], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000000\n",
      "accumulated_portfolio_val:   1.006983 [  0.873886,   1.204421]\n",
      "cash_bias                :   0.188286 [  0.042428,   0.610380]\n",
      "max_drawdown             :  -0.018191 [ -0.082555,   0.000000]\n",
      "mean_market_return       :   1.008427 [  0.875036,   1.138462]\n",
      "sharpe                   :  1347.128053 [ 321.352992,  4014.372317]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.164 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "Step 240000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.162 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "  240030/2000000.0: episode: 8001, duration: 1.001s, episode steps: 30, steps per second: 30, episode reward: -0.001, mean reward: -0.000 [-0.001, 0.000], mean action: 0.130 [-0.230, 0.412], mean observation: 1.001 [0.809, 1.298], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000005\n",
      "accumulated_portfolio_val:   1.007151 [  0.899473,   1.162142]\n",
      "cash_bias                :   0.190394 [  0.038147,   0.570681]\n",
      "max_drawdown             :  -0.018656 [ -0.157473,   0.000000]\n",
      "mean_market_return       :   1.007704 [  0.883825,   1.134824]\n",
      "sharpe                   :  1359.348913 [ 251.357817,  3431.209046]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.165 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "Step 255000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  255030/2000000.0: episode: 8501, duration: 0.918s, episode steps: 30, steps per second: 33, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.001], mean action: 0.157 [-0.109, 0.417], mean observation: 1.011 [0.904, 1.165], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000003\n",
      "accumulated_portfolio_val:   1.006049 [  0.852699,   1.195015]\n",
      "cash_bias                :   0.189342 [  0.041873,   0.525499]\n",
      "max_drawdown             :  -0.019181 [ -0.103489,   0.000000]\n",
      "mean_market_return       :   1.006446 [  0.850383,   1.112649]\n",
      "sharpe                   :  1330.029231 [ 170.737347,  3288.151862]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.168 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "Step 270000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.005, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.169 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "  270030/2000000.0: episode: 9001, duration: 1.011s, episode steps: 30, steps per second: 30, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.147 [-0.008, 0.310], mean observation: 1.005 [0.877, 1.165], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000016\n",
      "accumulated_portfolio_val:   1.008297 [  0.865353,   1.133413]\n",
      "cash_bias                :   0.199206 [  0.041794,   0.627929]\n",
      "max_drawdown             :  -0.019412 [ -0.142287,   0.000000]\n",
      "mean_market_return       :   1.009780 [  0.929943,   1.138462]\n",
      "sharpe                   :  1338.658976 [ 253.056093,  3968.625559]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.178 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "Step 285000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  285030/2000000.0: episode: 9501, duration: 0.915s, episode steps: 30, steps per second: 33, episode reward: 0.001, mean reward: 0.000 [-0.001, 0.001], mean action: 0.180 [-0.122, 0.550], mean observation: 0.997 [0.668, 1.362], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000013\n",
      "accumulated_portfolio_val:   1.004887 [  0.876659,   1.122973]\n",
      "cash_bias                :   0.199337 [  0.044621,   0.702284]\n",
      "max_drawdown             :  -0.019332 [ -0.084462,   0.000000]\n",
      "mean_market_return       :   1.005145 [  0.918913,   1.112565]\n",
      "sharpe                   :  1328.043425 [ 345.266861,  4323.562208]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.163 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "Step 300000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.165 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "  300030/2000000.0: episode: 10001, duration: 0.995s, episode steps: 30, steps per second: 30, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.106 [-0.178, 0.350], mean observation: 1.000 [0.936, 1.091], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000008\n",
      "accumulated_portfolio_val:   1.009967 [  0.909287,   1.126714]\n",
      "cash_bias                :   0.191699 [  0.039023,   0.518564]\n",
      "max_drawdown             :  -0.019369 [ -0.135279,   0.000000]\n",
      "mean_market_return       :   1.009886 [  0.845515,   1.126893]\n",
      "sharpe                   :  1327.301187 [ 251.339983,  3027.181744]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.172 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "Step 315000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  315030/2000000.0: episode: 10501, duration: 0.922s, episode steps: 30, steps per second: 33, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.092 [-0.446, 0.372], mean observation: 1.013 [0.921, 1.216], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000008\n",
      "accumulated_portfolio_val:   1.007969 [  0.889875,   1.146733]\n",
      "cash_bias                :   0.197531 [  0.041189,   0.609285]\n",
      "max_drawdown             :  -0.019314 [ -0.082008,   0.000000]\n",
      "mean_market_return       :   1.008037 [  0.929005,   1.116586]\n",
      "sharpe                   :  1346.952500 [ 328.800763,  3723.073282]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "Step 330000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.170 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "  330030/2000000.0: episode: 11001, duration: 1.287s, episode steps: 30, steps per second: 23, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.192 [-0.053, 0.558], mean observation: 1.010 [0.942, 1.126], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000016\n",
      "accumulated_portfolio_val:   1.010051 [  0.895428,   1.147138]\n",
      "cash_bias                :   0.195235 [  0.040282,   0.620973]\n",
      "max_drawdown             :  -0.019903 [ -0.118940,   0.000000]\n",
      "mean_market_return       :   1.009824 [  0.910822,   1.122414]\n",
      "sharpe                   :  1328.237536 [ 239.871513,  3988.378611]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.164 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "Step 345000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  345030/2000000.0: episode: 11501, duration: 0.926s, episode steps: 30, steps per second: 32, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.219 [0.090, 0.333], mean observation: 1.001 [0.914, 1.054], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000000\n",
      "accumulated_portfolio_val:   1.008450 [  0.903939,   1.177250]\n",
      "cash_bias                :   0.191347 [  0.041410,   0.601826]\n",
      "max_drawdown             :  -0.019516 [ -0.160020,   0.000000]\n",
      "mean_market_return       :   1.008874 [  0.875036,   1.145252]\n",
      "sharpe                   :  1332.951281 [ 250.154750,  4090.790991]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.164 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "Step 360000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.169 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "  360030/2000000.0: episode: 12001, duration: 0.999s, episode steps: 30, steps per second: 30, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.131 [-0.212, 0.297], mean observation: 1.000 [0.960, 1.039], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000002\n",
      "accumulated_portfolio_val:   1.009069 [  0.915482,   1.168883]\n",
      "cash_bias                :   0.193733 [  0.044289,   0.652343]\n",
      "max_drawdown             :  -0.018850 [ -0.121577,   0.000000]\n",
      "mean_market_return       :   1.008355 [  0.929472,   1.100187]\n",
      "sharpe                   :  1356.692105 [ 242.693121,  3472.218786]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.168 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "Step 375000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  375030/2000000.0: episode: 12501, duration: 0.916s, episode steps: 30, steps per second: 33, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.164 [-0.049, 0.373], mean observation: 0.995 [0.944, 1.057], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000010\n",
      "accumulated_portfolio_val:   1.006753 [  0.911838,   1.173865]\n",
      "cash_bias                :   0.194311 [  0.044377,   0.480015]\n",
      "max_drawdown             :  -0.018845 [ -0.147784,   0.000000]\n",
      "mean_market_return       :   1.008230 [  0.930162,   1.172381]\n",
      "sharpe                   :  1341.747251 [ 235.149207,  4300.699871]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.165 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "Step 390000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.168 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "  390030/2000000.0: episode: 13001, duration: 0.998s, episode steps: 30, steps per second: 30, episode reward: -0.000, mean reward: -0.000 [-0.000, 0.000], mean action: 0.155 [-0.020, 0.482], mean observation: 1.003 [0.807, 1.133], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000000\n",
      "accumulated_portfolio_val:   1.007569 [  0.904824,   1.170128]\n",
      "cash_bias                :   0.193581 [  0.037337,   0.489195]\n",
      "max_drawdown             :  -0.018936 [ -0.065437,   0.000000]\n",
      "mean_market_return       :   1.008133 [  0.917121,   1.166964]\n",
      "sharpe                   :  1348.172065 [ 397.648182,  4518.714362]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.164 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "Step 405000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  405030/2000000.0: episode: 13501, duration: 0.913s, episode steps: 30, steps per second: 33, episode reward: -0.000, mean reward: -0.000 [-0.000, 0.000], mean action: 0.127 [-0.054, 0.370], mean observation: 1.009 [0.915, 1.145], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000024\n",
      "accumulated_portfolio_val:   1.007255 [  0.850467,   1.115343]\n",
      "cash_bias                :   0.192422 [  0.043323,   0.577022]\n",
      "max_drawdown             :  -0.018899 [ -0.081347,   0.000000]\n",
      "mean_market_return       :   1.007090 [  0.839279,   1.103638]\n",
      "sharpe                   :  1349.568533 [ 193.130406,  3688.844095]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.008, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "Step 420000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.168 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "  420030/2000000.0: episode: 14001, duration: 1.004s, episode steps: 30, steps per second: 30, episode reward: -0.001, mean reward: -0.000 [-0.001, 0.000], mean action: 0.158 [0.034, 0.302], mean observation: 1.039 [0.892, 1.421], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000016\n",
      "accumulated_portfolio_val:   1.007749 [  0.792083,   1.102395]\n",
      "cash_bias                :   0.193481 [  0.045012,   0.658316]\n",
      "max_drawdown             :  -0.018696 [ -0.262141,   0.000000]\n",
      "mean_market_return       :   1.008350 [  0.848208,   1.100794]\n",
      "sharpe                   :  1336.761432 [ 150.618989,  4329.860672]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.164 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "Step 435000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  435030/2000000.0: episode: 14501, duration: 0.925s, episode steps: 30, steps per second: 32, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.172 [0.020, 0.425], mean observation: 0.998 [0.918, 1.067], loss: 0.000000, mean_squared_error: 0.000001, mean_q: 0.000001\n",
      "accumulated_portfolio_val:   1.006388 [  0.858304,   1.202730]\n",
      "cash_bias                :   0.190565 [  0.037799,   0.707949]\n",
      "max_drawdown             :  -0.018822 [ -0.180411,   0.000000]\n",
      "mean_market_return       :   1.008133 [  0.926452,   1.181485]\n",
      "sharpe                   :  1372.028427 [ 192.994197,  3923.111367]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.164 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "Step 450000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.165 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "  450030/2000000.0: episode: 15001, duration: 1.014s, episode steps: 30, steps per second: 30, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.172 [-0.053, 0.456], mean observation: 1.014 [0.783, 1.340], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000009\n",
      "accumulated_portfolio_val:   1.008257 [  0.917758,   1.202061]\n",
      "cash_bias                :   0.192095 [  0.038546,   0.533053]\n",
      "max_drawdown             :  -0.018750 [ -0.083552,   0.000000]\n",
      "mean_market_return       :   1.007904 [  0.923670,   1.177038]\n",
      "sharpe                   :  1361.869151 [ 462.805528,  3704.555675]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.160 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "Step 465000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  465030/2000000.0: episode: 15501, duration: 0.920s, episode steps: 30, steps per second: 33, episode reward: -0.000, mean reward: -0.000 [-0.000, 0.000], mean action: 0.162 [-0.185, 0.480], mean observation: 1.004 [0.843, 1.125], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000016\n",
      "accumulated_portfolio_val:   1.006687 [  0.867623,   1.201478]\n",
      "cash_bias                :   0.185175 [  0.037891,   0.541069]\n",
      "max_drawdown             :  -0.020052 [ -0.177453,   0.000000]\n",
      "mean_market_return       :   1.008185 [  0.918776,   1.175881]\n",
      "sharpe                   :  1351.630438 [ 223.412125,  4700.500370]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.162 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "Step 480000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.005, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.168 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "  480030/2000000.0: episode: 16001, duration: 1.022s, episode steps: 30, steps per second: 29, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.234 [-0.006, 0.480], mean observation: 1.006 [0.938, 1.076], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000000\n",
      "accumulated_portfolio_val:   1.006144 [  0.862871,   1.099103]\n",
      "cash_bias                :   0.195488 [  0.046404,   0.577700]\n",
      "max_drawdown             :  -0.018652 [ -0.201453,   0.000000]\n",
      "mean_market_return       :   1.007038 [  0.850383,   1.088585]\n",
      "sharpe                   :  1367.531655 [ 189.592793,  3197.536432]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.172 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "Step 495000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  495030/2000000.0: episode: 16501, duration: 0.920s, episode steps: 30, steps per second: 33, episode reward: -0.001, mean reward: -0.000 [-0.001, 0.000], mean action: 0.158 [-0.169, 0.428], mean observation: 1.006 [0.777, 1.259], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000002\n",
      "accumulated_portfolio_val:   1.005307 [  0.885932,   1.117922]\n",
      "cash_bias                :   0.199635 [  0.045137,   0.608082]\n",
      "max_drawdown             :  -0.018434 [ -0.067911,   0.000000]\n",
      "mean_market_return       :   1.005858 [  0.854714,   1.097258]\n",
      "sharpe                   :  1362.376164 [ 348.819922,  3615.903764]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.175 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "Step 510000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.157 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "  510030/2000000.0: episode: 17001, duration: 1.027s, episode steps: 30, steps per second: 29, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.234 [0.011, 0.577], mean observation: 0.998 [0.892, 1.137], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000023\n",
      "accumulated_portfolio_val:   1.005500 [  0.920274,   1.176317]\n",
      "cash_bias                :   0.189740 [  0.042987,   0.694862]\n",
      "max_drawdown             :  -0.019086 [ -0.083069,   0.000000]\n",
      "mean_market_return       :   1.006907 [  0.931724,   1.124289]\n",
      "sharpe                   :  1336.535110 [ 341.512349,  3436.209098]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.165 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "Step 525000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  525030/2000000.0: episode: 17501, duration: 0.913s, episode steps: 30, steps per second: 33, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.102 [-0.131, 0.489], mean observation: 1.000 [0.915, 1.089], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000008\n",
      "accumulated_portfolio_val:   1.006874 [  0.889649,   1.196018]\n",
      "cash_bias                :   0.190207 [  0.041271,   0.521202]\n",
      "max_drawdown             :  -0.018870 [ -0.171266,   0.000000]\n",
      "mean_market_return       :   1.007213 [  0.920273,   1.175881]\n",
      "sharpe                   :  1374.989358 [ 252.811820,  3262.661176]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.158 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 54 (530000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 540000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.161 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "  540030/2000000.0: episode: 18001, duration: 1.021s, episode steps: 30, steps per second: 29, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.199 [0.035, 0.542], mean observation: 1.006 [0.885, 1.221], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000003\n",
      "accumulated_portfolio_val:   1.007506 [  0.909215,   1.126274]\n",
      "cash_bias                :   0.186955 [  0.039340,   0.691189]\n",
      "max_drawdown             :  -0.019185 [ -0.078987,   0.000000]\n",
      "mean_market_return       :   1.008276 [  0.923179,   1.120255]\n",
      "sharpe                   :  1317.906509 [ 388.660916,  4926.269394]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.156 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "Step 555000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  555030/2000000.0: episode: 18501, duration: 0.917s, episode steps: 30, steps per second: 33, episode reward: 0.001, mean reward: 0.000 [-0.001, 0.001], mean action: 0.153 [-0.156, 0.410], mean observation: 0.982 [0.643, 1.201], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000013\n",
      "accumulated_portfolio_val:   1.008488 [  0.889471,   1.144357]\n",
      "cash_bias                :   0.186835 [  0.041934,   0.510647]\n",
      "max_drawdown             :  -0.018669 [ -0.127905,   0.000000]\n",
      "mean_market_return       :   1.009370 [  0.937498,   1.157721]\n",
      "sharpe                   :  1352.191508 [ 282.881462,  3245.218919]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.162 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "Step 570000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.006 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.161 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "  570030/2000000.0: episode: 19001, duration: 1.006s, episode steps: 30, steps per second: 30, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.079 [-0.097, 0.287], mean observation: 0.998 [0.897, 1.061], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000019\n",
      "accumulated_portfolio_val:   1.009115 [  0.911634,   1.124912]\n",
      "cash_bias                :   0.186614 [  0.038146,   0.471439]\n",
      "max_drawdown             :  -0.020178 [ -0.101481,   0.000000]\n",
      "mean_market_return       :   1.009090 [  0.932928,   1.100187]\n",
      "sharpe                   :  1311.947365 [ 334.898845,  3541.443049]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.158 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "Step 585000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  585030/2000000.0: episode: 19501, duration: 0.925s, episode steps: 30, steps per second: 32, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.131 [-0.142, 0.506], mean observation: 0.998 [0.907, 1.056], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000020\n",
      "accumulated_portfolio_val:   1.009242 [  0.930065,   1.143217]\n",
      "cash_bias                :   0.189402 [  0.039221,   0.704096]\n",
      "max_drawdown             :  -0.019000 [ -0.123379,   0.000000]\n",
      "mean_market_return       :   1.009103 [  0.938155,   1.144028]\n",
      "sharpe                   :  1348.988781 [ 288.326431,  3975.405853]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "Step 600000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.005, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.168 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "  600030/2000000.0: episode: 20001, duration: 1.014s, episode steps: 30, steps per second: 30, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.191 [0.004, 0.551], mean observation: 1.000 [0.890, 1.115], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000005\n",
      "accumulated_portfolio_val:   1.009016 [  0.849778,   1.111880]\n",
      "cash_bias                :   0.192587 [  0.044217,   0.572557]\n",
      "max_drawdown             :  -0.018698 [ -0.212784,   0.000000]\n",
      "mean_market_return       :   1.008568 [  0.848208,   1.111982]\n",
      "sharpe                   :  1365.353304 [ 173.002873,  3230.082842]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.006, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.165 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "Step 615000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  615030/2000000.0: episode: 20501, duration: 0.915s, episode steps: 30, steps per second: 33, episode reward: 0.002, mean reward: 0.000 [-0.000, 0.000], mean action: 0.156 [-0.045, 0.328], mean observation: 0.998 [0.917, 1.079], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000002\n",
      "accumulated_portfolio_val:   1.006846 [  0.844408,   1.116449]\n",
      "cash_bias                :   0.191142 [  0.036015,   0.455727]\n",
      "max_drawdown             :  -0.019175 [ -0.189448,   0.000000]\n",
      "mean_market_return       :   1.006427 [  0.899156,   1.138641]\n",
      "sharpe                   :  1349.332492 [ 218.718130,  3439.047592]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.171 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "Step 630000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.005, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.174 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "  630030/2000000.0: episode: 21001, duration: 1.003s, episode steps: 30, steps per second: 30, episode reward: -0.000, mean reward: -0.000 [-0.001, 0.000], mean action: 0.191 [-0.158, 0.624], mean observation: 1.002 [0.806, 1.298], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000015\n",
      "accumulated_portfolio_val:   1.007291 [  0.863835,   1.177044]\n",
      "cash_bias                :   0.201941 [  0.047560,   0.481019]\n",
      "max_drawdown             :  -0.019637 [ -0.169230,   0.000000]\n",
      "mean_market_return       :   1.008612 [  0.925171,   1.093757]\n",
      "sharpe                   :  1295.736717 [ 219.648374,  3415.441047]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.165 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 65 (640000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 645000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  645030/2000000.0: episode: 21501, duration: 0.917s, episode steps: 30, steps per second: 33, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.152 [-0.074, 0.381], mean observation: 0.999 [0.915, 1.067], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000005\n",
      "accumulated_portfolio_val:   1.008039 [  0.936919,   1.104805]\n",
      "cash_bias                :   0.193676 [  0.037411,   0.514672]\n",
      "max_drawdown             :  -0.018117 [ -0.116132,   0.000000]\n",
      "mean_market_return       :   1.007928 [  0.908750,   1.090569]\n",
      "sharpe                   :  1390.545731 [ 359.563258,  3465.775387]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.167 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "Step 660000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.162 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "  660030/2000000.0: episode: 22001, duration: 1.016s, episode steps: 30, steps per second: 30, episode reward: -0.000, mean reward: -0.000 [-0.000, 0.000], mean action: 0.130 [-0.233, 0.299], mean observation: 1.002 [0.881, 1.201], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000008\n",
      "accumulated_portfolio_val:   1.006521 [  0.893552,   1.138527]\n",
      "cash_bias                :   0.190109 [  0.043419,   0.691972]\n",
      "max_drawdown             :  -0.019658 [ -0.185836,   0.000000]\n",
      "mean_market_return       :   1.007079 [  0.928141,   1.143923]\n",
      "sharpe                   :  1327.788689 [ 206.243908,  4581.408534]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.163 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "Step 675000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  675030/2000000.0: episode: 22501, duration: 0.925s, episode steps: 30, steps per second: 32, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.001], mean action: 0.191 [-0.006, 0.374], mean observation: 1.024 [0.872, 1.290], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000012\n",
      "accumulated_portfolio_val:   1.005906 [  0.858422,   1.170595]\n",
      "cash_bias                :   0.191862 [  0.041957,   0.469873]\n",
      "max_drawdown             :  -0.019448 [ -0.100090,   0.000000]\n",
      "mean_market_return       :   1.006384 [  0.886216,   1.118404]\n",
      "sharpe                   :  1327.024952 [ 302.051560,  3074.600160]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "Step 690000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.161 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "  690030/2000000.0: episode: 23001, duration: 1.089s, episode steps: 30, steps per second: 28, episode reward: -0.000, mean reward: -0.000 [-0.000, 0.000], mean action: 0.202 [-0.046, 0.387], mean observation: 1.016 [0.971, 1.067], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000003\n",
      "accumulated_portfolio_val:   1.008010 [  0.902025,   1.146242]\n",
      "cash_bias                :   0.188583 [  0.042441,   0.442045]\n",
      "max_drawdown             :  -0.018908 [ -0.123834,   0.000000]\n",
      "mean_market_return       :   1.008758 [  0.883571,   1.090595]\n",
      "sharpe                   :  1344.432949 [ 264.147445,  3660.160896]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.167 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "Step 705000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  705030/2000000.0: episode: 23501, duration: 0.915s, episode steps: 30, steps per second: 33, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.171 [-0.006, 0.442], mean observation: 0.989 [0.859, 1.062], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000003\n",
      "accumulated_portfolio_val:   1.009364 [  0.875592,   1.147106]\n",
      "cash_bias                :   0.193745 [  0.045809,   0.562891]\n",
      "max_drawdown             :  -0.018366 [ -0.070237,   0.000000]\n",
      "mean_market_return       :   1.009295 [  0.894388,   1.102002]\n",
      "sharpe                   :  1361.834701 [ 405.864971,  5829.733439]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.167 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "Step 720000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "  720030/2000000.0: episode: 24001, duration: 1.035s, episode steps: 30, steps per second: 29, episode reward: -0.001, mean reward: -0.000 [-0.000, 0.000], mean action: 0.236 [0.011, 0.529], mean observation: 1.008 [0.873, 1.208], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000001\n",
      "accumulated_portfolio_val:   1.007273 [  0.877605,   1.130511]\n",
      "cash_bias                :   0.193335 [  0.043515,   0.527217]\n",
      "max_drawdown             :  -0.018900 [ -0.088661,   0.000000]\n",
      "mean_market_return       :   1.008485 [  0.847991,   1.113339]\n",
      "sharpe                   :  1319.629855 [ 406.570217,  3563.398971]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.159 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "Step 735000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  735030/2000000.0: episode: 24501, duration: 0.911s, episode steps: 30, steps per second: 33, episode reward: -0.000, mean reward: -0.000 [-0.000, 0.000], mean action: 0.119 [-0.195, 0.343], mean observation: 0.995 [0.913, 1.061], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000006\n",
      "accumulated_portfolio_val:   1.007776 [  0.899608,   1.148906]\n",
      "cash_bias                :   0.186831 [  0.039276,   0.668688]\n",
      "max_drawdown             :  -0.019442 [ -0.257188,   0.000000]\n",
      "mean_market_return       :   1.008710 [  0.835000,   1.114854]\n",
      "sharpe                   :  1332.701265 [ 165.386418,  3882.113086]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.167 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "Step 750000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.168 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "  750030/2000000.0: episode: 25001, duration: 1.029s, episode steps: 30, steps per second: 29, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.177 [-0.111, 0.371], mean observation: 1.001 [0.936, 1.059], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000012\n",
      "accumulated_portfolio_val:   1.008105 [  0.899895,   1.129424]\n",
      "cash_bias                :   0.196373 [  0.045188,   0.582827]\n",
      "max_drawdown             :  -0.018993 [ -0.124283,   0.000000]\n",
      "mean_market_return       :   1.008604 [  0.896634,   1.110635]\n",
      "sharpe                   :  1331.409081 [ 153.182229,  3387.408001]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.169 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "Step 765000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  765030/2000000.0: episode: 25501, duration: 0.909s, episode steps: 30, steps per second: 33, episode reward: -0.000, mean reward: -0.000 [-0.000, 0.000], mean action: 0.182 [0.001, 0.353], mean observation: 0.996 [0.946, 1.064], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000003\n",
      "accumulated_portfolio_val:   1.010165 [  0.899967,   1.168059]\n",
      "cash_bias                :   0.193901 [  0.039099,   0.582473]\n",
      "max_drawdown             :  -0.020784 [ -0.352272,   0.000000]\n",
      "mean_market_return       :   1.009517 [  0.919204,   1.111982]\n",
      "sharpe                   :  1363.920368 [ 124.660617,  3520.583620]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.163 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "Step 780000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.169 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "  780030/2000000.0: episode: 26001, duration: 1.023s, episode steps: 30, steps per second: 29, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.176 [0.035, 0.333], mean observation: 0.997 [0.926, 1.060], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000021\n",
      "accumulated_portfolio_val:   1.007854 [  0.894160,   1.147773]\n",
      "cash_bias                :   0.193116 [  0.043890,   0.591516]\n",
      "max_drawdown             :  -0.019267 [ -0.187468,   0.000000]\n",
      "mean_market_return       :   1.007850 [  0.919557,   1.100102]\n",
      "sharpe                   :  1339.988678 [ 216.937008,  4523.412018]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.168 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "Step 795000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  795030/2000000.0: episode: 26501, duration: 0.913s, episode steps: 30, steps per second: 33, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.110 [-0.199, 0.340], mean observation: 1.007 [0.939, 1.079], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000008\n",
      "accumulated_portfolio_val:   1.007678 [  0.866184,   1.113411]\n",
      "cash_bias                :   0.194356 [  0.040247,   0.476312]\n",
      "max_drawdown             :  -0.019636 [ -0.114418,   0.000000]\n",
      "mean_market_return       :   1.007111 [  0.845515,   1.089739]\n",
      "sharpe                   :  1364.763005 [ 277.864086,  5704.802539]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.164 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "Step 810000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "  810030/2000000.0: episode: 27001, duration: 1.015s, episode steps: 30, steps per second: 30, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.116 [-0.143, 0.304], mean observation: 0.993 [0.862, 1.120], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000003\n",
      "accumulated_portfolio_val:   1.007388 [  0.899800,   1.168367]\n",
      "cash_bias                :   0.191602 [  0.040524,   0.477169]\n",
      "max_drawdown             :  -0.018875 [ -0.098703,   0.000000]\n",
      "mean_market_return       :   1.007178 [  0.848208,   1.102705]\n",
      "sharpe                   :  1364.748806 [ 287.529692,  4813.551346]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.174 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "Step 825000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  825030/2000000.0: episode: 27501, duration: 0.942s, episode steps: 30, steps per second: 32, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.001], mean action: 0.130 [-0.047, 0.321], mean observation: 0.987 [0.857, 1.093], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000012\n",
      "accumulated_portfolio_val:   1.008374 [  0.884440,   1.161666]\n",
      "cash_bias                :   0.199017 [  0.056874,   0.529023]\n",
      "max_drawdown             :  -0.018271 [ -0.085183,   0.000000]\n",
      "mean_market_return       :   1.008734 [  0.933178,   1.134824]\n",
      "sharpe                   :  1391.583029 [ 259.186685,  3540.040345]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "Step 840000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.170 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "  840030/2000000.0: episode: 28001, duration: 1.006s, episode steps: 30, steps per second: 30, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.208 [-0.020, 0.496], mean observation: 1.004 [0.949, 1.096], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000003\n",
      "accumulated_portfolio_val:   1.004975 [  0.886132,   1.135873]\n",
      "cash_bias                :   0.194932 [  0.040838,   0.452358]\n",
      "max_drawdown             :  -0.019301 [ -0.107955,   0.000000]\n",
      "mean_market_return       :   1.006735 [  0.912642,   1.088189]\n",
      "sharpe                   :  1326.130469 [ 306.526418,  3458.752798]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.165 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 86 (850000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 855000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  855030/2000000.0: episode: 28501, duration: 0.914s, episode steps: 30, steps per second: 33, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.132 [-0.095, 0.374], mean observation: 0.988 [0.874, 1.078], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000004\n",
      "accumulated_portfolio_val:   1.008358 [  0.891976,   1.111605]\n",
      "cash_bias                :   0.193507 [  0.041040,   0.526186]\n",
      "max_drawdown             :  -0.019878 [ -0.150882,   0.000000]\n",
      "mean_market_return       :   1.009528 [  0.897183,   1.157721]\n",
      "sharpe                   :  1315.658711 [ 217.215553,  7279.484873]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.172 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "Step 870000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.164 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "  870030/2000000.0: episode: 29001, duration: 1.012s, episode steps: 30, steps per second: 30, episode reward: -0.000, mean reward: -0.000 [-0.000, 0.000], mean action: 0.190 [0.002, 0.451], mean observation: 1.005 [0.926, 1.070], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000019\n",
      "accumulated_portfolio_val:   1.007623 [  0.884343,   1.151406]\n",
      "cash_bias                :   0.193640 [  0.044517,   0.520106]\n",
      "max_drawdown             :  -0.019150 [ -0.188947,   0.000000]\n",
      "mean_market_return       :   1.007882 [  0.897378,   1.101431]\n",
      "sharpe                   :  1336.497016 [ 222.862101,  3870.335854]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.008, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.169 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "Step 885000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  885030/2000000.0: episode: 29501, duration: 0.925s, episode steps: 30, steps per second: 32, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.001], mean action: 0.143 [-0.137, 0.473], mean observation: 1.002 [0.897, 1.139], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000012\n",
      "accumulated_portfolio_val:   1.007935 [  0.795670,   1.130705]\n",
      "cash_bias                :   0.194550 [  0.048241,   0.510785]\n",
      "max_drawdown             :  -0.018508 [ -0.070482,   0.000000]\n",
      "mean_market_return       :   1.008901 [  0.854680,   1.101496]\n",
      "sharpe                   :  1340.120824 [ 318.355525,  3256.732525]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.170 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "Step 900000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.006, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.175 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "  900030/2000000.0: episode: 30001, duration: 1.012s, episode steps: 30, steps per second: 30, episode reward: -0.001, mean reward: -0.000 [-0.000, 0.000], mean action: 0.111 [-0.115, 0.306], mean observation: 1.017 [0.943, 1.117], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000002\n",
      "accumulated_portfolio_val:   1.005818 [  0.843583,   1.120632]\n",
      "cash_bias                :   0.201643 [  0.042177,   0.489046]\n",
      "max_drawdown             :  -0.019545 [ -0.170295,   0.000000]\n",
      "mean_market_return       :   1.006799 [  0.847685,   1.089240]\n",
      "sharpe                   :  1332.354134 [ 218.802645,  4029.182468]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "Step 915000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  915030/2000000.0: episode: 30501, duration: 0.905s, episode steps: 30, steps per second: 33, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.140 [-0.031, 0.419], mean observation: 1.000 [0.929, 1.068], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000019\n",
      "accumulated_portfolio_val:   1.007226 [  0.895360,   1.153235]\n",
      "cash_bias                :   0.196961 [  0.037388,   0.546664]\n",
      "max_drawdown             :  -0.019896 [ -0.214687,   0.000000]\n",
      "mean_market_return       :   1.006355 [  0.883825,   1.182211]\n",
      "sharpe                   :  1339.010141 [ 195.886849,  3612.690478]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.173 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "Step 930000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.174 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "  930030/2000000.0: episode: 31001, duration: 1.024s, episode steps: 30, steps per second: 29, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.144 [-0.264, 0.453], mean observation: 0.998 [0.803, 1.107], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000003\n",
      "accumulated_portfolio_val:   1.005752 [  0.866669,   1.158041]\n",
      "cash_bias                :   0.198772 [  0.048158,   0.449198]\n",
      "max_drawdown             :  -0.019469 [ -0.174580,   0.000000]\n",
      "mean_market_return       :   1.006332 [  0.849419,   1.167212]\n",
      "sharpe                   :  1313.173266 [ 220.920044,  3009.132665]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "Step 945000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  945030/2000000.0: episode: 31501, duration: 0.926s, episode steps: 30, steps per second: 32, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.151 [-0.152, 0.372], mean observation: 0.997 [0.943, 1.060], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000011\n",
      "accumulated_portfolio_val:   1.008907 [  0.868898,   1.215589]\n",
      "cash_bias                :   0.190768 [  0.036553,   0.431203]\n",
      "max_drawdown             :  -0.018526 [ -0.103104,   0.000000]\n",
      "mean_market_return       :   1.009816 [  0.924263,   1.167212]\n",
      "sharpe                   :  1351.086731 [ 204.047471,  3657.060702]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.161 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "Step 960000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 episodes - episode_reward: 0.000 [-0.005, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.167 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "  960030/2000000.0: episode: 32001, duration: 1.037s, episode steps: 30, steps per second: 29, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.192 [-0.106, 0.476], mean observation: 1.021 [0.798, 1.181], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000010\n",
      "accumulated_portfolio_val:   1.006898 [  0.866798,   1.192563]\n",
      "cash_bias                :   0.191934 [  0.038739,   0.466768]\n",
      "max_drawdown             :  -0.018200 [ -0.114583,   0.000000]\n",
      "mean_market_return       :   1.006641 [  0.850965,   1.097022]\n",
      "sharpe                   :  1389.483499 [ 203.524275,  3552.470681]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.162 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "Step 975000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "  975030/2000000.0: episode: 32501, duration: 0.904s, episode steps: 30, steps per second: 33, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.132 [-0.077, 0.320], mean observation: 0.993 [0.858, 1.089], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000012\n",
      "accumulated_portfolio_val:   1.005827 [  0.897264,   1.126247]\n",
      "cash_bias                :   0.190021 [  0.037095,   0.558716]\n",
      "max_drawdown             :  -0.019348 [ -0.123345,   0.000000]\n",
      "mean_market_return       :   1.006638 [  0.921044,   1.134339]\n",
      "sharpe                   :  1325.481899 [ 292.427970,  4347.619909]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.162 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "Step 990000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.164 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "  990030/2000000.0: episode: 33001, duration: 0.995s, episode steps: 30, steps per second: 30, episode reward: -0.001, mean reward: -0.000 [-0.000, 0.000], mean action: 0.186 [-0.164, 0.535], mean observation: 0.996 [0.842, 1.090], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000003\n",
      "accumulated_portfolio_val:   1.008971 [  0.893347,   1.128688]\n",
      "cash_bias                :   0.189002 [  0.040795,   0.576242]\n",
      "max_drawdown             :  -0.019005 [ -0.105610,   0.000000]\n",
      "mean_market_return       :   1.008538 [  0.907717,   1.098241]\n",
      "sharpe                   :  1337.395033 [ 309.294125,  3947.658795]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.169 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "Step 1005000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      " 1005030/2000000.0: episode: 33501, duration: 0.927s, episode steps: 30, steps per second: 32, episode reward: 0.002, mean reward: 0.000 [-0.000, 0.000], mean action: 0.117 [-0.121, 0.367], mean observation: 0.986 [0.823, 1.067], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000018\n",
      "accumulated_portfolio_val:   1.010218 [  0.895713,   1.131387]\n",
      "cash_bias                :   0.193023 [  0.039961,   0.626816]\n",
      "max_drawdown             :  -0.018869 [ -0.080362,   0.000000]\n",
      "mean_market_return       :   1.009674 [  0.901621,   1.180678]\n",
      "sharpe                   :  1336.061789 [ 292.667375,  4028.157945]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.158 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "Step 1020000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.007, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.167 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      " 1020030/2000000.0: episode: 34001, duration: 1.152s, episode steps: 30, steps per second: 26, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.232 [0.065, 0.498], mean observation: 1.002 [0.951, 1.051], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000002\n",
      "accumulated_portfolio_val:   1.005941 [  0.805131,   1.116902]\n",
      "cash_bias                :   0.189947 [  0.041122,   0.504808]\n",
      "max_drawdown             :  -0.019335 [ -0.088943,   0.000000]\n",
      "mean_market_return       :   1.005409 [  0.827522,   1.100794]\n",
      "sharpe                   :  1334.835818 [ 134.519277,  3344.687228]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.170 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "Step 1035000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      " 1035030/2000000.0: episode: 34501, duration: 0.985s, episode steps: 30, steps per second: 30, episode reward: -0.001, mean reward: -0.000 [-0.000, 0.000], mean action: 0.238 [-0.015, 0.476], mean observation: 0.995 [0.929, 1.049], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000003\n",
      "accumulated_portfolio_val:   1.007447 [  0.868957,   1.165668]\n",
      "cash_bias                :   0.191129 [  0.045676,   0.563251]\n",
      "max_drawdown             :  -0.019560 [ -0.124103,   0.000000]\n",
      "mean_market_return       :   1.007695 [  0.839843,   1.155004]\n",
      "sharpe                   :  1332.841353 [ 200.209526,  3435.056535]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.006, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.159 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "Step 1050000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.160 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      " 1050030/2000000.0: episode: 35001, duration: 1.041s, episode steps: 30, steps per second: 29, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.001], mean action: 0.179 [-0.135, 0.643], mean observation: 1.004 [0.888, 1.180], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000010\n",
      "accumulated_portfolio_val:   1.006371 [  0.840921,   1.137449]\n",
      "cash_bias                :   0.188556 [  0.038986,   0.582319]\n",
      "max_drawdown             :  -0.020079 [ -0.192424,   0.000000]\n",
      "mean_market_return       :   1.007029 [  0.835000,   1.090224]\n",
      "sharpe                   :  1325.562492 [ 200.744853,  3196.294863]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.171 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1065000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      " 1065030/2000000.0: episode: 35501, duration: 0.955s, episode steps: 30, steps per second: 31, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.181 [-0.036, 0.414], mean observation: 0.995 [0.899, 1.067], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000009\n",
      "accumulated_portfolio_val:   1.007051 [  0.901042,   1.200060]\n",
      "cash_bias                :   0.197430 [  0.039150,   0.580278]\n",
      "max_drawdown             :  -0.018875 [ -0.172364,   0.000000]\n",
      "mean_market_return       :   1.007665 [  0.835000,   1.170414]\n",
      "sharpe                   :  1394.855418 [ 231.936978,  3841.618544]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "Step 1080000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.164 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      " 1080030/2000000.0: episode: 36001, duration: 1.032s, episode steps: 30, steps per second: 29, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.176 [-0.091, 0.641], mean observation: 1.010 [0.947, 1.145], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000006\n",
      "accumulated_portfolio_val:   1.005603 [  0.896214,   1.138640]\n",
      "cash_bias                :   0.190252 [  0.038969,   0.728403]\n",
      "max_drawdown             :  -0.018985 [ -0.076120,   0.000000]\n",
      "mean_market_return       :   1.005719 [  0.902395,   1.101580]\n",
      "sharpe                   :  1357.000642 [ 427.520394,  4250.008676]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.169 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "Step 1095000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      " 1095030/2000000.0: episode: 36501, duration: 0.938s, episode steps: 30, steps per second: 32, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.255 [0.054, 0.501], mean observation: 1.014 [0.892, 1.180], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000002\n",
      "accumulated_portfolio_val:   1.007238 [  0.885115,   1.147073]\n",
      "cash_bias                :   0.192732 [  0.039930,   0.687350]\n",
      "max_drawdown             :  -0.018905 [ -0.176646,   0.000000]\n",
      "mean_market_return       :   1.007205 [  0.893992,   1.138641]\n",
      "sharpe                   :  1334.055967 [ 240.337589,  3944.744841]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.163 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "Step 1110000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.171 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n",
      " 1110030/2000000.0: episode: 37001, duration: 0.982s, episode steps: 30, steps per second: 31, episode reward: -0.001, mean reward: -0.000 [-0.000, 0.000], mean action: 0.177 [-0.008, 0.610], mean observation: 1.022 [0.898, 1.218], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000005\n",
      "accumulated_portfolio_val:   1.006979 [  0.889664,   1.124513]\n",
      "cash_bias                :   0.195823 [  0.043447,   0.534658]\n",
      "max_drawdown             :  -0.018480 [ -0.097389,   0.000000]\n",
      "mean_market_return       :   1.007274 [  0.918920,   1.104224]\n",
      "sharpe                   :  1363.794117 [ 335.749638,  4200.316304]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.003 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.158 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "Step 1125000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      " 1125030/2000000.0: episode: 37501, duration: 0.976s, episode steps: 30, steps per second: 31, episode reward: 0.001, mean reward: 0.000 [-0.001, 0.001], mean action: 0.129 [-0.171, 0.499], mean observation: 0.951 [0.722, 1.121], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000001\n",
      "accumulated_portfolio_val:   1.005708 [  0.864756,   1.142419]\n",
      "cash_bias                :   0.184716 [  0.044677,   0.597516]\n",
      "max_drawdown             :  -0.020182 [ -0.251695,   0.000000]\n",
      "mean_market_return       :   1.006148 [  0.918287,   1.141544]\n",
      "sharpe                   :  1327.513936 [ 154.870464,  3433.484320]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.160 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "1130763/|/reward=-0.0001 info=(reward: -0.0001, log_return: -0.0043, portfolio_value: 0.9943, rate_of_return: -0.0043, cost: 0.0000, steps: 4.0000, cash_bias: 0.0922, mean_market_returns: 0.9944, ) 57%|| 1130763/2000000.0 [9:42:09<10:50:37, 22.27it/s]Step 1140000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.167 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      " 1140030/2000000.0: episode: 38001, duration: 1.055s, episode steps: 30, steps per second: 28, episode reward: -0.000, mean reward: -0.000 [-0.000, 0.000], mean action: 0.158 [-0.189, 0.353], mean observation: 0.999 [0.902, 1.095], loss: 0.000000, mean_squared_error: 0.000000, mean_q: 0.000017\n",
      "accumulated_portfolio_val:   1.006879 [  0.891763,   1.193138]\n",
      "cash_bias                :   0.192871 [  0.036929,   0.535157]\n",
      "max_drawdown             :  -0.018819 [ -0.120954,   0.000000]\n",
      "mean_market_return       :   1.007575 [  0.908750,   1.108754]\n",
      "sharpe                   :  1343.381971 [ 313.458146,  4068.472469]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.005 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490 - cash_bias: 0.160 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "Step 1155000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      " 1155030/2000000.0: episode: 38501, duration: 0.975s, episode steps: 30, steps per second: 31, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.203 [-0.022, 0.531], mean observation: 0.997 [0.918, 1.071], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000007\n",
      "accumulated_portfolio_val:   1.006493 [  0.807524,   1.193326]\n",
      "cash_bias                :   0.190825 [  0.041342,   0.501213]\n",
      "max_drawdown             :  -0.020249 [ -0.129263,   0.000000]\n",
      "mean_market_return       :   1.008081 [  0.875036,   1.143923]\n",
      "sharpe                   :  1317.407572 [ 270.022090,  4209.255653]\n",
      "\n",
      "333 episodes - episode_reward: 0.000 [-0.007, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500 - cash_bias: 0.171 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1170000: saving model to outputs/agent_portfolio-ddpg-keras/agent_portfolio-ddpg-keras-rl_weights.h5f\n",
      "334 episodes - episode_reward: 0.000 [-0.008, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.004 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510 - cash_bias: 0.166 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      " 1170030/2000000.0: episode: 39001, duration: 1.027s, episode steps: 30, steps per second: 29, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.155 [-0.105, 0.424], mean observation: 0.990 [0.896, 1.043], loss: 0.000000, mean_squared_error: 0.000000, mean_q: -0.000001\n",
      "accumulated_portfolio_val:   1.005816 [  0.781733,   1.184502]\n",
      "cash_bias                :   0.194062 [  0.042417,   0.438133]\n",
      "max_drawdown             :  -0.018857 [ -0.111280,   0.000000]\n",
      "mean_market_return       :   1.007057 [  0.847730,   1.117254]\n",
      "sharpe                   :  1345.538799 [ 127.585555,  3588.499975]\n",
      "\n",
      "1174114/|/reward= 0.0003 info=(reward: 0.0003, log_return: 0.0091, portfolio_value: 1.0116, rate_of_return: 0.0091, cost: 0.0000, steps: 5.0000, cash_bias: 0.1415, mean_market_returns: 1.0075, ) 59%|| 1174114/2000000.0 [10:06:51<13:22:07, 17.16it/s]"
     ]
    }
   ],
   "source": [
    "history = agent.fit(env, \n",
    "                  nb_steps=2e6, \n",
    "                  visualize=False, \n",
    "                  verbose=0,\n",
    "                  callbacks=[\n",
    "                      TrainIntervalLoggerTQDMNotebook(),\n",
    "                      TrainEpisodeLoggerPortfolio(500),\n",
    "                      ModelIntervalCheckpoint(save_path, 500*30, 1)\n",
    "                    ]\n",
    "                 )\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights(save_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T00:08:54.105Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:20:31.754002Z",
     "start_time": "2017-07-20T07:20:31.697525+08:00"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5577f462e098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs/agent_portfolio-ddpg-keras/agent_{}_weights.h5f'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'portfolio-ddpg-keras-rl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "agent.save_weights('outputs/agent_portfolio-ddpg-keras/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl'), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:21:52.485412Z",
     "start_time": "2017-07-21T14:20:55.951544+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 0.000, steps: 7613\n",
      "APV (Accumulated portfolio value): \t 28.713756\n",
      "SR (Sharpe ratio):                 \t 676.338784\n",
      "MDD (max drawdown):                \t-17.556893%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f51c9f95d68>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAFhCAYAAADeAstjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xd4FNX6wPHvbEnvvZAQQqihIx0BEb1WQIqCKIpeUbmi\nXhW7IiKCwLVgAcGCBSwIgt6rKD8BRVEEqaL0ToCQQBKSbNrO/P7YsMmyJZuQTX0/z+PjzJkzM2eH\nZN+cM6comqZpCCGEEKJG6Gq7AEIIIURjIoFXCCGEqEESeIUQQogaJIFXCCGEqEESeIUQQogaJIFX\nCCGEqEGGmrpRWlpaTd2q3oiLi5Pn4iHybKuXPE/PkudbverK84yLi3OYLjVeIYQQogZJ4BVCCCFq\nkAReIYQQogZJ4BVCCCFqkAReIYQQogZJ4BVCCCFqkAReIYQQogZJ4BVCCCFqkATeKti2bRv79+8H\n4Msvv+S2225j9erVDvMuXLiQr776in379vHBBx94rEwnT55kwoQJHru+EEKI6lFjM1c1JN9++y0D\nBw6kefPmrFu3jsmTJ5OcnOzynJSUFFJSUmqohEIIIeqqCgOvqqrMmzePEydOAHDXXXfh5eXFm2++\niaIoJCQkcOedd6LTVb3yrC55H+2PX6p8viNK1z7oRo5zmWflypX8/PPP5Ofnk52dzdixY/H39+fd\nd9/Fy8uL4OBgHn30Ufbt28fbb7+N0Wika9eu/P777+zdu5e///6bPXv2MGvWLJ599lnWrVvH6tWr\n0ev1dOjQgbvvvtt6r61bt/LVV1/x7LPPsmrVKpYuXYq/vz9RUVE8/PDDGAz2/xTZ2dncf//9LFy4\nEEVReO211+jSpQuBgYF8+OGHqKqKyWTi6aefxmg0Ws8bNWoUH374IV5eXsyfP5/ExESuuuoqFixY\nwPbt21FVlZEjRzJgwIBqe95CCCHcU2Hg3bRpEwBTp05l586dfPrpp2iaxqhRo0hNTWX+/Pls2rSJ\n7t27e7ywnlBQUMDs2bPJyspiwoQJKIrCnDlziIyM5IsvvuCjjz6iV69eFBUVMXfuXABOnDjBwIED\n6d69O1u2bOGhhx7CZDKxZs0a3njjDfR6PZMnT+bXX3+1u192djYLFy5kwYIFpKSk8NRTT/H1119z\nww032OUNDg4mOTmZ7du306ZNG7Zs2cJ9993H119/zZNPPklERAQff/wxP/74I4MGDXL5OTds2MCJ\nEyd4/fXXKSoqYsKECVxyySUEBARUz4MUQoh6TF36Afj4orv2Rqd5tKJCtNX/RRlwDYqPb5XvVWHg\n7d69O127dgXg9OnT+Pn5sWPHDtq2bQtA586d2bZt20UFXt3IcVBB7dRTOnbsiE6nIywsDB8fH1RV\nJTIy0nrsnXfeoVevXiQkJLi8zpEjR2jbtq215tq+fXsOHTpkl+/EiRMkJSXh5+cHQIcOHax/3Dhy\n3XXX8d1333HmzBn69OmDXq8nIiKC119/HV9fXzIyMmjXrp3T8zVNA+DAgQPs2bOHBx98EICSkhJO\nnjwpzd9CiEZNO7QXwiLQVi61JFwQeNWP3kQ7sAfds6+izp8F235H2/AT+smvVfmebr3j1ev1vPHG\nG2zcuJGHHnqIHTt2oCgKAL6+vuTn51d4DWerNNSmkJAQ/vjjD+Li4sjIyKCkpAQAg8FAVFQU33//\nPa1atSI8PBx/f3/rZ/D39yc0NJS4uDi8vb2JjIwkNDSU5cuXExUVhV6vZ8+ePQwdOpRdu3YRHBxM\neHg4vr6+dOrUiRkzZhASEgLAvn37SE1Ndfp8YmNjee+99zh37hyTJ08mLi6OV155hVWrVhEQEMBj\njz1GQEAAUVFReHl5ERcXh5+fH3q9ntjYWI4dO0bHjh1p2rQpWVlZTJ06FVVVeeutt+jSpUuDrvHW\nxZ+5+kyep2fJ861e7jxP85kM0qY9bJMWGxmJUvrqTlNVjv30HQB+Xy8md9vvlkzHDl7Uv5fbnavu\nu+8+srKyePLJJykqKrKmm0wm/P39Kzy/LizRdKGsrCzS0tIYNWoUeXl53H///ej1eu6++24URSEw\nMJDHH3+cgwcPYjKZrJ+hadOmzJgxAy8vLwoLCzl9+jSJiYn07t2b4cOHo2ka7du3p23btmzcuBGj\n0UhmZiYmk4mCggJuueUWRo8ebQ3aY8aMcfl8evXqxebNmzEYDKSlpTFw4EBuvPFGfHx8CAsLo7i4\nmPT0dIqKikhLS2PEiBHccccdxMTE4O/vT1ZWFj169GD16tWMGDECk8lE3759ycnJIScnp6Yed42q\nK8uCNRTyPD1Lnm/1cvd5akf226Wl7d2FEhJuOX6u7Psx9+vPbPO5cX1nwVnRzrdFOvHTTz+RmZnJ\nDTfcQH5+PpMmTSImJoZhw4ZZ3/G2a9eO3r17uyxAXfyhWrlyJUeOHGH8+PG1cn/5ZfMcebbVS56n\nZ8nzrV5uB949O1FnPWGTprvvGZSO3QBQf/oO7aM3HZ6rX/CV/fU0DY4egIRkFEVxGnjdesf71ltv\nMXnyZEpKSrj99tuJj4/n7bffpqSkhPj4eHr27FnhBxTOnTp1iunTp9uld+zYkXHjaufdtxBCNHh5\n5+yS1OUfoy8NvNqq5Y7PC41wmKy+9Bjs34Vy6wSUflc5vW2FgdfHx4eHHnrILn3KlCkVnVrnXXWV\n8wdTk6Kjo3n11VdruxhCCNGoaDlZ9onHDlqOlZTAyeOOTywpdpy+f5fl3I0/g4vAKzNXCSGEaJS0\npWWzCdq9cz1Vrqm6g6UGrIx7gILwOCguujC3LXOJy8MSeIUQQjROStnmyP7TuaP3MxBmGU6KZgYg\nz+DD2T7Xol/wFX8k9uDm9g/yQ0gqWrlar5aXi7bjj7LLJjZ3eVsJvEIIIRqn/Dy+TOjP38FJqIqe\nLK9AKCq0HCu0/P/Wvs9z599+7M4w8cKPxwB4s/WNqPcOt15Gff151DnlXr9WUOOVuZqFEEI0OtrO\nLaT5RvBR82tt0gvyTfgd2gsFJop0ZSHy0e8OO79Y6btd67XXfovW/ypw0qtZarz1SGVWIDp16hTr\n16+vtnunpaUxduxYh72vhRCiPtE0DfXNaeQbfOyOZXsFoK1bhZabwwnfcNfXOZvp9Jg6b6bTYxJ4\nG6gtW7bw559/Vtv1duzYQc+ePXniiScqziyEaLS0vHNoZnNtF8MlbfnHUFyESe9td2xHSAraTyvR\n1v/A/BbDHJ4fXFQ6DEm1/ZxfJF7GsAEz+TuoKZxy0iOaOtLU/P7mdNYfqd4ZlHonBjGuS5TT4ytX\nrmT9+vUUFRWRmZnJ8OHD+eWXXzh48CD33HMPffv2Ze3atSxZsgSdTkf79u0ZP348p0+f5pVXXrGe\nd+edd9K3b1/uvPNOOnbsyP79+1EUhRdeeMFmOsaFCxdy/PhxsrOzycnJYejQofz+++/s27ePxx9/\nnLZt27JgwQJ2795NTk4OzZs357HHHmPhwoXs3LkTk8nEpEmTADCbzbz00kskJSVx8803s2zZMn74\n4QcUReGyyy5j6NChLF68mMLCQlJTU+nTp4+1HDfffDNt2rQhLS2NpKQkJk2aRH5+PrNmzbLOYjVx\n4kSSk5MZNWoUCQkJREREsHPnTgoLC4mPj6dt27bMmTMHnU6Hl5cXjzzyCJqm8eSTTxIUFESPHj3Y\nsGEDzZs35+DBg/j6+tKhQwc2btxIbm4us2bNQqfTMWvWLPLy8sjIyGDo0KEMGTKEBx98kJSUFA4e\nPEh+fj6TJ08mJiaGjz76iJ9//hmz2czgwYMZPHiw3ecePnw4Qojao50+ifrkeJS+V6DcNrG2i+OU\n9s0SAM4Z/eyOvdV6JINOboS/tvL3gJtdX2fr79Cmg3V/cfLVAHzW7Eqe27bA6Xl1IvDWFpPJxKxZ\ns1i9ejVLlizhrbfeYuvWrSxdupQOHTqwcOFC5s2bh4+PDy+++CKbNm1CURRuvPFGOnXqxJ9//snC\nhQvp27cveXl5DBw4kPvvv58XXniB33//nYEDB9rcz9vbm5kzZ7J48WJ+++035s+fz7vvvsvq1atp\n2rQpAQEBzJ49G1VVGTduHKdPnwYgMTGRiRMncvLkScxmM9OmTaNDhw4MHTqUQ4cOsWbNGubMmQPA\npEmT6NatGzfffDNHjhyxCbpgWehi1qxZxMfH89xzz/Hzzz/z999/06VLF4YMGcKxY8d46aWXeP31\n10lPT+ftt98mODjYOsvXkCFDuPvuu5k0aRIpKSn8/PPPvPXWW9x7772cOXPGunzihg0baN26NRMn\nTuTRRx/F29ub2bNnM336dLZt20ZUVBQDBw6kX79+ZGRk8OCDDzJkyBAAWrduzX333cc777zD6tWr\n6datGxs2bOCtt95CVVUWLFjAwYMHHX7uxMRET//YCCGcOXIAAO3nVVCHA+95x/ycV85O+oTZpQV4\n6cgtUik2WlYm0j6dbxmGFB4FmenWfMWK69BaJwLvuC5RLmunnnJ+ZZ6AgACaNm1qnZ+5qKiI48eP\nk5WVxeOPPw5Afn4+aWlptG/fno8//phvvvkGwLqwAkCLFi0AiIqKspnP+sLjAQEBJCUlAVjv5+3t\nbV3EwNfXF5PJhLm0uab8ykj79+/H398fk8kEwMGDBzl16hQPP2yZ6PvcuXMcP+68iSMqKor4+HgA\n2rVrx9GjRzl48CBbtmxhzZo11muAZVnC4OBgu2tkZmZan13Hjh1ZsMDyl11sbKzNusAtW7Z0+nlD\nQ0P54osvWLduHX5+fk6f45kzZzh69Cht2rRBr9ej1+uZMGECa9ascfi5JfAKUXvU39bWdhEqpJ04\nat1eGe98quNCfdl32ZP94vHz0tEuyo9HvzvMnswCzCjoS0f/HjNprG1WNmFGQv4pl2WoE4G3tpxf\nYcmR2NhYoqKimD17NgaDgZUrV5KSksL777/PtddeS48ePfj2229ZuXKlW9er6PiGDRtIT09n8uTJ\nZGVlsW7dOuuSfjpd2av4li1bMn36dO699166d+9OQkICSUlJvPTSSyiKwpIlS0hOTmbr1q2oqmp3\nn4yMDM6cOUNYWBh//vknV1xxBVlZWQwaNIhBgwZx9uxZ/ve//7ksb3h4OPv376d58+Zs27aNJk2a\nOMzv6vN+/vnnpKamMmTIELZs2cKGDRucnpeYmMiKFStQVRVVVXn88ce55557HH5uIUQt2vpbbZeg\nQtqBPdbtXJ9AKP2aHNAsiN0nz1GQlQ1Akc7Lms/boKN9tGUxoD2ZBQCMHPASy9Y+CsCkrg9QqC/L\nr7heAqFxB15XQkJCGDlyJA8++CBms5mYmBgGDBhA//79mTt3LosXLyYiIoLs7OxquV+bNm346KOP\neOCBBwCsSxU64u3tzb///W+mT59uXd5v4sSJFBcX07p1ayIiIkhOTmbRokW0bNnSpsnbaDTy2muv\nkZ6eTtu2benduzft2rVj1qxZ/Pe//yU/P5/bbrvNZVkfeeQR5syZg6Zp6PV667vnyujduzdz5sxh\n9erVBAQEoNfrHbYSgKVlonv37kycOBFVVRkyZAgpKSkOP7cQom7QiotQjF4VZ6xBWmEhHN5n2b58\nCCWlfaOe6BdPz4RAbvtiD1neQRTqDBSUC6SFJfaVmPLKB12A7+J70Td9G85Wca9wdaLqIitv2KuN\nFUmGDRvGsmXLavSetUFWe6le8jw9q74/X3XN/8BgRPvwDWua7umXUZqm1Ep5nD1P85znYccmAL4d\n+hgLsizDhVaMaQ3AkEWW8bijDn5HUu4JZrS/HYAvRrXEqNfZ5AF4Z/1UworOMWyA/dCh2PwMvpp8\no8PySY1XCCFElWnFxWiL37Y/oNPXfGEqUhp0SxSdNeg68mmzf3DzgW8BGNY2zBp0L1SiOP+MJ/yc\nt8DJON5GpjHUdoUQNejQXsfp56derCPM82ZYt+e2GmHdHpgcZN0e1yXSun1+aFBsoG0z8ktXNrVu\n/xjThX2BTSpdFgm8Qgghqu6CSSS2h6SQbfSHmnmL6RatpBj+KJvJb03MJdbtWzqWBdsQH/tG4M1p\nuTb7rSN9uTLN0onsk2ZX8WjX+4nNd9wfxxlpahZCCFFl2oYfrdu7gpryXKfxACzXXHdIqknqf56x\nbl/454C3oaz+WWS2/2Phnu4xdmnrI9vb7LtqVnZEarxCCCGqTFv3vXU7yyuw3IG6EXjVX36AfX9Z\n9/cOvse6fW2rUAK8yt7Tto+2nckqwavEYS041+jv8F6+Bh2f39SywjJJ4BVCCFEtZrYba93Wdm6p\nxZKU0Ra+VrbTJIkTqWWz+Y2/JNomr6/RNiSaKtko/OlNLW1q0M5I4BVCCFFpmqahlh8+9Opi2+Pf\nLEFzMIlPbcp8YDqfbLe8j72nW7Td8QtrtxmOpxao0ITuMYxuL72ahRBCVCPth6+tzczbQlPYm28b\nTjK9giDnbG0Uzap84FdG3sHkn06SnlcMQKHZ8R8FC4Y0t25fmWI/Za4zNxhOWLf/0SKEUR0k8Aoh\nhKgGmqahmfLRPnsHABWFKR3HM+n7Izb5coz+aKu+qo0iljl6wLqZ2+860s4VW/e7xAU4OoOogLI5\nmnslBDrM44h3QtOKM5WSXs1CCCHcpr46Gf7aCooCmsb+wHiH+Up0erTvv0QtyEd3679quJQW2sqy\neQtuXbrP5lhCUMXTWTqpFDvkHeJ+7VhqvEIIIdz311bL/0vH6ZZfTKA8VbGEF+2n79CKy16Wannn\n0PbvQl25FG37RrTD+1A/mY9mNju8zkVJaQPA5rBWNsn/SAmpcFEbgBLV/bHIBl3F17PmdTunEEII\ncYFfOg92mK5SLhAdPwxJluU+1QfHWJPLhzWldQfo3LN6C3cum3y9Ny90uNMmeUIP+7G5jhQ7CbwP\n9orl1V9P2KRlF7j/h4PUeIUQQlRNbAIribNJ+sfxX4GyGi+AOu1h1C8/Rv3l/5xeSn3/NafHqkI7\nm4n2v8+5r7vt6mnvD6t44YbznapahPs4PD6gWZBdmpPpnB2SwCuEEKJKzCeP2+zPG5xMcGnfpHNG\n28kotG8+R1s4x/nFTHmoP33n8n7a2UzMdw1G/e7LCsumls7NnOVdFiTv6xFDmG/FDb0TusfwyY0t\n7OZpPs9RM3VlRk5J4BVCCOGW8sNzNGBO+1us+4tHWgKVTmcJKzPbuV7X2+H1P3rT9fEl71n+/8X7\nVLSirdKync2+UadwRUqIW+VQFAU/Y+VWV2p3waxXrkjgFUIIJ7Scs6jzZ6Gdqdwk+A1WftmCAd/2\nvo11YWXBza901iel3FSRuQZfp5dS+l+F7uEX3L61uvFntI3rrPvaN0tcn6Bp5OnLmordaWKuqutb\nh9Ip1vE0ko5I4BVCCCfUh29D27gOdebjtV2UuuHIfnINvmy7dBTveKXaHLI2v6Z2saatGvaE3aIE\nALoX5qGMuReldQf0C75Cv6Di8b7ad7ZLmmrLP3Z9Qm4OyxP7W3cDvat3fWB96apMw078wj+72s+C\n5YoEXiGEqEhmem2XoNZp+bmor0xmbN8pTNF3sTk28x9lk0cUR5Z1tvoo3YcHuz0EgO61T9C9/hm6\nKW+gRMfZvyeNtowH1oqLcSjfdnk+5fLr0Xb8gfmuwWhHD9ocM5/JQPvl//BSnVyrGmil5VdaplaQ\n054EXiGEcMA85/naLkKd4mrRg5Swsibdr3adsTl21D8Gk94Lxc8fxceXjOBYvtubxQdb0vn92Dn+\nPJXPrJ+PU5x+EgB1wnDHNzHl2Zbnh69R50yxnPP8A6hfltWA0269CgCltLp9ebL7k1u463xNXmeo\n/KhcGccrhBCO7Nhks6vlZKEEudc5pyHS5s9ymP5gr1j05SaPcLSm7eaw1vQr3f7n8v0Or9MzrjO9\nj28CvX1Y0ooKIfec6/J98znmbz637ptRWJxsCcDXtQp1eW5VaKXDpdyZiONCUuMVQgg3aLt31HYR\n6qTL3KhN/if1FoYs2sWtX+x1mmd2ixsxo6D0Hmh/sDToasDmwfdRrFT8vvadFkOt26FuDCGqqvW5\n3pU+RwKvEEK441x2bZegXmjiYg7knELXszvNbzkMCkwOjlhq0f/XdSQv5CRyU//p1iO6yY7HBpfo\nyoJzUDV3rCrvWFHlg7oEXiGEcIMSXP3NlfXNhTXN0Q6WvqvElMV2VsX1QNu4Du2CZn6yszjjFcTc\nwG7WpDTfCHSvLkZpkoTuudftrhWWZOnwdVunSJum8OqW5P7wXSsJvEIIcQEtP88+MaD6O+jUNznG\nsrGqr1/XjJvahdvlOV+7vCTO+bhWP6OOKQMTmHt9MoOaB9MvyXYKRvXCjm05Z3mj9UibpPt6PIri\nb1naT4lvim7SdEhpaz2ebbQc69bE8fJ/1WVIx9hKnyOBVwghLrRrGwBFOgPLEgZw1isAzCW1XKja\no2kaGnBX76cBuLZVKInB3g47Fj3YO44rU4K5v1cszw5o4vB6C4el0CnWn7ggLyb2jOXhPnFE+Bnw\nKylrZtYy09HOnLZs/7Ge0962Hdv6JAZyPKeIx747TH6xGaVlKkqn7gDkGXz4PtcS+IN9PNuHWF+F\nyrT0ahZCCCdG9XsRgF+iOvJKIw68FBZwyifMuhvo5bzOFulv5F89LLXA8k28K8a0dnmLjPwSMPiy\nM7gZqdkHUR//J2CZbEP7dTV5vfvY5P/lyDl+OWLpdDX6871MvTyBdqXTSN7at6zGHOCirNVBJ72a\nhRCiGvjYvrg7GBjfuGu8q1bwfMd/Wvfdnce4CjGJZzrfa7OvTv03ZhSyvAJdn/fDUW7ITGVsn8k2\n6VUJjJVRmVWJznNZ4y0pKWHu3LmcPn2a4uJihg8fTnh4ODNmzCA21vIXzZVXXknv3r2rVGAhhKiL\ntEN7+brJpbaJnliovZ7QvlrMyQEzrfu7MkwMcec899eR597u0cz9/RSXZPxle6DQxFnvsvfrHWL8\n2H4y3+l1csu9hx7U3PPv5Y26ykdel4F33bp1BAYGMnHiRHJzc5k0aRIjRozguuuu4/rrr69yQYUQ\noi7TvvyI98sFGoBj+RoJtVSeWtckyWb3wJkCt047P6PVsLZhFeSEzqWLDASUOA+qTUO8ebp/E278\nbI817d+9Y3ll/QmH+Sf2rHzHJ3dNGZjAqv1ZdHbRicwZl4G3V69e9OzZE7C8XNfr9Rw4cIC0tDQ2\nbdpETEwMt99+O76+zlegOC8uLq7CPI2RPBfPkWdbvRrT8zzqIC1NCaSHB59BXX6+J71tJ4l455Zu\nRAc6XiT+QhseaeJWc29AQTFwgLUxlzBh9xcYyq1ydL6HcvdmETRLbAJYAu/HY7vRKjqQLilNuPXD\njTbXu7tPM48+07g4uKZr1c51GXh9fCwP1mQy8fLLLzNq1CiKi4u5/PLLSU5OZtmyZSxZsoSxY8dW\neKO0tLSqlbABi4uLk+fiIfJsq1eje556+3eY6dl5HnsGdf35mvfv5nx1/8ubW2E+d4Y01zM4Vv4e\nalm79JhLp/LZT09Z98+WLmbvpxWRlpbGq9ck4WfUEWg+R1raOYKAGVck8vG20/yZbukZ7WU21foz\ndRb4K2yczsjIYMqUKVx66aX07duX7t27k5ycDED37t05dOhQtRZUCCFqnYP3uXGbVtVCQaqXlnYE\nbc+faGczbRa1d5hX09AO7rWs/gPoNJU2kb4e66xUvgd0sc6IioKKJW1nsCXmnF/ar1moD9EBtjNk\ntYnysxmzG+9iBq3a5rLGm5WVxbRp07jjjjto3749gHU/JSWFHTt2WIOwEEI0FI7mAtbnZtVCSaqP\nduIY6uT7rPvKNTei3HCLXT51/iy0jetQbv0X2kdvApbxzKqiw9tQcwNhRgx4iYS8kzy9/T1WlK6r\nq1bQWav88TaRVZhSqoa4DLxffvklubm5LF26lKVLlwIwduxYPvjgA/R6PSEhIYwfP75GCiqEEDVB\nO7CbLWGt7NLNSj0ffXnC9s219s3nUBp4taxM1DemgU4HBy3vT88HXbCMYwYoNruuJVe3o/4xPNvp\nbut+apTr/kS9EgL5YMtp7u1btyuELgPvuHHjGDdunF361KlTPVYgIYSoaVraEQiNQPH1Qzu8D9VB\nc2qJrvbnG9LOnLaUs0rNvfbVRfXHlSgduqGt+x4O73N65hutbwJgZ7qjBQw865Rv2bSU0QFGl3lj\nA734YlRLEpvEc+KE457OdUHt/yQJIUQt0vLzLE2woRHoZ74HxUX4mIusx5ONhRwo9qbEjaXoPMk8\nfRIc2A1eXujf/KLyF3AwqFb7+C00QOl9udPTdBOeJPqwgVN5JTzVP77y960m17YMwejGbBVGva6K\nf5jUnHrediKEEBepoHTc6NkMAJSIGA4ElgWYKwIsCybUduDlwG7L/4uKXOdzxsVsFtr6H5weUzr3\nxKv03W7nWM8uOODKoOYhFWeqJ6TGK4Ro3C4ISFpRIR8nX2Pd15fOgl9+fdfqvb2GanI+aYSd5s7n\nPNYKC8FoQHFQVq242O1b6F6cj7b2G4iyDIc5mm0J9saqrAhQTfw9POdyTWo4n0QIIariwqFD5RZi\nD/HRYygd5uKpGq/28yqOj+iH9sd61xnDLGvfKqGW/6vffoH6y/+VXedcDup9I9Hef83h6XkFxQwb\nMNP639/BSU5vpUTGoBt5Bxub92HIol2V+0AeEuTdcOqJDeeTCCFEVVwYeAsL8DYXUaj34uE+cZzN\n8IP0c2wPbcE/qvG22s4toKloH74BgDpvBjRvjdKlF7orbyjLZzbD7u1wxtIUrpVYaq7asg8txV/6\nAcqou1AMlq9z7be1mH9bi+7F+SiRMdbr7Mi1rWc91XkCX/ptRPtmCSoKv0W245KkcN5tPYxVi3Yx\nrksk728+XY2fuGLPlC4jOHXtMbtjvsaGU0+UwCuEaNzUssBrfutFKDBRGN0GgAg/I9tKLBMx/Bzd\niUnVedtXJ9sn7t+Ftn8X2sDrUAyWHrzafz9F++9nZXmKitDKN4+fy0ZbMButtEZsvf5/nkY/4x3r\n/vmAXZ550FB03yxhY0RbZqfeakk8apmH+cKgO6mv56e0vCS+9t4h16SG8yeEEEJURfka75bfUP/e\nbt2NC/J7ak3bAAAgAElEQVRC54HXmlp+nuvjv67B/PS9lsXgywddgJIiKHbQwaq0RmyVmW57zXKf\n67wRXx1n2ICZ/Np5sMvyDEwOom/TIJd5hPsk8AohGrcLmprPTxZxnk+52Zq06loaMOOUy8Pah2/A\nqePWxeABtoekMGzATFad84eiQrduc75mrKWfQMvNcZpvhz7C6bEPhqfwQK+6u4BDfSSBVwjRqGm7\ntqOiWKeXyDbaLvNm827RUU2zCtTF8yp9znOdLLMEzm01AvV19yYxOj/7lLZyqcvOYWcLnP9BEeJT\n+28ka2Jd3ZokgVcI0agVffkx43s9ybyWwwHwUktsjhvLtzVXZmV3V/bb9hSOemmB26fqNLVsTO8F\nNEBrU1Zj19Z9j/rjSmiRSqHe8q769s6RFd7j85ta8tilcbx4RaLb5fKUj0a0YEL3mIoz1iMSeIUQ\njVq2VwBnvINZFdcDAP8Sy3CikamWqQr1NoG3+ucqLlF0bPZPRHn9M+jSy2m+89Ml9jy9A4AZ7cay\nIGUIuoemonTtg+nlxQwfMJOF/e4j66GZZUX++C0oNFFQGnhjAstW7bmmpf2kFB2i/fA26OidGERq\nVO0sNDCxpyXQ9k8KIshbb/tv0ADUfhuCEELUIg3bL3WT3rLo+/ll5Wy+86urxlvOvJbDWf3FNnwM\nCvP7XU/Bzr+IKMy2yVP82CxObbD0Sj4/kcfvEe0A+HYz3NT7DpqesfxR8PXus3wNTOt+PW1+/9py\ngZwsCnWWz+Nj0NEy3Ic9mQWMSA3nmz1lqy6tGON8co6aNKh5CM3DfGhSh5f2uxgSeIUQjdqFqw6Z\nDD5A2bvdjjHl3vmWrjunFZjAyxtFV8VGw+BQlkd2JwNvVsd2A6CgRGPsVgP0eoqlax+1/jmwadxU\nXtxQFvCLSteqLe+zHZmE+9l+nT/ldynLsARebecWdoZcCoCPXmHqoESyC0oI9zMy48pE5vx6gjeu\nq1sr+jQL9antIniMNDULIRolraQY89zpmMt1Ovo2rhdH/aIA8CsNvKG+BpLM2fiVmEAzoxWYUCfe\n5HYHJ4dKSviwyeV806Svw8Obw1rzVKd7yZq5iBcPetscK9IZKdTbr9KTmV9il0aX3gAU6gxsC2sJ\nWJrOfQw660LybSL9mDu4eYNrzq3LpMYrhGic/vwDNv9KiX+0NWlBy7IZo8r3ZvbBbGmqVTU4fbLs\n/CpK17leV3ZahzsAuOOb43bH/gpJJtfg3rvXwqSWeG9ezxJzgjUtLrBhNt/WJ1LjFUI0Sto5y7hW\n1ckC977lxu96FZsw6/QU5+ejvv9q2TVO2gfGCu+rafwS2rbS55V3d68n3co3+kxbTHovwsu9Mw7w\nruVVloQEXiFEI5V2FHC++EH5pldjniVIF6/6Co4etKars90LgDZM+XxUbvUjgIcGtqjwNHc6Pl3T\nMoS51yfjVW4VoTGXvoC+tDf2v7s0nKX16jNpahZCNEra/60ALO9THQnwKgvIutLAZf51rW2m7LOV\nv+/234FYm7TRXRNoH6Ixbtm+Sl2rbaQvT/Rvwp4ME13i/NGVLgC/eGQLRny6x5rvfE9tv0B/h9cR\nNUtqvEKIRkc7m2nd/qzZlQ7zBJZrklXOT72oKBDT5KLurfg7nvM4zNfAC4MSHB6bfJnje3ZvEkCQ\nt55L4gOsQRfAqNexbHQr6/7OEEuPZW+DfOXXBfKvIIRodNRHx1Uqv650QkkNBU7aLlmnVXJsr5aZ\njrfZ8dSTPg4C45P94+kS53jVHlc9kfU6xdozO790iJS3Xr7y6wL5VxBCiAqcr/GaS2uVmV5BFOlK\n39SVOBjG44K2aC5GtYQEQxFTBiYwf0jZ+NkLA+OXN7eiR5NAp9dSKwj6N7QNAyw9oQG8DTJkqC6Q\nwCuEEBXQYXnHqykKeQYf7ur9NE91nmA5WImFE9SfVwGWsbjeeoVOsf7W8bQAiSFlY3Yj/Aw2zceO\nJAZ7uzxe/j01XDDvtKg1EniFEI2KVmhZUq9QZ2B22zFunaOWflWq6DhWOsHG/sDS9675uY7vU1yE\nuu57tJJitNMn0fLz0D54HQ0o0hvxMjjuTX1F6Uo8Jarz2uzT/Zsw9fIEp03Q5+kvCNz+XjKUqC6Q\nXs1CiAZLXf8DoKDrPdCapn39CQAbw1NZf8Hau878GtUBsKzVuzDlemv6t3G9uPrtmeif+o/dOdqn\n76D9tNKyti5AcCgAJ3wta9/+lee43nO+A1SR2Xng9fPSubWAQbto2zyhvvKVXxdIjVcI0WBp77+G\nVm7CCwDtu2WAZVWiyiofdKF0pqtDex3f+4Dt0n/nhx4dLTdTliPn1569s2uU3bE7u0YR5W8gJcy9\neYzjG+giA/WdBF4hRIOkFRdbt9Vvl9ql7Q+Mr9J1W2Ufcny/3X+ifvVJWUK5e5V3fjKLG9uFOzze\nLNSHpaNbMai5/WQXg1uHsWBoSqWGBZ1fPOGp/lX7vKL6SbuDEKJBUl97zrqtLfsAtfT/562NuaRK\n190dnGSXpmmadRYrNSLa0rSdm+Pw/BfbW4YyBfs4f99qqMZOUO8MbU5BiYqfUd7v1hVS4xVCNDia\npsHuHbZp5YLu3kDXk2C0inC9iEF5ZkUHZnPZfc43beeds89bbjk/R2N2PUGnKBJ06xgJvEKIBkf7\n7B2Xxx/rer/NfrBetW53iPHjmQHuz05VrOhtAi/YNmnrXl2E7u0vAbiz9zPW9BAfaXBsrCTwCiEa\nHO2Hr93OOzI1nKERZWNxu8T620wXWZEivRHMZZNoKL0vh5PHSPcJZfSlL7DhDMzdeJo1sZeQU65D\nl69M39hoyb+8EKJRG54ajne51XxcjOJxaFVsDyjIL0soLEB95VnmtL6JQr0X0386znf7sni91Y02\n50UF2C9mLxoHCbxCiAZFO7Db5fHzK/WcZ9CBV7lvwsrWRBclX23TkUr74xc4l805o/NxtpF+BiL9\nJfA2VhJ4hRANivbHL9btDO9gMr1sVwNa2Pxam32DTrFZv/aKlGC37hMXWDZGVlu/mq2hLXgp9VbO\nV5gL9M7H0CaFup7qUTRsEniFEA2LvqzT0vheT3FX76dtDq+K62mzrygKxnLfhF5uruDTNb5sbVv1\nh695vuNdbIhsz6JmVwFw2ifM6bnD2joewysaBwm8QogGRTtsWUy+/NCd9VdPwBQUwY/RnR2eo1cq\n+WIX2yZps1K2vazpQAp1tj2Wu8TaLkDf1o3pHkXDJf3ZhRANhpafC39tBeDzltdZ02ebkoi69EnS\n88p6H49qH064n+U9q66Sa+ouv7kVm9PyrPtmpawXdJO8U5wzlgXaFWNaAzBkkWUKyRAXE2eIxkFq\nvEKIhmPvX9bNJXGX2hwqH3QBRneI5MoUy7SM+gomimobaTuhhqIodInzJxzLSkclurJg2iLnCN+X\nNmeXn4Dq6haWe1W217RoeCTwCiEajtL3u/sDKjcvcUU13p4J9ovRK4pCi/w0AM4ZypqOzxn9+aLp\n5QCUX9nv/NzL4zpHVqpsouFx2dRcUlLC3LlzOX36NMXFxQwfPpwmTZrw5ptvoigKCQkJ3Hnnneh0\nEr+FEHWAryUATrrkgUqe6Drwak6O63Ozwc92paNNEW2t2//uHWvdTgn3YdnoVuhlMfpGz2XgXbdu\nHYGBgUycOJHc3FwmTZpEUlISo0aNIjU1lfnz57Np0ya6d+9eU+UVQgintC2/uZXvXz1iLjjRdX61\nbEZJYgPLxt8aVMtUkU90uc/heQOa2Q5NkqAroILA26tXL3r2tLyr0DQNvV7PgQMHaNvW8hdd586d\n2bZtmwReIYRbtIxTaAf3ouvWt3qv+8d61J9Wwl9bOeNl3yxcIW/X42rLxV1mXNnUul3Ru2EhHHEZ\neH18LIstm0wmXn75ZUaNGsVHH32Eolh+2nx9fcnPz3d1Cau4uLiLLGrDJM/Fc+TZVq/qeJ5H7xoM\nQHjzFni3czy0p2rXnWHd/m+TioN6YFCwzec52sEbdm8DHH/O20Ij2ZZexD19m9G2adn4XIOLwDvh\n0uRKPTP5ea1edfl5VjicKCMjg9mzZ3PllVfSt29fPv74Y+sxk8mEv7+/i7PLpKWlVb2UDVRcXJw8\nFw+RZ1u9LvZ5aiePg76s5+/pv3egC4uujqKhvvuyzb47Q4OCMdl8nszMsqFBzj7n1MtigQKb43oX\nbdRXJhjdfmby81q96srzdBb8XQberKwspk2bxh133EH79u0BSEpKYufOnaSmprJlyxbatWtX/aUV\nQjQo6jP32iZUctysK9pva2329wQlWrcf6BWLgmWpv6PZRTQN8eZUbjGtLxgepFa1PC7e2Z5vGRTi\nQi4D75dffklubi5Lly5l6dKlANx+++28//77lJSUEB8fb30HLIQQtS3L6M+foSkAxAYYGJhc1rnp\n/GQZob7VN2/QzyFtHabLu1/hisufwHHjxjFu3Di79ClTpnisQEKIRkA1V5ynApqqwuH9Nmn7AxOs\n2yPaRbhfnCpWeM8ZfB2mD0+VuZiFczJlpBDCo7SSErs0xT/IQc7KUV/4Nxw9aJNmSO1o3d56Is86\naYWnGDQzJaXTRY5IDad9tB/v/ZHO1S1DPXpfUb/JzBdCCM8qKrRLUufNsNRYAa24CM1cuRqwdni/\nXdDVPTKNwGtusO6nnSt2+3peVWwbNhrKOowZ9QqdYv2Zc10zwqqxOVs0PBJ4hRCeVS7w7ghpzryW\nwyhRdJB2GK2kGHXCCNRXnq3UJbU//7Dd79obtUU73vr9lDWt/KxRFWkf7ceN7cJ55eqkSpXDq1zg\nvTzZvXV8hZA/y4QQnlVUyMfNrsLHXMTiZMtatR3P7KFPRjoEl74L3b2jctf0D7DZHR44FD7ZbZNm\nqMQsUYqiMKZj5edQNparKZdfJlAIVyTwCiE8q6iQZU0H2iTlG3zAYIRi+2Zod2iL5gGgXDGE3ZeN\nhu+P2OWpidkZywdeo3RlFm6SwCuE8KzCAsfpIaE2zdDaru0orTs4vYx27BDqlPsvuEYYjzsIujXF\nq9wCMRJ4hbukbUQI4VGqg2llzYoezCqkHbWmaYf2ur7Oq8/ZpWmtO9pnLOVTA02/QeUWtdfJhBnC\nTRJ4hRAeVZCba5fmYy4E1Yy68LWyRD/H089qRw9ivmswZJ+xST/sH8MjWxz3XO4Q7Uewj+cb9OxW\nORLCDdLULITwqMIC+/e4r7a9mQEXDCFSguzHvmrHDqE+b7+2rgb8u9tDkGs780XTEG+uaB7Mta1q\nZhxtlL+x4kxCXEACrxDCowoKHddKj+eZiepzJTcV96TX6e08cnAvJKWghJTN+mT3ThegZSrf59ov\n/fdEv3i6xQfU6Jq3sryuqAppahZCeFTB9s0O04/nFJBfoqEqOn6J6oT2zeeok8qmqFV/W+PwPP2k\n6egTkmzS7ugSRc+EwBpfaF4WQhBVIYFXCOFRhSdPOEz3/t9nFBWVTSd54XTJ2ruv2J2ju38yAOub\nWdbcbRLkxZDWoVzfWqZoFPWHBF4hhMdomkaek4UEAgpyMG393bq/Oay1y2uZ9F7ctjuQVfuy2HrS\n0lO6daQvd3SNlh7Fol6Rd7xCCM/5exu5Rj+Hh1RFxzHfKOv+Ef9oup7ZBYC2+09rujJ6PJw4yuSi\n9mQXqryx4aT12FUtPLsIgjumDEwgyFtfcUYhSkngFUJ4jJaeRoHey+GxEp2e+S2HWfeDivOs2+rs\nJ63baZdcwdu/n2LfKfvxwMY60LupU6zjYVBCOCOBVwjhMUpcUwr0Ox0eUxWdZTwvlh7KgcX5oLet\nOZ7wDedfXx90cLaFNDGL+kje8QohPMdcws9RjmeXKlH0dMv4y7r/ZeIAdgQmoe3cAoCKwr96PGZ3\nXpR/ufqCxF1RD0ngFUJ4jLb5V/YGJTo8tjsokcJyzdC7g5OY3Olu1FctPZe/aHq5w/MWDE2xbvsb\n5StM1D/yUyuE8JiStd86PbY4+WoyvZ2vYftpsyvt0p4Z0ASA+UOSeap/POF+MnOUqH/kHa8QwmM2\n9LvFZl+ngFpuwO4f4W0qdb22UZahSdEBXkQHOO60JURdJ4FXCOExpuxsKJ3bYurlCRzPKWLexlMV\nnvd0p3us269f24wzphKOZhfiZ5RhO6L+k6ZmIYRHaOkniDlkGY97RbyRDjH+GNwc/vNXSLJ1OzHE\nm06x/lzfOswj5RSipkngFUJ4RtYZzpVOnuHvY2kW7pcUVOFpZ70CPFosIWqbBF4hhGfodMxtNRyA\n5fstk2N4G3T4e9l/7cy9PplW2YcAuLP3szVWRCFqgwReIYRnKAoRBVkANAv1tiZ3iLafQjIuyIsS\nvX0P5ds6RXqufELUEgm8QgjPUBSU0jWHxl8SbU2+cLapm5MtQXl/QLzdJYalhtulCVHfSeAVQnjE\n0c1bORBoGXdbfhGBXgm2i9iHRjpe0u+yZhW/DxaiPpLAK4TwiByvssAZUC7wXnpBByuzeuFKvBa3\nd45ymC5EfSeBVwjhETpj2TQBAV7Ox986G2IU4ivTDIiGSQKvEMIjlAKTddvV+F2j3vaYToFPbmzh\nsXIJUdsk8AohPEIrLHArn1dp4D3f8/mh3nEyQ5Vo0KQtRwhRrTSzGTSN73N8wY014rvEWSbMmHxZ\nApuO59KnaWAFZwhRv0ngFUJUC03T0FatQFvyHugNNIvpwZoWLbiplfOZqEKN4GOwNLyF+hq4IiWk\npoorRK2RwCuEqBba/z5DW7HYsmMuoUhnmRCjRYzzYUH+8g0kGiF5xyuEqBbWoFsq32hZwi/Ay3l0\nvb2D4zG8QjRkEniFEB6Rp7d0lvJ3MZQoKEQmyRCNjzT0CCGqjQYcCIinSG/ku/jeAPg5WBThPPcW\nCRSiYZHAK4SoFhnewYzv9ZRdur+LoUGKRF7RCElTsxCiWkzu+i+H6b5GVzVeibyi8XGrxrt3714W\nLVrEc889x8GDB5kxYwaxsbEAXHnllfTu3dujhRRC1H0nvOyHArXU5bo8x8sggVc0PhUG3hUrVvDT\nTz/h4+MDwIEDB7juuuu4/vrrPV44IUT95h0R4fJ4QpBXDZVEiLqjwqbm6OhoHnnkEev+gQMH2Lx5\nM5MnT2bu3LmYTCYXZwshGgOtpNhh+o5019NGKvKSVzRCFdZ4e/bsSXp6unU/JSWFyy+/nOTkZJYt\nW8aSJUsYO3ZshTeKi4u7uJI2UPJcPEeebfVy9TzN2WdJyDvJUf8YN8/bVeE1Gxt5FtWrLj/PSvdq\n7t69O/7+/tbt9957z63z0tLSKnurBi8uLk6ei4fIs61eFT1P7VQa+Xofu/Ru8f4uz5N/Iwv5ea1e\ndeV5Ogv+le7VPG3aNPbt2wfAjh07SE5OvriSCSHqPXX2U+SVzlRV3r3d7WvAQjR2la7x/vOf/+T9\n999Hr9cTEhLC+PHjPVEuIUQ9UpJ9loLSmaoAvhjVEqNeRisK4YhbgTcqKopp06YBkJyczNSpUz1a\nKCFE/ZLXtpt1OzrAWGHQDfHRYypWPV0sIeokmblKCHHRcg2WZuY+Tfx4tH9ihfnfuyHF00USos6S\nwCuEuGiPBg4E4Jdj+W7l1+tkGJFovOQljBDiomh7dmIyWHo0X9ZMVhsSoiISeIUQF0Wd9QRdM/8G\n4PYuUbVcGiHqPgm8QoiLZlYsXyW+BvlKEaIi8lsihLhoW8NaAeCll3e3QlREAq8Qosq09BN8kHyt\ndV/mXhaiYhJ4hRBVpm34kRWJ/Wu7GELUKxJ4hRBVpsQ2qe0iCFHvSOAVQlRZ7rFj1u0W4faLJAgh\n7EngFUJU2brN+63bT/aX2q8Q7pDAK4SoMq/YeOt2mK9MhCeEOyTwCiGqLPt0JgD3dY+u5ZIIUX9I\n4BVCVIl2Ko1Pk64AQEWGEQnhLgm8Qogq0X7/iVY5hwHonRhYy6URov6QwCuEqBL1q8X8FZwMgL+X\nfJUI4S75bRFCVMkxvyjMOj0AOpmxSgi3SeAVQlSaZspnbUxXAEZ3iKjl0ghRv0jgFUJUmvb9cr5M\nvAyApiHetVwaIeoXCbxCiEpT//updfuSuIBaLIkQ9Y8EXiGEQ1pJCerq/6LlnbNNN5vJNpYFW6Ms\nBShEpUjgFUJYaaqKVlwEgPrKM2ifzEd9cIxtpoJ87ujzLAD/SAmu6SIKUe/JHG9CCCvtk/loa7+x\nTy8qLNvOy7Vu78ksqJFyCdGQSOAVQqDlnEX73xKboLs/IJ5JlzxAatZ+3isqsqYX5eRYt8d1iarR\ncgrREEjgFUKgvjENDu7h7+Aknuo8webYzpDmFBeU1WxzT2cA4XT1yqVjjH8Nl1SI+k/e8QohMB/a\nx2H/aLuge15BucB77oeVAERQ6DCvEMI1CbxCCD5MvoZ/d3vY6fGc335GU80AnClUAQiLlWZmIapC\nAq8Qgq8T+rk8nrH4HbTvlwOwIbYLAH7h4R4vlxANkQReIYSNEB+9XVqRzoi2728AvgtOBSDYQT4h\nRMUk8ArRyGknjtrsfzC8BW9dn0zPhAAGGEoXuu/xKDuVUNTvl9Mq+xAAfZoG1XRRhWgQJPAK0did\nyyak0DJEaOY/mgIQH+TFE/2aEFySb8223ByHtuQ9Mr2DidAKMOhkxiohqkICrxCN3LmCYrK8LbXX\nVhG+NseMimbd1oCDAbFk+IQSpS9CCFE1Mo5XiEbu0NlCwNfhMVO5r4jN4W3YHN4GgEhzXk0UTYgG\nSWq8QjRy2lnLe9we/g7G5TppTY4IlokzhKgqCbxCNHI5x48D0L5ZpN0xZ18Q2ZEJHiyREA2bBF4h\nGjGtpISTOZb3tUFB9rVYxUmNNzHY25PFEqJBk8ArRCOm3juMPwMstdcODuZddhR3R6SGc22rUA+X\nTIiGSwKvEI1Yoc7I1rBWAIT62ve1VPzsg/GtnSJlKJEQF8GtXs179+5l0aJFPPfcc5w8eZI333wT\nRVFISEjgzjvvRKeT+C1EfaPlnGV0v2ku8+iaJMHfZ637l8RJpyohLlaFEXPFihXMmzeP4uJiAD74\n4ANGjRrF888/j6ZpbNq0yeOFFEJUP/PDt1WYJ770Xe4liSG8d0NznuzfxNPFEqLBqzDwRkdH88gj\nj1j3Dxw4QNu2bQHo3Lkz27dv91zphBAesza6q3V7VumMVRe6rFkwD/SKZfrg9oT7GdFLE7MQF63C\npuaePXuSnp5uk6aUdnX09fUlPz/f0Wl24uLiqlC8hk+ei+fIs3VOPZfD621usu4P6NDcad5bmsQD\nEOIrz9OT5Oe1etXl51npmauUcuMLTCYT/v7uvfNJS0ur7K0avLi4OHkuHiLP1jXzy89A9BgAPhiW\nUuGzkufpWfJ8q1ddeZ7Ogn+le0UlJSWxc+dOALZs2UKbNm0urmRCiBq3P6fEuh3ioDezEMJzKv0b\nN3bsWN5++21KSkqIj4+nZ8+eniiXEMKDXky8obaLIESj5VbgjYqKYto0y7CDuLg4pkyZ4tFCCSE8\nRyswYVYsjV2RPtJZSoiaJgNwhWhk1FeeJccrAIBRHaNruTRCND4SeIVoZJYXlwXb/s2Ca7EkQjRO\nEniFaGQ+bH6ddduol6ZmIWqaBF4hGjgtPQ0t/QTquu8x3zXYmv7eDc7H7gohPEfGEQjRQGmqirbm\nG7RP5wNQoDOS4Ve25m64n7G2iiZEoyaBV4gGSp07g+JtG7lpwMzaLooQohwJvEI0INrpk2j//Qwt\n4yT707KY1H96bRdJCHEBCbxCNBBa1hmKnrqHXcFJ+JYUMumSB5zmfaBXbA2WTAhRngReIeo5TVXR\nflyJtngeNzppVr66RQjf7s3izeua0aR0qT8hRO2QwCtEPaZpGurUBylMO8ZoB0G3a5w/D/SKJdjH\nwD3dY2qhhEKIC8lwIiHqs31/89c5GN3vRbtDKWE+PHtZAsE+8ve1EHWJ/EYKUY8dfGceT3eeYJM2\n+6qm5BWppEb51lKphBCuSOAVop7S/ljPE23vtEn7/KaWeBukIUuIukx+Q4Woh7SiQorenkmBoayj\n1BP94iXoClEPSI1XiFqm/fEL2ubfoFMPdN36us6blYn2vyVoa7+x6cG8YkxrTxdTCFFNJPAKUYu0\n9BOcefcNjvrHEPrnXzSZPxOl12WQkIxy2TVof6xH++J9lBv/CUUFFH3wJruCk/hP72dru+hCiCqS\nwCtEDdLO5cCZdJSmKWin0ih6ZgJ39J9hPd7z9A46H9pNxK51dPr8XQ4ExPNG67HctuRLSnQG/tt+\nHNvDWtpc8/ObWl54GyFEHSaBVzRo2vEjEBWLYnS8IICWdgRt7Tcoo+9GUTy7RJ52NhP10XHWfZPe\nmzHlgi7Ab5Ht+S2yvd25z3e8y+E1l9/cyuPlFkJULwm8osHS0tNQn7sP2nSE4mKUrr3RDRpsk0ed\nfJ8l75pv0L04HyXSc5NMFD96ByNL38sOPbKGH2K7X9T15g9JlqArRD0kXSBFg6Ud2MO+wCYU7v6L\nPzMKOL18ic1xdd5LjO/5BMMGzOSYXxTqk+PRNM0jZVE3/Mis1Fut+8sTL+Oc0R8AL73CO0Ob0ynW\n361rTRmYwIoxrYkO8PJIWYUQniU1XtFgHT98nEe73k9c/mnSStehXYFlmkXtg9c5u30rGX2GAHB/\n90cAWJ6fB/4BNtfRigrhzz8gtSuKt3vzHGvFxVBUiOIfgJafy9kP3ub3Po47RH1+U0sURWHKwARO\nnCviVG4xr65Pw99Lz7GcItpE+vL3aRO3dIxgZLuIKj4NIURdIYFXNFgHtu2C1K7WoAugFeSjThzF\nYf8Y/t1nst05BZmZ+JYLvObnJsLxwxTpDHipJegXfFXhfbVz2agPldZuwyIxn8206UD1yY0tGP35\nXut++ebi2EAvYgO9WDi8RaU+qxCi/pDAKxqsl1PH2KWpE0fxn7Y380tUJ4fnZMyeQny3riiXXYu2\n7ns2mXx5sfS9bPOco/wnJwslKMTpPbWTxzE/cy/DS8+5+cBKvm5zqfV4pxg//Ix6lt/cii0n8mgf\n7TNNmNIAABW2SURBVHcxH1EIUQ/JO17RqAwbMNMm6F7RPJgZVyZa9+/r8Sird2eQ98Ij/L1pBy92\nuMN6bH9QAqZJd6BtXu/w2tqW31DLBV2AxclXWd/lAjx7WQJgqeV2iQvAqJdfQSEaG/mtF41Wh2g/\n7usZS5tIP+6OL7Cmv97mJsZc+gJPdvmX3Tmj+73IwQ8XYr5rMOpvazA/MwEt4xQAJ9+byzAn6+GC\nZeiPXie9kIVo7CTwikYpOsDI1EFlNd3wQOdNvuMviaZVQFlv5393e4iFza/l+OKP+NCvA/lPTUAr\nMHFPzydszuvepOxdcfNQbxn6I4QA5B2vaKAqGhb09uBkm/2QyFDYlWmXr1moN9e2CiXeX8/kH9Os\n6V8l9OerhP4ALE8cwJKJo2DAS9bj0wYl0q70/e2OU3kkhfhU+bMIIRoWCbyiQcr7dikQ5vDYuC6R\ndrXPyMhQoCzwXtcqlDu7RqErzdc21naI0YVGlgu6j/SJswZdgPbR7o3PFUI0DhJ4RYP03bc/Q6Jl\nlqoLh+8MbRNulz/M18Ab1zUjxMfAWVMJCcFeNsHZS69j2ehWqJrGiE/3uLz3pUlB1fQphBANkbzj\nFQ3Su0lXW7f9jHq3zkkI9ibQW09iiOP3sXqdglGvY8WY1kzsaZlasqm/7a/QohEy/lYI4ZoEXtEg\nDTJamo3HNfPMj/ig5iGsGNOah/qXddB6f1gKAd7uBXkhROMlgVc0SPkFxQC0jw0EYMYVifgadHxy\nY/XWSJNCffjPVUl8ODyFMF95cyOEqJh8U4gG6Uwx4AUhYZb3rW2i/PjUQ+vWpoRLj2UhhPukxisa\npPWGOACCXYzPFUKI2iCBVzQo2sljmO4ehllneddqkJmihBB1jDQ1iwZBM5shPY2XPv2VX/u9WNvF\nEUIIp6TGKxqEn555jqGr8/g1qoM1TY9nFrUXQoiLITVeUa+V/LUN02/r7JYA7NUsjPu72U+UIYQQ\ntU0Cr6jXRmw2onkPsklrUXSaOSMGkpaW5uQsIYSoPVUOvI899hi+vr4AREVFMWHChGorlBDu0hT7\ntyX928TWQkmEEMI9VQq8RUVFaJrGc889V83FEcJ9JYVFdmmtvAr4R7cODnILIUTdUKXAe/jwYQoL\nC3nhhRcwm82MHj2ali09MzmBEM6Y8vNt9kcajnPLyMtrqTRCCOEeRato4VIHjhw5wp49e7j88ss5\nceIE06dP59VXX0Wvl3lqRc157sX3+J8xybr/yqWR9O3ZvvYKJIQQbqhSjTc2NpaYmBgURSEuLo6A\ngADOnj1LRESE03Oko4u9uLg4eS5VtPmPXTZBFyAmMtD6POXZVi95np4lz7d61ZXnGRcX5zC9SoF3\nzZo1HDlyhH/+85+cOXMGk8lEaGjoRRVQiMqYsqts++U+IUTHReLnJS0uQoi6r0qBd+DAgbz55ps8\n88wzKIrCvffeK83MotY0T4qp7SIIIYTbqhR4DQYDDzzwQHWXRQi3ZGactW53MB0HWtdeYYQQopJk\nykhR73zz4w7r9qTrZeiQEKJ+kcAr6p30Qsv/ryjYT1B0ZO0WRgghKkkCr6g1p/OKuX/FHj7ccooj\nWQVun/eTFgVA6+gATxVNCCE8RuZqFrXi8x+2s+ikFwCH/zrL0r8s721fvSqRB1ceAWDuVfHEhQei\nahrFZg1vg468IrP1Gv0HXlLzBRdCiIskgVfUivNB90Lngy7AMysP8O6Yjjy1dBt/FfrY5TX6+Xus\nfEII4SnS1Cxq3Lxfj1m3g4pynebLwJshi3Y5DLovB+7xSNmEEMLTJPCKGlVQVMK3ByzB1rekgP+k\nmnm5eD0fp2SSWHQGgMd3LHR5jWGZm2g+eLCniyqEEB4hTc2iRi3/cSfgDcDH3RQMbXsQ1b0HAHO6\nFMOB3XDzdCa+8havR5cteNA9408ubxrATwezGDPumtoouhBCVAsJvKLG7D+eySfplqD7Ulw6hrb9\nbI4rRiO0agfAoIf+Rc+Nv+J9+jjFWzbgO/EplKAQetZ4qYUQonpJ4BWV9sFvR1i2P99lnmWjW3Ey\nt5gJXx8A4NWrm/LQ2tPW460GXFrhfQK69QLAeM2IiyitEELULRJ4RaVVFHQBhn2y22b/wW8PW7fv\nPrISRZFpHoUQjZN0rhKVkp3n/kQXzlz92P3VUBIhhKifJPCKSjl26ITN/hVpG3j/lyks761jqdev\nLDr8PhEFZ23ytM4+CMDgoz/yuvYbik5+7IQQjZc0NQu3/Xgwm7X7cgEjd+f/f3t3H1VVvedx/L0P\nBziApEhKAqJAhaD5kHN9ICjTck11s5lVqal3aiyzfJq8YjOZOmjqWFoul5pTubRVy1u3lplTC+06\nPmHqIlxyLw5KiuADKCIKIvLM2fMHiZEPKfewD5w+rz+Ufc5++O7f2p6Pv80+v98BHnvkdxiRzwPP\nA2CPvJd2wIeH/sYffqjgsrc/b9T/lYEv/B4KCzDGTnRn+SIirYKCV27JwfxS3ttbCHgD0PmeaIzI\ne667rldcH9b7ZmOeyMFI+GcMH1+IirGwWhGR1kvBKzdVXedk1J9/xMRo8np4SIebbmdE98CI1gNU\nIiK/pOCVm/pox5Emoftw4X6CqssIGfWyG6sSEWm7FLxyXfVOkw1Z59ladPW1x/O/Z+K/v+S+okRE\nPICCV67xzeHzrDlwrslr/3b2f3k46VU3VSQi4jkUvAKA0zQpKq9lSeopckprm7z38R1ZdBgzGcMw\nbrC1iIjcKgXvb9SBnELmpZUyJiYA3wB/SvLP8HVR08vhvzOWETLzP7GFPO2mKkVEPI+C143qnE5M\n03RLT3LhvvNg8+JPP14GLvPLS+Hrf6jDGPuB5XWJiHg6Ba/FnKaJaV4ZyzgbgLcGB9E7KsTlx7pc\nU4/NMPi/s5dZsKuA8T0DeapvGEfPlFBn87pm/fvPZzP3wS5wb08M/3Yur0dERBS8LS47/zztA/zY\nm3OOT45UXnedOftKWNEhkIiO/i47bk29kzFfHm3y2tqsS6zNym5c7nUxl9mdz2MLCsI4sBfvybMU\nuCIiLUzB24K27j3Myrwb30ae+OMGPohp+P3p1M0nAehur+Y/Ho/l1MVqurb3pUugzy0f72DuWQwf\nb97cVfTrKwOTB4fi1++nSeWH/f6WjyMiIs2n4G0hBw8dv27ojsndzFm/YJ5/rB89klYybH86I3eW\nN75/vM6XV/4nt3E54Y5qHL4+5FfbSBoahbfh5PmNebdVy7jcFJ6Z/Ae++HgTfwpJAGDj452wBQU3\n8+xERKS5FLwuYJomGYdPMS+jYZ7aQGc1l2y+AIwr3kunskK8O4UQ//ILGLarwygahoFvWDgr705n\nSk7gdff9fZnvlaPw0tfHbqke/7pKnj6xnU+jn+C/wi4QN/aPAIz640uMrKwAh5++GiQi4iYK3mba\ntPcIa/Oc133vSuhGVBbxzLR//dWQ6zrwd2wa2PCzefkS+DRs/+6qjezu1Oe628w9u5l3godSZfel\nW/kZ4s/9jf49wgiLjMAR2wubfz+euc52hp/rfo8sIiK3T8F7Gw4fzcf09eOzfXlk1l37ENKdVSUU\nO4L4l9rD9Al00umhQbfdszQCrvZ8k14bxYz6eqitxnD44yw6Q/3JXOy97sdwTOfPjWv2AB5u9nmJ\niIh1PC54TdNk05Yf+Ou5avy9oMYJA8LaMXBwL9r7NX1Q6XxZBdu+2cl6onA4a6myNUx5F+W8yOTE\nbqQfOsXnJQ1BGFp3kdP29kA5cDV0u14upNLLlw/+6R7swVduI7tuVh7Dywu8Gnqpts5dsHXu4rJ9\ni4iI9Vpl8DqdTurq6yk4c4GjeWfoERNBfVUVZn09d97VGW9vLwpOnCG44x0EBbenqrqWC8Ul/CXt\nCBsr7wTaw5Wvqdog/Qys+iqXzrWXuLuykB/aRVJnu3LqUQCNoQuQa2vPjD2lwNXeZ0PoQreKs3Sr\nv8iwu4Po+0girgxZERHxfJYF71Prs695LbymhECjjv5BBk7TJKpzIKknL5Na//OnbR2Q//Ovx/z8\nid7L2MwCnIbtp+U7G/6sLmV6lJPDhZc4eBGC7fVsd0RR5B1IkXfTh5ieqM1lzJBY7AHt+OYvPxDZ\nzs5b5d0AiCs/xSnfjvxjxRGiIzrTN74ffh0VtCIi0nxu7fHm+wQBcPjKt2nyABzXXTex5iQ1ToOD\n9jupsPvRt6qAGmzUmgYVhjcFjmCiKwp5LLiWh595ELuvL72AZ3/afkp1FRfyTuDlrKfyQgmd7onC\n5uOLPehqkD479gkANsFPQzleea+/i89cRER+qywL3q/HxHAh7wTVFRV07NaVssKzVFXVcOFCGUcK\nL1HjBLy8KKrz4rGYYGL79/rFHn7Z07xez/PGvVEvXwedesTccr36uo2IiLQEy4LXMAyCo7o3LjsC\nG275RgB9rSpCRETEzWy/voqIiIi4ioJXRETEQgpeERERCyl4RURELNSsh6ucTidr1qzhxIkTeHt7\n88orr3DXXXe5ujYRERGP06web3p6OrW1tSxcuJAxY8bwySefuLouERERj9SsHm92djZ9+zZ8Ceje\ne+/l2LFfn64uNDS0OYfyeGqXlqO2dS21Z8tS+7pWa27PZgVvZWUl/v5Xp5ez2WzU19fj5eV1w21O\nnz7dnEN5tNDQULVLC1Hbupbas2WpfV2rtbTnjcK/Wbea/fz8qKysbFw2TfOmoSsiIiINmhW8MTEx\nZGRkAHDkyBEiIiJcWpSIiIinatat5gEDBpCZmcns2bMxTZNJkya5ui4RERGPZJimabq7CBERkd8K\nDaAhIiJiIQWviIiIhRS8IiIiFlLwioiIWEjBKyIiYiEFr4iIiIUUvCIiIhZS8LagrKwsRo4cyZ49\ne5q8npSUxKpVq9xUlefZtGkTL7/8MjU1Ne4upc3StWqd5ORkCgoK3F2Gx7lZu06ePLlVfT4oeFtY\nWFhYkw+zkydPUl1d7caKPM/u3buJj49n79697i6lTdO1KmKNZg0ZKbeuW7dunD59moqKCvz9/UlN\nTSUhIYHi4mK2bNlCWloa1dXVBAYGMnPmTL7//nt27NiB0+lk5MiR3Hfffe4+hVYtKyuLkJAQhg8f\nzooVKxgyZAjJycmNs5OYpsn06dMpKChg/fr12O12HnnkER588EF3l97q3O61umrVKhITE7n//vvJ\nz8/n008/5Y033nD3abQJX375JXFxcQwfPpyCggI++ugjkpOTSUpKIi4ujhMnTmAYBq+//nqTmeDk\n5m7Urq2NerwWGDhwIGlpaZimybFjx4iJicE0TS5dusScOXNYtGgRTqeTnJwcAAICAnjrrbcUurdg\n27ZtDBs2jNDQUOx2O0ePHgUaJvJITk4mPj6er776CoDa2lrmz5+v0L2J27lWhw0bxs6dOwHYsWMH\nQ4cOdW/xHqCyspIHHniAefPm0bFjx8bJaMSzqMdrgYSEBNasWUNISAg9evQAwDAM7HY7y5cvx+Fw\ncP78eerr64HWPYFza1JeXk5GRgZlZWVs3ryZiooKtmzZAkCvXr2AhgDev38/AF26dHFbrW3F7Vyr\nPXv2ZN26dZSVlZGZmclzzz3n5upbr6qqKux2O3b7tR+5vxwuPzIyEoDg4GBqa2stqa+tup12bU3U\n47VASEgIVVVVpKSkkJiYCDT8zzY9PZ3p06czfvx4TNNsvFAMw3BnuW3G7t27GTp0KLNnz+bNN99k\n0aJFZGZmUlZWRm5uLgDZ2dmEh4cDYLPpcv81t3OtGoZBYmIia9eupXfv3tf98JMGK1euJDs7G6fT\nycWLF4mIiKC0tBSAvLw8N1fXdrXVdtW/FIvEx8eTmppKaGgoRUVF2Gw2fH19mTNnDgAdOnSgpKTE\nzVW2Ldu3b2fKlCmNy76+vgwcOJBt27axc+dOvv32WxwOB1OmTOHkyZNurLRtuZ1rdciQIbz66qss\nXbrUnSW3ek8++STr1q0DYNCgQSQkJLBs2TIOHTpEVFSUm6tru9pqu2paQPE4ycnJTJgwgbCwMHeX\n4vEuXLjAypUrmTt3rrtLEWkz1OMVkWZJS0vjiy++YMKECe4uRaRNUY9XRETEQnraRERExEK61exi\ndXV1rF69mnPnzlFbW8vTTz9NeHg4q1atwjAMunbtyosvvtj4hG1hYSFLlizh3XffBeDjjz/m+PHj\nAJSWlhIQEMDChQvddToiIuJiCl4X2717N4GBgUydOpXy8nJmzpxJ9+7dGT16ND179uTDDz9k//79\nDBgwgNTUVFJSUigrK2vc/oUXXgAaAnzu3LlMnDjRTWciIiItQbeaXWzw4MGMGjUKaPgCt5eXF7m5\nucTFxQHQr18/MjMzgYYRqm40nNmWLVvo3bs3ERERltQtIiLWUPC6mMPhwM/Pj8rKSt577z1Gjx4N\nXB0Uw8/Pj4qKCgD69++Pw+G4Zh91dXVs3bqVESNGWFe4iIhYQsHbAoqLi5k3bx6JiYkkJCQ0GYmq\nsrKSgICAm26fmZlJbGysBkcXEfFACl4XKy0tZeHChYwdO7Zx0Pju3buTlZUFQEZGBrGxsTfdx8GD\nB+nXr1+L1yoiItbTw1UutnHjRsrLy9mwYQMbNmwAGh6YWrduHXV1dYSFhTFo0KCb7uP06dM89NBD\nVpQrIiIW0wAaIiIiFtKtZhEREQspeEVERCyk4BUREbGQgldERMRCCl4RERELKXhF2ohjx441TqZx\nK8rKyhg5cmQLViQizaHgFWkjoqOjmTFjhrvLEJG/kwbQEGkjsrKyWLt2LVFRUfj5+XHq1CmKi4sJ\nCwvjtddew+FwkJaWxueff46Pjw/R0dFNtt++fTvfffcdpmkSGBjI+PHj6dKlCwsWLCAqKopx48aR\nmZnJ+++/z+LFi+nQoYObzlTEs6nHK9IG5eXlMWvWLJYtW0ZJSQn79u2jtLSU1atXM2PGDN5++206\nderUuP6hQ4fYtWsX8+fP55133mHEiBEsXboUm83G1KlT2bVrF+np6axevZpp06YpdEVakHq8Im1Q\nnz598Pb2BqBr166Ul5eTnZ1NREQE4eHhADz66KN89tlnABw4cIDCwkJmz57duI/y8nLKy8sJCgpi\n4sSJLFmyhGeffbZxCksRaRkKXpE2yMfHp/FnwzAwTbPx7ytstqs3tJxOJ4mJiYwbN65xuaSkpHGm\nrPz8fNq3b09OTo5FZyDy26VbzSIeIjY2lvz8fI4fPw7Azp07G9/r3bs3e/bsoaSkBICtW7cyf/58\nAHJyckhJSWHx4sVUVFSQkpJidekivynq8Yp4iDvuuINp06axYsUK7HZ7k+kn+/bty1NPPcWCBQsw\nDAM/Pz+SkpKoqqpi+fLljB8/no4dOzJp0iRmzZpFbGwskZGRbjwbEc+l2YlEREQspFvNIiIiFlLw\nioiIWEjBKyIiYiEFr4iIiIUUvCIiIhZS8IqIiFhIwSsiImKh/wcGi55i2EIuTwAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f51cae47e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# one big test\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n",
    "steps=len(df_test)-window_length-2\n",
    "env_test = PortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=steps, \n",
    "    scale=True, \n",
    "    augment=0.00,\n",
    "    trading_cost=0, # let just overfit first\n",
    "    window_length=window_length,\n",
    ")\n",
    "env_test.seed = 0  \n",
    "agent.test(env_test, nb_episodes=1, visualize=False)\n",
    "\n",
    "df = pd.DataFrame(env_test.infos)\n",
    "df.index=df['index']\n",
    "\n",
    "s=sharpe(df.rate_of_return+1)\n",
    "mdd=MDD(df.rate_of_return+1)\n",
    "print('APV (Accumulated portfolio value): \\t{: 2.6f}'.format(df.portfolio_value.iloc[-1]))\n",
    "print('SR (Sharpe ratio):                 \\t{: 2.6f}'.format( s))\n",
    "print('MDD (max drawdown):                \\t{: 2.6%}'.format( mdd))\n",
    "print('')\n",
    "\n",
    "# show one run vs average market performance\n",
    "df.portfolio_value.plot()\n",
    "df.mean_market_returns.cumprod().plot(label='mean market performance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-19T09:16:52.655Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The weights appear to be static, so the model hasn't learnt much\n",
    "df.index = df['index']\n",
    "df.weights.apply(lambda x:x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-19T09:16:52.657Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets evaluate a few 30 step intervals\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n",
    "env_test = PortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augment=0.00,\n",
    "    trading_cost=0, # let just overfit first\n",
    "    window_length=window_length,\n",
    ")\n",
    "env_test.seed = 0  \n",
    "\n",
    "for i in range(10):\n",
    "    agent.test(env_test, nb_episodes=1, visualize=False)\n",
    "    df = pd.DataFrame(env_test.infos)\n",
    "    s=sharpe(df.rate_of_return+1)\n",
    "    mdd=MDD(df.rate_of_return+1)\n",
    "    print('APV (Accumulated portfolio value): \\t{: 2.6f}'.format(df.portfolio_value.iloc[-1]))\n",
    "    print('SR (Sharpe ratio):                 \\t{: 2.6f}'.format( s))\n",
    "    print('MDD (max drawdown):                \\t{: 2.6%}'.format( mdd))\n",
    "    print('')\n",
    "    df.portfolio_value.plot(label=str(i))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T00:26:16.847387Z",
     "start_time": "2017-07-19T08:26:12.521925+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-19T09:16:52.659Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# history\n",
    "df_hist = pd.DataFrame(history.history)\n",
    "df_hist\n",
    "df_hist['episodes'] = df_hist.index\n",
    "\n",
    "g = sns.jointplot(x=\"episodes\", y=\"episode_reward\", data=df_hist, kind=\"reg\", size=10)\n",
    "plt.show()\n",
    "\n",
    "# g = sns.jointplot(x=\"episodes\", y=\"rewards\", data=history, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-01T03:02:19.820742Z",
     "start_time": "2017-07-01T11:02:19.740692+08:00"
    }
   },
   "source": [
    "# dummy metrics\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
