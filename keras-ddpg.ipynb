{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:38.432291Z",
     "start_time": "2017-07-23T14:09:37.784671+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:__main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# numeric\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:54.958396Z",
     "start_time": "2017-07-23T14:09:38.433930+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# reinforcement learning\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization, Conv1D, InputLayer, Dropout, regularizers, Conv2D, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:54.990899Z",
     "start_time": "2017-07-23T14:09:54.960526+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:55.020046Z",
     "start_time": "2017-07-23T14:09:54.993182+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/agent_portfolio-ddpg-keras/2017-07-21__weights.h5f'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_length = 50\n",
    "save_path= 'outputs/agent_portfolio-ddpg-keras/{}_weights.h5f'.format('2017-07-21_')\n",
    "save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "You can see the base environment class [here](https://github.com/openai/gym/blob/master/gym/core.py#L13) and openai's nice docs [here](https://gym.openai.com/docs). My environment is in `src/environments/portfolio.py` and the PortfolioEnvironment load a datasource and simulation subclass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:55.050521Z",
     "start_time": "2017-07-23T14:09:55.021418+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:55.223396Z",
     "start_time": "2017-07-23T14:09:55.052342+08:00"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_hdf('./data/poloniex_30m.hf',key='train')\n",
    "env = PortfolioEnv(\n",
    "    df=df_train,\n",
    "    steps=1440, \n",
    "    scale=True, \n",
    "    augment=0.0000, # let just overfit first,\n",
    "    trading_cost=0, # let just overfit first,\n",
    "    window_length = window_length,\n",
    "    \n",
    ")\n",
    "env.seed = 0   \n",
    "\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n",
    "env_test = PortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=1440, \n",
    "    scale=True, \n",
    "    augment=0.00,\n",
    "    trading_cost=0, # let just overfit first\n",
    "    window_length=window_length,\n",
    ")\n",
    "env_test.seed = 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-04T01:42:37.345932Z",
     "start_time": "2017-07-04T09:42:37.328860+08:00"
    },
    "collapsed": true
   },
   "source": [
    "# Model\n",
    "\n",
    "arXiv:1612.01277 indicated that CNN's are just as effective. That's great because I like them, they are fast so I can try more things and see the results faster. So we will be using a CNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:55.628200Z",
     "start_time": "2017-07-23T14:09:55.225321+08:00"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1, 5, 50, 3)       0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 5, 50, 3)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 5, 48, 2)          20        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 5, 48, 2)          8         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 1, 20)          1940      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 5, 1, 20)          80        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 1, 1)           21        \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 5, 1, 1)           4         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 36        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 2,109\n",
      "Trainable params: 2,063\n",
      "Non-trainable params: 46\n",
      "_________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "observation_input (InputLayer)   (None, 1, 5, 50, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 5, 50, 3)      0           observation_input[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 5, 48, 2)      20          reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 5, 48, 2)      8           conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 5, 1, 20)      1940        batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 5, 1, 20)      80          conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "action_input (InputLayer)        (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 100)           0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 106)           0           action_input[0][0]               \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 64)            6848        concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 64)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 64)            256         activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 32)            2080        batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 32)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 32)            128         activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 16)            528         batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 16)            0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 16)            64          activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             17          batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 1)             0           dense_5[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 11,969\n",
      "Trainable params: 11,701\n",
      "Non-trainable params: 268\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, merge, Reshape\n",
    "from keras.layers import concatenate, Conv2D\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.models import Model\n",
    "\n",
    "window_length=50\n",
    "nb_actions=env.action_space.shape[0]\n",
    "reg=1e-8\n",
    "\n",
    "# Simple CNN actor model\n",
    "actor = Sequential()\n",
    "actor.add(InputLayer(input_shape=(1,)+env.observation_space.shape))\n",
    "actor.add(Reshape(env.observation_space.shape))\n",
    "actor.add(Conv2D(\n",
    "    filters=2,\n",
    "    kernel_size=(1,3),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(BatchNormalization()) # lets add batch norm to decrease training time\n",
    "\n",
    "actor.add(Conv2D(\n",
    "    filters=20,\n",
    "    kernel_size=(1,window_length-2),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(BatchNormalization())\n",
    "\n",
    "actor.add(Conv2D(\n",
    "    filters=1,\n",
    "    kernel_size=(1,1),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(BatchNormalization())\n",
    "\n",
    "actor.add(Flatten())\n",
    "actor.add(Dense(\n",
    "    nb_actions, \n",
    "    kernel_regularizer=l2(reg)\n",
    ")) # this adds cash bias\n",
    "actor.add(Activation('softmax'))\n",
    "print(actor.summary())\n",
    "\n",
    "# Lets have nice flexible critic so it can approximate the Q-function\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "\n",
    "observation_input = Input(shape=(1,)+env.observation_space.shape, name='observation_input')\n",
    "y = Reshape(env.observation_space.shape)(observation_input)\n",
    "\n",
    "y = Conv2D(\n",
    "    filters=2,\n",
    "    kernel_size=(1,3),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    ")(y)\n",
    "y = BatchNormalization()(y) # lets add batch norm to decrease training time\n",
    "\n",
    "y = Conv2D(\n",
    "    filters=20,\n",
    "    kernel_size=(1,window_length-2),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    ")(y)\n",
    "y = BatchNormalization()(y)\n",
    "\n",
    "y = Flatten()(y)\n",
    "\n",
    "x = concatenate([action_input, y])\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(16)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:55.660715Z",
     "start_time": "2017-07-23T14:09:55.630064+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# # prioritised experience memory, lets make the tensorforce one work for keras-rl\n",
    "# # https://github.com/matthiasplappert/keras-rl/blob/master/rl/memory.py#L115\n",
    "# # https://github.com/reinforceio/tensorforce/blob/master/tensorforce/core/memories/prioritized_replay.py\n",
    "# from tensorforce.core.memories import PrioritizedReplay\n",
    "# from collections import namedtuple\n",
    "# Experience = namedtuple('Experience',\n",
    "#                         'state0, action, reward, state1, terminal1')\n",
    "\n",
    "\n",
    "# class KRLPrioritizedReplay(PrioritizedReplay):\n",
    "#     def get_recent_state(self, current_observation):\n",
    "#         return current_observation\n",
    "\n",
    "#     def sample(self, batch_size, batch_idxs=None):\n",
    "#         tensorforce_batch = super().get_batch(batch_size)\n",
    "#         experiences = []\n",
    "#         # now convert dict(states=states, actions=actions, rewards=rewards, terminals=terminals, internals=internals)\n",
    "#         # into Experience = namedtuple('Experience', 'state0, action, reward, state1, terminal1')\n",
    "#         for i in range(len(tensorforce_batch['states'])-1):\n",
    "#             experiences.append(\n",
    "#                 Experience(\n",
    "#                     state0=tensorforce_batch['states'][i],\n",
    "#                     action=tensorforce_batch['action'][i],\n",
    "#                     reward=tensorforce_batch['reward'][i],\n",
    "#                     state1=tensorforce_batch['states'][i+1],\n",
    "#                     terminal1=tensorforce_batch['terminals'][i+1],\n",
    "#                 ))\n",
    "#         return experiences\n",
    "\n",
    "#     def append(self, observation, action, reward, terminal, training=True):\n",
    "#         \"\"\"Backwards: Store most recent experience in memory.\"\"\"\n",
    "#         actions = dict([(name, action[i]) for i, name in enumerate(self.action_spec.keys())])\n",
    "#         states = dict([(name, state[i]) for i, name in enumerate(self.state_spec.keys())])\n",
    "#         return super().add_observation(\n",
    "#             state=states,\n",
    "#             action=actions,\n",
    "#             reward=reward,\n",
    "#             terminal=terminal,\n",
    "#             internal=[])\n",
    "\n",
    "#     @property\n",
    "#     def nb_entries(self):\n",
    "#         return len(self.observations)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = {\n",
    "#             'window_length': self.window_length,\n",
    "#             'ignore_episode_boundaries': self.ignore_episode_boundaries,\n",
    "#             'limit': self.limit\n",
    "#         }\n",
    "#         return config\n",
    "    \n",
    "    \n",
    "# # test\n",
    "# from tensorforce.config import Configuration\n",
    "# # Configuration.from_json\n",
    "# states={\n",
    "#     'state':dict(shape=tuple(env.observation_space.shape), type='float'),\n",
    "# }\n",
    "# actions={'action' + str(n): dict(continuous=True) for n in range(env.action_space.shape[0])}\n",
    "# config = Configuration(\n",
    "#     states=states,\n",
    "#     actions=actions,\n",
    "# )\n",
    "\n",
    "# from rl.memory import SequentialMemory\n",
    "# memory0 = SequentialMemory(limit=10000, window_length=1)\n",
    "# memory1 = KRLPrioritizedReplay(\n",
    "#     capacity=10000,\n",
    "#     states_config=config.states,\n",
    "#     actions_config=config.actions,\n",
    "#     prioritization_weight=1.0\n",
    "# )\n",
    "\n",
    "\n",
    "# observation = np.zeros(states['state']['shape'])\n",
    "# action = [0.5 for i in actions]\n",
    "# reward=0.5\n",
    "# terminal=False\n",
    "\n",
    "# for _ in range(10):\n",
    "#     memory0.append(observation, action, reward, terminal, training=True)\n",
    "#     state0 = memory0.get_recent_state(observation)\n",
    "# sample0 = memory0.sample(1)\n",
    "\n",
    "\n",
    "# for _ in range(10):\n",
    "#     memory1.append(observation, action, reward, terminal, training=True)\n",
    "#     state1 = memory1.get_recent_state(observation)\n",
    "# sample1 = memory1.sample(1)\n",
    "\n",
    "# assert sample0==sample1\n",
    "\n",
    "\n",
    "# TODO make batch return state1 https://github.com/matthiasplappert/keras-rl/blob/master/rl/memory.py#L153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:57.619763Z",
     "start_time": "2017-07-23T14:09:55.662424+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rl.agents.ddpg.DDPGAgent at 0x7f06654fcc50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.agents.ddpg import DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "# We configure and compile our agent.\n",
    "\n",
    "# We are providing the last 50 steps so we don't need memory, use window_lenght=1 as placeholder\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "\n",
    "random_process = OrnsteinUhlenbeckProcess(\n",
    "    size=nb_actions, theta=.15, mu=0., sigma=.1)\n",
    "\n",
    "agent = DDPGAgent(\n",
    "    nb_actions=nb_actions,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    critic_action_input=action_input,\n",
    "    random_process=random_process,\n",
    "    memory=memory,\n",
    "    batch_size=50,\n",
    "    nb_steps_warmup_critic=150,\n",
    "    nb_steps_warmup_actor=150,    \n",
    "    gamma=.00, # discounted factor of zero as per paper\n",
    "    target_model_update=1e-3\n",
    ")\n",
    "\n",
    "# agent.compile(Adam(lr=3e-5), metrics=['mse'])\n",
    "# agent.compile(Adam(lr=1e-3), metrics=['mse'])\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:06:00.585378Z",
     "start_time": "2017-07-23T14:06:00.562162+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:57.647349Z",
     "start_time": "2017-07-23T14:09:57.621300+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.callbacks.keras_rl_callbacks import TrainIntervalLoggerTQDMNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:57.675197Z",
     "start_time": "2017-07-23T14:09:57.649151+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sharpe(returns, freq=30, rfr=0):\n",
    "    \"\"\"Given a set of returns, calculates naive (rfr=0) sharpe (eq 28) \"\"\"\n",
    "    return (np.sqrt(freq) * np.mean(returns-rfr)) / np.std(returns - rfr)\n",
    "\n",
    "\n",
    "def MDD(returns):\n",
    "    \"\"\"Max drawdown.\"\"\"\n",
    "    peak = returns.max()\n",
    "    i = returns.argmax()\n",
    "    trough = returns[returns.argmax():].min()\n",
    "    return (trough-peak)/trough "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:57.719372Z",
     "start_time": "2017-07-23T14:09:57.676651+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # https://github.com/matthiasplappert/keras-rl/blob/master/rl/callbacks.py#L104\n",
    "from rl.callbacks import TrainEpisodeLogger\n",
    "class TrainEpisodeLoggerPortfolio(TrainEpisodeLogger):\n",
    "    \"\"\"Print custom stats every <log_intv> episode.\"\"\"\n",
    "    def __init__(self, log_intv):\n",
    "        super().__init__()\n",
    "        self.log_intv=log_intv\n",
    "        self.episode_metrics={} # custom metrics\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        \n",
    "        # save custom metrics\n",
    "        df = pd.DataFrame(self.env.infos)\n",
    "        self.episode_metrics[episode]=dict(\n",
    "            max_drawdown=MDD(df.portfolio_value), \n",
    "            sharpe=sharpe(df.rate_of_return), \n",
    "            accumulated_portfolio_value=df.portfolio_value.iloc[-1],\n",
    "            mean_market_return=df.mean_market_returns.cumprod().iloc[-1],\n",
    "            cash_bias=df.weights.apply(lambda x:x[0]).mean()\n",
    "        )\n",
    "        \n",
    "        if episode%self.log_intv==0:\n",
    "            # print normal metrics\n",
    "            super().on_episode_end(episode, logs)\n",
    "            \n",
    "            # print custom metrics for last N episodes\n",
    "            df = pd.DataFrame(self.episode_metrics).T[-self.log_intv:]          \n",
    "            for col in df.columns:\n",
    "                print('{name:25.25s}: {mean: 10.6f} [{min: 10.6f}, {max: 10.6f}]'.format(\n",
    "                    name=df[col].name, \n",
    "                    min=df[col].min(), \n",
    "                    mean=df[col].mean(), \n",
    "                    max=df[col].max(), \n",
    "                ))\n",
    "            print('')           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:57.743581Z",
     "start_time": "2017-07-23T14:09:57.720934+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rl.callbacks import ModelIntervalCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:09:58.248540Z",
     "start_time": "2017-07-23T14:09:57.745131+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.load_weights(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:52:21.893146Z",
     "start_time": "2017-07-23T14:09:58.251708+08:00"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a9dbd48c4344c7a56fec77e30f23b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000.0 steps ...\n",
      "Training for 2000000.0 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1440/2000000.0: episode: 1, duration: 39.401s, episode steps: 1440, steps per second: 37, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.079 [-0.449, 0.807], mean observation: 1.006 [0.716, 1.260], loss: 0.000000, mean_absolute_error: 0.000026, mean_q: 0.000000\n",
      "accumulated_portfolio_val:   1.213516 [  1.213516,   1.213516]\n",
      "cash_bias                :   0.317508 [  0.317508,   0.317508]\n",
      "max_drawdown             :  -0.112905 [ -0.112905,  -0.112905]\n",
      "mean_market_return       :   1.203628 [  1.203628,   1.203628]\n",
      "sharpe                   :   0.158765 [  0.158765,   0.158765]\n",
      "\n",
      "6 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.262 - rate_of_return: 0.000 - cost: 0.000 - steps: 716.060 - cash_bias: 0.193 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "Step 14400: saving model to outputs/agent_portfolio-ddpg-keras/2017-07-21__weights.h5f\n",
      "   15840/2000000.0: episode: 11, duration: 48.403s, episode steps: 1440, steps per second: 30, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.108 [-0.437, 0.892], mean observation: 1.000 [0.806, 1.179], loss: 0.000000, mean_absolute_error: 0.000058, mean_q: 0.000000\n",
      "accumulated_portfolio_val:   1.577905 [  0.932739,   2.036671]\n",
      "cash_bias                :   0.185535 [  0.088615,   0.316687]\n",
      "max_drawdown             :  -0.112632 [ -0.265706,   0.000000]\n",
      "mean_market_return       :   1.660421 [  0.988435,   2.715450]\n",
      "sharpe                   :   0.323919 [ -0.049854,   0.611510]\n",
      "\n",
      "7 episodes - episode_reward: 0.000 [-0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.122 - rate_of_return: 0.000 - cost: 0.000 - steps: 716.700 - cash_bias: 0.212 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "Step 28800: saving model to outputs/agent_portfolio-ddpg-keras/2017-07-21__weights.h5f\n",
      "7 episodes - episode_reward: 0.000 [-0.001, 0.000] - loss: 0.000 - mean_absolute_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.238 - rate_of_return: 0.000 - cost: 0.000 - steps: 717.340 - cash_bias: 0.127 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "   30240/2000000.0: episode: 21, duration: 44.595s, episode steps: 1440, steps per second: 32, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.218 [-0.413, 1.028], mean observation: 0.999 [0.793, 1.219], loss: 0.000000, mean_absolute_error: 0.000056, mean_q: 0.000000\n",
      "accumulated_portfolio_val:   1.248745 [  0.407212,   1.904446]\n",
      "cash_bias                :   0.156590 [  0.082655,   0.287145]\n",
      "max_drawdown             :  -0.378910 [ -2.285048,   0.000000]\n",
      "mean_market_return       :   1.447858 [  0.958901,   2.005738]\n",
      "sharpe                   :   0.104191 [ -0.685377,   0.620068]\n",
      "\n",
      "7 episodes - episode_reward: 0.000 [-0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.146 - rate_of_return: 0.000 - cost: 0.000 - steps: 717.980 - cash_bias: 0.200 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "Step 43200: saving model to outputs/agent_portfolio-ddpg-keras/2017-07-21__weights.h5f\n",
      "   44640/2000000.0: episode: 31, duration: 45.267s, episode steps: 1440, steps per second: 32, episode reward: -0.000, mean reward: -0.000 [-0.000, 0.000], mean action: 0.162 [-0.382, 1.237], mean observation: 1.002 [0.714, 1.329], loss: 0.000000, mean_absolute_error: 0.000056, mean_q: 0.000000\n",
      "accumulated_portfolio_val:   1.219054 [  0.498454,   2.219928]\n",
      "cash_bias                :   0.167086 [  0.022215,   0.295571]\n",
      "max_drawdown             :  -0.422660 [ -2.040575,  -0.001453]\n",
      "mean_market_return       :   1.390501 [  0.456720,   2.253033]\n",
      "sharpe                   :   0.087768 [ -0.382995,   0.536017]\n",
      "\n",
      "7 episodes - episode_reward: 0.000 [-0.000, 0.001] - loss: 0.000 - mean_absolute_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.175 - rate_of_return: 0.000 - cost: 0.000 - steps: 718.620 - cash_bias: 0.126 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "Step 57600: saving model to outputs/agent_portfolio-ddpg-keras/2017-07-21__weights.h5f\n",
      "   59040/2000000.0: episode: 41, duration: 47.461s, episode steps: 1440, steps per second: 30, episode reward: 0.000, mean reward: 0.000 [-0.000, 0.000], mean action: 0.231 [-0.299, 1.025], mean observation: 1.003 [0.814, 1.204], loss: 0.000000, mean_absolute_error: 0.000052, mean_q: 0.000000\n",
      "accumulated_portfolio_val:   1.463877 [  0.626685,   3.190795]\n",
      "cash_bias                :   0.157259 [  0.046178,   0.282287]\n",
      "max_drawdown             :  -0.227408 [ -0.987281,   0.000000]\n",
      "mean_market_return       :   1.546166 [  0.694661,   2.609289]\n",
      "sharpe                   :   0.260935 [ -0.149933,   1.046400]\n",
      "\n",
      "7 episodes - episode_reward: 0.000 [-0.000, 0.001] - loss: 0.000 - mean_absolute_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.201 - rate_of_return: 0.000 - cost: 0.000 - steps: 719.260 - cash_bias: 0.151 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "7 episodes - episode_reward: 0.000 [-0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.060 - rate_of_return: 0.000 - cost: 0.000 - steps: 719.900 - cash_bias: 0.158 - mean_market_returns: 1.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "Step 72000: saving model to outputs/agent_portfolio-ddpg-keras/2017-07-21__weights.h5f\n",
      "   73440/2000000.0: episode: 51, duration: 56.738s, episode steps: 1440, steps per second: 25, episode reward: 0.001, mean reward: 0.000 [-0.000, 0.000], mean action: 0.217 [-0.358, 1.214], mean observation: 1.000 [0.546, 1.568], loss: 0.000000, mean_absolute_error: 0.000051, mean_q: 0.000000\n",
      "accumulated_portfolio_val:   1.325440 [  0.831368,   2.544167]\n",
      "cash_bias                :   0.153516 [  0.031240,   0.262192]\n",
      "max_drawdown             :  -0.105198 [ -0.335216,  -0.006713]\n",
      "mean_market_return       :   1.408127 [  1.013494,   1.890050]\n",
      "sharpe                   :   0.159886 [ -0.188950,   0.521265]\n",
      "\n",
      "done, took 2543.323 seconds\n",
      "done, took 2543.323 seconds\n"
     ]
    }
   ],
   "source": [
    "history = agent.fit(env, \n",
    "                  nb_steps=2e6, \n",
    "                  visualize=False, \n",
    "                  verbose=0,\n",
    "                  callbacks=[\n",
    "                      TrainIntervalLoggerTQDMNotebook(),\n",
    "                      TrainEpisodeLoggerPortfolio(10),\n",
    "                      ModelIntervalCheckpoint(save_path, 10*1440, 1)\n",
    "                    ]\n",
    "                 )\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights(save_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T00:08:54.105Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-23T06:52:22.264835Z",
     "start_time": "2017-07-23T14:52:21.895982+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.save_weights('outputs/agent_portfolio-ddpg-keras/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl'), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-23T06:09:37.798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorforce' from '/media/isisilon/Data/linuxOpt/tensorforce/tensorforce/__init__.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorforce\n",
    "tensorforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-23T06:09:37.800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n"
     ]
    }
   ],
   "source": [
    "# one big test\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n",
    "steps=len(df_test)-window_length-2\n",
    "env_test = PortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=steps, \n",
    "    scale=True, \n",
    "    augment=0.00,\n",
    "    trading_cost=0, # let just overfit first\n",
    "    window_length=window_length,\n",
    ")\n",
    "env_test.seed = 0  \n",
    "agent.test(env_test, nb_episodes=1, visualize=False)\n",
    "\n",
    "df = pd.DataFrame(env_test.infos)\n",
    "df.index=df['index']\n",
    "\n",
    "s=sharpe(df.rate_of_return+1)\n",
    "mdd=MDD(df.rate_of_return+1)\n",
    "mean_market_return=df.mean_market_returns.cumprod().iloc[-1]\n",
    "print('APV (Accumulated portfolio value): \\t{: 2.6f}'.format(df.portfolio_value.iloc[-1]))\n",
    "print('SR (Sharpe ratio):                 \\t{: 2.6f}'.format( s))\n",
    "print('MDD (max drawdown):                \\t{: 2.6%}'.format( mdd))\n",
    "print('MDR (mean_market_return):          \\t{: 2.6f}'.format( mean_market_return))\n",
    "print('')\n",
    "\n",
    "# show one run vs average market performance\n",
    "df.portfolio_value.plot()\n",
    "df.mean_market_returns.cumprod().plot(label='mean market performance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-23T06:09:37.802Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The weights appear to be static, so the model hasn't learnt much :(\n",
    "df = pd.DataFrame(env_test.infos)\n",
    "df.index=df['index']\n",
    "df.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-23T06:09:37.804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets evaluate a few 30 step intervals\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n",
    "env_test = PortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augment=0.00,\n",
    "    trading_cost=0, # let just overfit first\n",
    "    window_length=window_length,\n",
    ")\n",
    "env_test.seed = 0  \n",
    "\n",
    "for i in range(10):\n",
    "    agent.test(env_test, nb_episodes=1, visualize=False)\n",
    "    df = pd.DataFrame(env_test.infos)\n",
    "    s=sharpe(df.rate_of_return)\n",
    "    mdd=MDD(df.rate_of_return+1)\n",
    "    mean_market_return=df.mean_market_returns.cumprod().iloc[-1]\n",
    "    print('APV (Accumulated portfolio value): \\t{: 2.6f}'.format(df.portfolio_value.iloc[-1]))\n",
    "    print('MMR (mean_market_return):          \\t{: 2.6f}'.format(mean_market_return))    \n",
    "    print('SR (Sharpe ratio):                 \\t{: 2.6f}'.format( s))\n",
    "    print('MDD (max drawdown):                \\t{: 2.6%}'.format( mdd))\n",
    "    print('')\n",
    "    df.portfolio_value.plot(label=str(i))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T00:26:16.847387Z",
     "start_time": "2017-07-19T08:26:12.521925+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-23T06:09:37.807Z"
    }
   },
   "outputs": [],
   "source": [
    "# history\n",
    "df_hist = pd.DataFrame(history.history)\n",
    "df_hist\n",
    "df_hist['episodes'] = df_hist.index\n",
    "\n",
    "g = sns.jointplot(x=\"episodes\", y=\"episode_reward\", data=df_hist, kind=\"reg\", size=10)\n",
    "plt.show()\n",
    "\n",
    "# g = sns.jointplot(x=\"episodes\", y=\"rewards\", data=history, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
