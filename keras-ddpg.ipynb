{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/hackthemarket/gym-trading/blob/master/gym_trading/envs/TradingEnv.ipynb\n",
    "\n",
    "TODO:\n",
    "- test data, val data\n",
    "- multiple stocks?\n",
    "- bitcoin data env?\n",
    "    - quandl bter (200 results) bitfinex (26), BCHARTs\n",
    "- finanical metrics e.g. \n",
    "    - http://www.cs.utexas.edu/~ai-lab/pubs/AMEC04-plat.pdf sharpes\n",
    "    - return\n",
    "    - dummy score\n",
    "        - all buy, all hold, all sell\n",
    "        - random etc\n",
    "    - quantopians\n",
    "- add more observational data\n",
    "    - [x] the last few steps - add memmory\n",
    "    - [ ] sentiment? e.g. https://www.quandl.com/data/NS1-FinSentS-Web-News-Sentiment\n",
    "    - [ ] overall stock market e.g. https://www.quandl.com/data/UMICH/SOC4-University-of-Michigan-Consumer-Survey-Index-of-Consumer-Sentiment-Within-Regions\n",
    "- replay https://github.com/matthiasplappert/keras-rl/issues/40\n",
    "- or try openai baseline with tensorflow\n",
    "- model\n",
    "    - cnn\n",
    "    - lstm\n",
    "- unit tests\n",
    "    - env should give poor result with random steps, only buys, only holds\n",
    "    - model should overfit on small amount of data\n",
    "    \n",
    "- [x] pretraining? helps a lot. Lets the keras-rl beat the market by a few percent initially\n",
    " bugs:\n",
    " - [x] seems to be discontinuities causing huge navs e.g. 1e51\n",
    " \n",
    " \n",
    " regression vs classification\n",
    " \n",
    " window length and memory\n",
    " \n",
    " experience replay\n",
    " \n",
    " I used [arXiv:1612.01277](https://arxiv.org/abs/1706.10059) paper a lot for understanding the problem and ideas for model design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T10:14:01.147406Z",
     "start_time": "2017-06-27T18:14:01.142926+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:08:54.941951Z",
     "start_time": "2017-07-17T08:08:54.114402+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:__main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# numeric\n",
    "import quandl\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:09:14.440247Z",
     "start_time": "2017-07-17T08:08:54.943813+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# reinforcement learning\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization, Conv1D, InputLayer, Dropout, regularizers, Conv2D, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T07:24:50.874995Z",
     "start_time": "2017-07-12T15:24:50.872400+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:09:14.486513Z",
     "start_time": "2017-07-17T08:09:14.441600+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from src.callbacks.rl_callbacks import ReduceLROnPlateau, TrainIntervalLoggerTQDMNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "Day trading over 256 days. We scale and augument the training data.\n",
    "\n",
    "You can see the base environment class [here](https://github.com/openai/gym/blob/master/gym/core.py#L13) and openai's nice docs [here](https://gym.openai.com/docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:09:14.518970Z",
     "start_time": "2017-07-17T08:09:14.488293+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:09:14.711735Z",
     "start_time": "2017-07-17T08:09:14.520470+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 50, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_hdf('./data/poliniex_30m.hf',key='train')\n",
    "env = PortfolioEnv(\n",
    "    df=df_train,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.0005,\n",
    "    trading_cost=0 # let just overfit first,\n",
    "    window_length = window_length,\n",
    "    \n",
    ")\n",
    "env.seed = 0   \n",
    "\n",
    "df_test = pd.read_hdf('./data/poliniex_30m.hf',key='test')\n",
    "env_test = PortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.00,\n",
    "    trading_cost=0, # let just overfit first\n",
    "    window_length=window_length,\n",
    ")\n",
    "env_test.seed = 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-04T01:42:37.345932Z",
     "start_time": "2017-07-04T09:42:37.328860+08:00"
    },
    "collapsed": true
   },
   "source": [
    "# Model\n",
    "\n",
    "arXiv:1612.01277 indicated that CNN's are just as effective. That's great because I like them, they are fast so I can try more things and see the results faster. So we will be using a CNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:09:15.085019Z",
     "start_time": "2017-07-17T08:09:14.908341+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1, 5, 50, 3)       0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 5, 50, 3)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 5, 48, 2)          20        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 1, 20)          1940      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 1, 1)           21        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 36        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 2,017\n",
      "Trainable params: 2,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "observation_input (InputLayer)   (None, 1, 5, 50, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "action_input (InputLayer)        (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 750)           0           observation_input[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 756)           0           action_input[0][0]               \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 8)             6056        concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 8)             0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 8)             72          activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 8)             0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             9           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 1)             0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 6,137\n",
      "Trainable params: 6,137\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, merge, Reshape\n",
    "from keras.layers import concatenate, Conv2D\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.models import Model\n",
    "\n",
    "window_length=50\n",
    "nb_actions=env.action_space.shape[0]\n",
    "reg=1e-8\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(InputLayer(input_shape=(1,)+env.observation_space.shape))\n",
    "actor.add(Reshape(env.observation_space.shape))\n",
    "actor.add(Conv2D(\n",
    "    filters=2,\n",
    "    kernel_size=(1,3),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(Conv2D(\n",
    "    filters=20,\n",
    "    kernel_size=(1,window_length-2),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(Conv2D(\n",
    "    filters=1,\n",
    "    kernel_size=(1,1),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(Flatten())\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('softmax'))\n",
    "print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,)+env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = concatenate([action_input, flattened_observation])\n",
    "x = Dense(8)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(8)(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = Dense(16)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T00:09:15.526948Z",
     "start_time": "2017-07-17T08:09:15.086494+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rl.agents.ddpg.DDPGAgent at 0x7f2ac55ac5f8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.agents.ddpg import DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(\n",
    "    size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "agent = DDPGAgent(\n",
    "    nb_actions=nb_actions,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    critic_action_input=action_input,\n",
    "    random_process=random_process,\n",
    "    memory=memory,\n",
    "    batch_size=50,\n",
    "    nb_steps_warmup_critic=100,\n",
    "    nb_steps_warmup_actor=100,    \n",
    "    gamma=.00, # discounted factor of zero as per paper\n",
    "    target_model_update=1e-3\n",
    ")\n",
    "agent.compile(Adam(lr=3e-5), metrics=['mse'])\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T00:08:54.101Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.callbacks.keras_rl_callbacks import TrainIntervalLoggerTQDMNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T00:08:54.103Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000.0 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 176s - reward: -7.0683e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 231s - reward: -8.9392e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.994 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 217s - reward: -8.0063e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 227s - reward: -7.3850e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.005, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 219s - reward: -6.7791e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 219s - reward: -7.9060e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.994 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 222s - reward: -8.0577e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.005, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.994 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 226s - reward: -8.1895e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.009, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.994 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 246s - reward: -7.1972e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 234s - reward: -7.3881e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.006, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 236s - reward: -1.0817e-05   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.993 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 237s - reward: -1.0554e-05    ETA: 0s - r\n",
      "334 episodes - episode_reward: -0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.994 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 238s - reward: -6.9517e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.007, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 234s - reward: -1.0992e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.998 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 234s - reward: -7.8206e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.005, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 235s - reward: -6.5965e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 234s - reward: -8.2568e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.994 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 243s - reward: -4.8032e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 188s - reward: -1.1523e-05   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.993 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 173s - reward: -8.3636e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 192s - reward: -8.3188e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: -5.1575e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 23 (220000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 160s - reward: -8.0883e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 171s - reward: -9.1139e-06    E\n",
      "334 episodes - episode_reward: -0.000 [-0.006, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.994 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 168s - reward: -1.1566e-05   \n",
      "333 episodes - episode_reward: -0.000 [-0.005, 0.002] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.992 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 165s - reward: -3.7778e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.997 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 164s - reward: -1.0160e-05   \n",
      "334 episodes - episode_reward: -0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.993 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: -2.7222e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.997 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 152s - reward: -4.7582e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: -5.3166e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 163s - reward: -6.4844e-06    ETA: 0s - reward: -\n",
      "333 episodes - episode_reward: -0.000 [-0.005, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.994 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 178s - reward: -7.3514e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 166s - reward: -1.0011e-05   \n",
      "334 episodes - episode_reward: -0.000 [-0.010, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.993 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 165s - reward: -6.9862e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 172s - reward: -5.0892e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.009, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 163s - reward: -7.5050e-06    ETA: 0s - reward: -7.5195e-\n",
      "334 episodes - episode_reward: -0.000 [-0.008, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 167s - reward: -8.7591e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.002] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.994 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: -6.3906e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: -8.5219e-06    ETA: 0s - reward:\n",
      "334 episodes - episode_reward: -0.000 [-0.008, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.994 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 155s - reward: -1.0625e-05   \n",
      "333 episodes - episode_reward: -0.000 [-0.005, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.993 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: -6.0778e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 156s - reward: -6.0950e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 174s - reward: -3.7554e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 162s - reward: -5.8797e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 45 (440000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6464/10000 [==================>...........] - ETA: 66s - reward: -1.6105e-06"
     ]
    }
   ],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "history = agent.fit(env, \n",
    "                  nb_steps=2e6, \n",
    "                  visualize=False, \n",
    "                  verbose=1,\n",
    "                  callbacks=[\n",
    "                      TrainIntervalLoggerTQDMNotebook(),\n",
    "#                       ReduceLROnPlateau(monitor='episode_reward', patience = 150)\n",
    "                    ]\n",
    "                 )\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights('outputs/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T00:08:54.105Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T00:08:54.107Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.save_weights('outputs/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl'), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T00:08:54.108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(env_test, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T00:08:54.110Z"
    }
   },
   "outputs": [],
   "source": [
    "# history\n",
    "df_hist = pd.DataFrame(history.history)\n",
    "df_hist\n",
    "df_hist['episodes'] = df_hist.index\n",
    "\n",
    "g = sns.jointplot(x=\"episodes\", y=\"episode_reward\", data=df_hist, kind=\"reg\", size=10)\n",
    "plt.show()\n",
    "\n",
    "# g = sns.jointplot(x=\"episodes\", y=\"rewards\", data=history, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_test.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-01T03:02:19.820742Z",
     "start_time": "2017-07-01T11:02:19.740692+08:00"
    }
   },
   "source": [
    "# dummy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T05:43:10.780067Z",
     "start_time": "2017-07-12T05:42:52.587Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T00:08:54.113Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_flat = X_train.reshape((len(X_train),-1))\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_flat, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "def test_env(env, model, memory):\n",
    "    obs = env.reset()\n",
    "    state = memory.get_recent_state(obs)\n",
    "    for t in range(env.days):\n",
    "        x_batch = np.array([state])\n",
    "        x_flat = x_batch.reshape((len(x_batch),-1))\n",
    "        x_flat[np.isnan(x_flat)]=0\n",
    "        y_pred = model.predict(x_flat)\n",
    "        action = y_pred.argmax(1)\n",
    "        obs, rew, done, info = env.step(action[0])\n",
    "        state = memory.get_recent_state(obs)\n",
    "    \n",
    "    df_test = env.sim.to_df()\n",
    "    end = df_test.iloc[-1]\n",
    "    gain = end.bod_nav - end.mkt_nav    \n",
    "    return gain\n",
    "\n",
    "dummy_scores = []\n",
    "for strategy in ['most_frequent', 'uniform', 'prior', 'stratified']:\n",
    "    memory = Memory(window_length=window_length)\n",
    "    clf = DummyClassifier(strategy=strategy)\n",
    "    clf.fit(X_train, y_train)\n",
    "    gain = test_env(env_test, clf, memory)\n",
    "    df=env_test.sim.to_df()\n",
    "    print('{:20.20s}: {: 3.2%} /day NAV gain above market'.format(strategy, (df.mkt_nav-df.bod_nav).mean()))\n",
    "    \n",
    "    plot_env(env_test, title=strategy)  \n",
    "\n",
    "for strategy in ['mean', 'median']:\n",
    "    memory = Memory(window_length=window_length)\n",
    "    clf=DummyRegressor(strategy=strategy)\n",
    "    clf.fit(X_train, y_train)\n",
    "    gain = test_env(env_test, clf, memory)\n",
    "    df=env_test.sim.to_df()\n",
    "    print('{:20.20s}: {: 3.2%} /day NAV gain above market'.format(strategy, (df.mkt_nav-df.bod_nav).mean()))\n",
    "    \n",
    "    plot_env(env_test, title=strategy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T00:08:54.114Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_env(env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
