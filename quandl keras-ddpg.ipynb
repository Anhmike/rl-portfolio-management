{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/hackthemarket/gym-trading/blob/master/gym_trading/envs/TradingEnv.ipynb\n",
    "\n",
    "TODO:\n",
    "- test data, val data\n",
    "- multiple stocks?\n",
    "- bitcoin data env?\n",
    "    - quandl bter (200 results) bitfinex (26), BCHARTs\n",
    "- finanical metrics e.g. \n",
    "    - http://www.cs.utexas.edu/~ai-lab/pubs/AMEC04-plat.pdf sharpes\n",
    "    - return\n",
    "    - dummy score\n",
    "        - all buy, all hold, all sell\n",
    "        - random etc\n",
    "    - quantopians\n",
    "- add more observational data\n",
    "    - [x] the last few steps - add memmory\n",
    "    - [ ] sentiment? e.g. https://www.quandl.com/data/NS1-FinSentS-Web-News-Sentiment\n",
    "    - [ ] overall stock market e.g. https://www.quandl.com/data/UMICH/SOC4-University-of-Michigan-Consumer-Survey-Index-of-Consumer-Sentiment-Within-Regions\n",
    "- replay https://github.com/matthiasplappert/keras-rl/issues/40\n",
    "- or try openai baseline with tensorflow\n",
    "- model\n",
    "    - cnn\n",
    "    - lstm\n",
    "- unit tests\n",
    "    - env should give poor result with random steps, only buys, only holds\n",
    "    - model should overfit on small amount of data\n",
    "    \n",
    "- [x] pretraining? helps a lot. Lets the keras-rl beat the market by a few percent initially\n",
    " bugs:\n",
    " - [x] seems to be discontinuities causing huge navs e.g. 1e51\n",
    " \n",
    " \n",
    " regression vs classification\n",
    " \n",
    " window length and memory\n",
    " \n",
    " experience replay\n",
    " \n",
    " I used [arXiv:1612.01277](https://arxiv.org/abs/1706.10059) paper a lot for understanding the problem and ideas for model design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T10:14:01.147406Z",
     "start_time": "2017-06-27T18:14:01.142926+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:00.940927Z",
     "start_time": "2017-07-16T12:53:00.169609+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:__main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# numeric\n",
    "import quandl\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:16.465087Z",
     "start_time": "2017-07-16T12:53:00.942656+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# reinforcement learning\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization, Conv1D, InputLayer, Dropout, regularizers, Conv2D, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T07:24:50.874995Z",
     "start_time": "2017-07-12T15:24:50.872400+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:16.505024Z",
     "start_time": "2017-07-16T12:53:16.467520+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from src.callbacks.rl_callbacks import ReduceLROnPlateau, TrainIntervalLoggerTQDMNotebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "Day trading over 256 days. We scale and augument the training data.\n",
    "\n",
    "You can see the base environment class [here](https://github.com/openai/gym/blob/master/gym/core.py#L13) and openai's nice docs [here](https://gym.openai.com/docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:16.537183Z",
     "start_time": "2017-07-16T12:53:16.507358+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:16.758826Z",
     "start_time": "2017-07-16T12:53:16.539146+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 50, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_hdf('./data/poliniex_30m.hf',key='train')\n",
    "env = PortfolioEnv(\n",
    "    df=df_train,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.0005    \n",
    ")\n",
    "env.seed = 0   \n",
    "\n",
    "df_test = pd.read_hdf('./data/poliniex_30m.hf',key='test')\n",
    "env_test = PortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.00)\n",
    "env_test.seed = 0  \n",
    "\n",
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T23:42:52.471117Z",
     "start_time": "2017-07-16T07:42:06.356455+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T01:21:05.703397Z",
     "start_time": "2017-07-12T09:21:05.644167+08:00"
    }
   },
   "source": [
    "## SELU?\n",
    "\n",
    "I tried SELU but it didn't help, It's mean to replace batchnorm and ELU with less parameters\n",
    "there have been varied reports for it [reddit discussion]( https://www.reddit.com/r/MachineLearning/comments/6g5tg1/r_selfnormalizing_neural_networks_improved_elu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:16.788853Z",
     "start_time": "2017-07-16T12:53:16.760209+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "# from keras import backend as K\n",
    "# def selu(x):\n",
    "#     \"\"\"Scaled Exponential Linear Unit. (Klambauer et al., 2017)\n",
    "#     # Arguments\n",
    "#         x: A tensor or variable to compute the activation function for.\n",
    "#     # References\n",
    "#         - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n",
    "#     \"\"\"\n",
    "#     alpha = 1.6732632423543772848170429916717\n",
    "#     scale = 1.0507009873554804934193349852946\n",
    "#     return scale * K.elu(x, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-04T01:42:37.345932Z",
     "start_time": "2017-07-04T09:42:37.328860+08:00"
    },
    "collapsed": true
   },
   "source": [
    "# Model\n",
    "\n",
    "arXiv:1612.01277 indicated that CNN's are just as effective. That's great because I like them, they are fast so I can try more things and see the results faster. So we will be using a CNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain the Q model as a normal classification problem\n",
    "\n",
    "We can pretrain on a regular (non-rl) classification problem. This might not be as elegant as end-to-end training but it helps with speed. \n",
    "\n",
    "It also helps me quickly test how a model fit's the data (can it overfit, how much does it generalize?). So it's a good sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:16.827097Z",
     "start_time": "2017-07-16T12:53:16.791596+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# augument the data to compensate for the low quantity\n",
    "def random_shift(x, fraction):\n",
    "    min_x, max_x = np.min(x), np.max(x)\n",
    "    m = np.random.uniform(-fraction, fraction, size=x.shape) + 1\n",
    "    c = np.random.uniform(-fraction, fraction, size=x.shape) * x.std()\n",
    "    return np.clip(x * m + c, min_x, max_x)\n",
    "\n",
    "def X_shift(X, fraction):\n",
    "    X = X.copy()\n",
    "    for i in range(X.shape[1]):\n",
    "        x = X[:,:,i]\n",
    "        X[:,:,i] = random_shift(x, fraction)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:16.858223Z",
     "start_time": "2017-07-16T12:53:16.828950+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTCBTC', 'LTCBTC', 'DOGEBTC', 'DASHBTC', 'XMRBTC', 'XRPBTC']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50 times, 8 price values (open, close, volume...), 6 assets 42x6x8 BUT we want 50x6x8\n",
    "# W, H, C 11x11x3\n",
    "# Conv2D?\n",
    "env.action_space.shape\n",
    "env.src.asset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:16.907065Z",
     "start_time": "2017-07-16T12:53:16.859499+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 50, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:16.931456Z",
     "start_time": "2017-07-16T12:53:16.908351+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(5, 50, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:49:18.472922Z",
     "start_time": "2017-07-16T12:49:18.436610+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:55:32.479722Z",
     "start_time": "2017-07-16T12:55:32.352724+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1, 5, 50, 3)       0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 5, 50, 3)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 5, 48, 2)          20        \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 5, 1, 20)          1940      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 5, 1, 1)           21        \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 12)                72        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 2,053\n",
      "Trainable params: 2,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "observation_input (InputLayer)   (None, 1, 5, 50, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "action_input (InputLayer)        (None, 12)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 750)           0           observation_input[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 762)           0           action_input[0][0]               \n",
      "                                                                   flatten_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 32)            24416       concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 32)            0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 32)            1056        activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 32)            0           dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 1)             33          activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 1)             0           dense_12[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 25,505\n",
      "Trainable params: 25,505\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, merge, Reshape\n",
    "from keras.layers import concatenate, Conv2D\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.models import Model\n",
    "\n",
    "window_length=50\n",
    "nb_actions=env.action_space.shape[0]\n",
    "reg=1e-8\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(InputLayer(input_shape=(1,)+env.observation_space.shape))\n",
    "actor.add(Reshape(env.observation_space.shape))\n",
    "actor.add(Conv2D(\n",
    "    filters=2,\n",
    "    kernel_size=(1,3),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(Conv2D(\n",
    "    filters=20,\n",
    "    kernel_size=(1,window_length-2),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(Conv2D(\n",
    "    filters=1,\n",
    "    kernel_size=(1,1),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(Flatten())\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('softmax'))\n",
    "print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,)+env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = concatenate([action_input, flattened_observation])\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32)(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:55:33.517341Z",
     "start_time": "2017-07-16T12:55:32.888738+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rl.agents.ddpg.DDPGAgent at 0x7f5ef8f77f98>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.agents.ddpg import DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "\n",
    "memory = SequentialMemory(limit=10, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(\n",
    "    size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "agent = DDPGAgent(\n",
    "    nb_actions=nb_actions,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    critic_action_input=action_input,\n",
    "    random_process=random_process,\n",
    "    memory=memory,\n",
    "    batch_size=50,\n",
    "    nb_steps_warmup_critic=100,\n",
    "    nb_steps_warmup_actor=100,    \n",
    "    gamma=.00, # discounted factor of zero as per paper\n",
    "    target_model_update=1e-3\n",
    ")\n",
    "agent.compile(Adam(lr=3e-5), metrics=['mse'])\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:55:34.189644Z",
     "start_time": "2017-07-16T12:55:34.167791+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.callbacks.keras_rl_callbacks import TrainIntervalLoggerTQDMNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:55:36.548463Z",
     "start_time": "2017-07-16T12:55:34.384251+08:00"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000.0 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  101/10000 [..............................] - ETA: 94s - reward: -0.0016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isisilon/.pyenv/versions/3.6.0/envs/jupyter3/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  146/10000 [..............................] - ETA: 141s - reward: 7.0453e-04"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "really? check this",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7e40a0a9ade2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                   \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                   callbacks=[\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#                       TrainIntervalLoggerTQDMNotebook(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#                       ReduceLROnPlateau(monitor='episode_reward', patience = 150)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isisilon/.pyenv/versions/3.6.0/envs/jupyter3/lib/python3.6/site-packages/rl/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_repetition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isisilon/.pyenv/versions/3.6.0/envs/jupyter3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/rl_keras_finance/portfolio-rl-jiang_2017/src/environments/portfolio.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# relative price vector (open/close)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversion_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# remove cash columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/rl_keras_finance/portfolio-rl-jiang_2017/src/environments/portfolio.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, w1, y1, c1)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"really? check this\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;31m# print(dict(mu1=mu1,p1=p1,dw1=dw1,y1=y1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: really? check this"
     ]
    }
   ],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "history = agent.fit(env, \n",
    "                  nb_steps=2e6, \n",
    "                  visualize=False, \n",
    "                  verbose=1,\n",
    "                  callbacks=[\n",
    "#                       TrainIntervalLoggerTQDMNotebook(),\n",
    "#                       ReduceLROnPlateau(monitor='episode_reward', patience = 150)\n",
    "                    ]\n",
    "                 )\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights('outputs/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:51.480461Z",
     "start_time": "2017-07-16T12:53:51.371209+08:00"
    }
   },
   "outputs": [],
   "source": [
    "agent.save_weights('outputs/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:17.758279Z",
     "start_time": "2017-07-16T04:53:00.172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(env_test, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:17.759179Z",
     "start_time": "2017-07-16T04:53:00.173Z"
    }
   },
   "outputs": [],
   "source": [
    "# history\n",
    "df_hist = pd.DataFrame(history.history)\n",
    "df_hist\n",
    "df_hist['episodes'] = df_hist.index\n",
    "\n",
    "g = sns.jointplot(x=\"episodes\", y=\"episode_reward\", data=df_hist, kind=\"reg\", size=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# g = sns.jointplot(x=\"episodes\", y=\"rewards\", data=history, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualise\n",
    "\n",
    "ideally a price with colored actions? like https://hackernoon.com/the-self-learning-quant-d3329fcc9915"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-01T03:02:19.820742Z",
     "start_time": "2017-07-01T11:02:19.740692+08:00"
    }
   },
   "source": [
    "# dummy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T05:43:10.780067Z",
     "start_time": "2017-07-12T05:42:52.587Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:17.760206Z",
     "start_time": "2017-07-16T04:53:00.175Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_flat = X_train.reshape((len(X_train),-1))\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_flat, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "def test_env(env, model, memory):\n",
    "    obs = env.reset()\n",
    "    state = memory.get_recent_state(obs)\n",
    "    for t in range(env.days):\n",
    "        x_batch = np.array([state])\n",
    "        x_flat = x_batch.reshape((len(x_batch),-1))\n",
    "        x_flat[np.isnan(x_flat)]=0\n",
    "        y_pred = model.predict(x_flat)\n",
    "        action = y_pred.argmax(1)\n",
    "        obs, rew, done, info = env.step(action[0])\n",
    "        state = memory.get_recent_state(obs)\n",
    "    \n",
    "    df_test = env.sim.to_df()\n",
    "    end = df_test.iloc[-1]\n",
    "    gain = end.bod_nav - end.mkt_nav    \n",
    "    return gain\n",
    "\n",
    "dummy_scores = []\n",
    "for strategy in ['most_frequent', 'uniform', 'prior', 'stratified']:\n",
    "    memory = Memory(window_length=window_length)\n",
    "    clf = DummyClassifier(strategy=strategy)\n",
    "    clf.fit(X_train, y_train)\n",
    "    gain = test_env(env_test, clf, memory)\n",
    "    df=env_test.sim.to_df()\n",
    "    print('{:20.20s}: {: 3.2%} /day NAV gain above market'.format(strategy, (df.mkt_nav-df.bod_nav).mean()))\n",
    "    \n",
    "    plot_env(env_test, title=strategy)  \n",
    "\n",
    "for strategy in ['mean', 'median']:\n",
    "    memory = Memory(window_length=window_length)\n",
    "    clf=DummyRegressor(strategy=strategy)\n",
    "    clf.fit(X_train, y_train)\n",
    "    gain = test_env(env_test, clf, memory)\n",
    "    df=env_test.sim.to_df()\n",
    "    print('{:20.20s}: {: 3.2%} /day NAV gain above market'.format(strategy, (df.mkt_nav-df.bod_nav).mean()))\n",
    "    \n",
    "    plot_env(env_test, title=strategy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:53:17.761019Z",
     "start_time": "2017-07-16T04:53:00.177Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_env(env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
