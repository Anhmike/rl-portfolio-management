{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/hackthemarket/gym-trading/blob/master/gym_trading/envs/TradingEnv.ipynb\n",
    "\n",
    "TODO:\n",
    "- test data, val data\n",
    "- multiple stocks?\n",
    "- bitcoin data env?\n",
    "    - quandl bter (200 results) bitfinex (26), BCHARTs\n",
    "- finanical metrics e.g. \n",
    "    - http://www.cs.utexas.edu/~ai-lab/pubs/AMEC04-plat.pdf sharpes\n",
    "    - return\n",
    "    - dummy score\n",
    "        - all buy, all hold, all sell\n",
    "        - random etc\n",
    "    - quantopians\n",
    "- add more observational data\n",
    "    - [x] the last few steps - add memmory\n",
    "    - [ ] sentiment? e.g. https://www.quandl.com/data/NS1-FinSentS-Web-News-Sentiment\n",
    "    - [ ] overall stock market e.g. https://www.quandl.com/data/UMICH/SOC4-University-of-Michigan-Consumer-Survey-Index-of-Consumer-Sentiment-Within-Regions\n",
    "- replay https://github.com/matthiasplappert/keras-rl/issues/40\n",
    "- or try openai baseline with tensorflow\n",
    "- model\n",
    "    - cnn\n",
    "    - lstm\n",
    "- unit tests\n",
    "    - env should give poor result with random steps, only buys, only holds\n",
    "    - model should overfit on small amount of data\n",
    "    \n",
    "- [x] pretraining? helps a lot. Lets the keras-rl beat the market by a few percent initially\n",
    " bugs:\n",
    " - [x] seems to be discontinuities causing huge navs e.g. 1e51\n",
    " \n",
    " \n",
    " regression vs classification\n",
    " \n",
    " window length and memory\n",
    " \n",
    " experience replay\n",
    " \n",
    " I used [arXiv:1612.01277](https://arxiv.org/abs/1706.10059) paper a lot for understanding the problem and ideas for model design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T10:14:01.147406Z",
     "start_time": "2017-06-27T18:14:01.142926+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:08:17.809881Z",
     "start_time": "2017-07-16T08:08:16.468582+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:__main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# numeric\n",
    "import quandl\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:08:34.671705Z",
     "start_time": "2017-07-16T08:08:17.811738+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# reinforcement learning\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization, Conv1D, InputLayer, Dropout, regularizers, Conv2D, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T07:24:50.874995Z",
     "start_time": "2017-07-12T15:24:50.872400+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:08:34.700976Z",
     "start_time": "2017-07-16T08:08:34.673530+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from src.callbacks.rl_callbacks import ReduceLROnPlateau, TrainIntervalLoggerTQDMNotebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "Day trading over 256 days. We scale and augument the training data.\n",
    "\n",
    "You can see the base environment class [here](https://github.com/openai/gym/blob/master/gym/core.py#L13) and openai's nice docs [here](https://gym.openai.com/docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:08:34.747138Z",
     "start_time": "2017-07-16T08:08:34.702956+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:10:07.785345Z",
     "start_time": "2017-07-16T08:10:07.288582+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_hdf('./data/poliniex_30m.hf',key='train')\n",
    "env = PortfolioEnv(\n",
    "    df=df_train,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.0005    \n",
    ")\n",
    "env.seed = 0   \n",
    "\n",
    "df_test = pd.read_hdf('./data/poliniex_30m.hf',key='test')\n",
    "env_test = PortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.00)\n",
    "env_test.seed = 0  \n",
    "\n",
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T23:42:52.471117Z",
     "start_time": "2017-07-16T07:42:06.356455+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T01:21:05.703397Z",
     "start_time": "2017-07-12T09:21:05.644167+08:00"
    }
   },
   "source": [
    "## SELU?\n",
    "\n",
    "I tried SELU but it didn't help, It's mean to replace batchnorm and ELU with less parameters\n",
    "there have been varied reports for it [reddit discussion]( https://www.reddit.com/r/MachineLearning/comments/6g5tg1/r_selfnormalizing_neural_networks_improved_elu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:10:07.919337Z",
     "start_time": "2017-07-16T08:10:07.896304+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "# from keras import backend as K\n",
    "# def selu(x):\n",
    "#     \"\"\"Scaled Exponential Linear Unit. (Klambauer et al., 2017)\n",
    "#     # Arguments\n",
    "#         x: A tensor or variable to compute the activation function for.\n",
    "#     # References\n",
    "#         - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n",
    "#     \"\"\"\n",
    "#     alpha = 1.6732632423543772848170429916717\n",
    "#     scale = 1.0507009873554804934193349852946\n",
    "#     return scale * K.elu(x, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-04T01:42:37.345932Z",
     "start_time": "2017-07-04T09:42:37.328860+08:00"
    },
    "collapsed": true
   },
   "source": [
    "# Model\n",
    "\n",
    "arXiv:1612.01277 indicated that CNN's are just as effective. That's great because I like them, they are fast so I can try more things and see the results faster. So we will be using a CNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain the Q model as a normal classification problem\n",
    "\n",
    "We can pretrain on a regular (non-rl) classification problem. This might not be as elegant as end-to-end training but it helps with speed. \n",
    "\n",
    "It also helps me quickly test how a model fit's the data (can it overfit, how much does it generalize?). So it's a good sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:10:08.426211Z",
     "start_time": "2017-07-16T08:10:08.384350+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# augument the data to compensate for the low quantity\n",
    "def random_shift(x, fraction):\n",
    "    min_x, max_x = np.min(x), np.max(x)\n",
    "    m = np.random.uniform(-fraction, fraction, size=x.shape) + 1\n",
    "    c = np.random.uniform(-fraction, fraction, size=x.shape) * x.std()\n",
    "    return np.clip(x * m + c, min_x, max_x)\n",
    "\n",
    "def X_shift(X, fraction):\n",
    "    X = X.copy()\n",
    "    for i in range(X.shape[1]):\n",
    "        x = X[:,:,i]\n",
    "        X[:,:,i] = random_shift(x, fraction)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:10:08.556966Z",
     "start_time": "2017-07-16T08:10:08.528275+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTCBTC', 'LTCBTC', 'DOGEBTC', 'DASHBTC', 'XMRBTC', 'XRPBTC']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50 times, 8 price values (open, close, volume...), 6 assets 42x6x8 BUT we want 50x6x8\n",
    "# W, H, C 11x11x3\n",
    "# Conv2D?\n",
    "env.action_space.shape\n",
    "env.src.asset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:01:59.232037Z",
     "start_time": "2017-07-16T08:01:59.209831+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:10:09.298968Z",
     "start_time": "2017-07-16T08:10:09.104187+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 50, 6, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 48, 6, 2)          50        \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 6, 20)          1940      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1, 6, 1)           21        \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 2,011\n",
      "Trainable params: 2,011\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "observation_input (InputLayer)   (None, 50, 6, 8)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "action_input (InputLayer)        (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 2400)          0           observation_input[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 2406)          0           action_input[0][0]               \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 8)             19256       concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 8)             0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 8)             72          activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 8)             0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 8)             72          activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 8)             0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             9           activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 1)             0           dense_8[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 19,409\n",
      "Trainable params: 19,409\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, merge, Reshape\n",
    "from keras.layers import concatenate, Conv2D\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.models import Model\n",
    "\n",
    "window_length=50\n",
    "nb_actions=env.action_space.shape[0]\n",
    "reg=1e-8\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(InputLayer(input_shape=(window_length,) + env.observation_space.shape))\n",
    "actor.add(Conv2D(\n",
    "    filters=2,\n",
    "    kernel_size=(3,1),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(Conv2D(\n",
    "    filters=20,\n",
    "    kernel_size=(window_length-2,1),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(Conv2D(\n",
    "    filters=1,\n",
    "    kernel_size=(1,1),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(Reshape((nb_actions,)))\n",
    "actor.add(Activation('softmax'))\n",
    "print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(window_length,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = concatenate([action_input, flattened_observation])\n",
    "x = Dense(8)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(8)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(8)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:10:09.720965Z",
     "start_time": "2017-07-16T08:10:09.300675+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rl.agents.ddpg.DDPGAgent at 0x7f138ecd67f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.agents.ddpg import DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "\n",
    "memory = SequentialMemory(limit=1000, window_length=window_length)\n",
    "random_process = OrnsteinUhlenbeckProcess(\n",
    "    size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "agent = DDPGAgent(\n",
    "    nb_actions=nb_actions,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    critic_action_input=action_input,\n",
    "    random_process=random_process,\n",
    "    memory=memory,\n",
    "    batch_size=50,\n",
    "    nb_steps_warmup_critic=100,\n",
    "    nb_steps_warmup_actor=100,    \n",
    "    gamma=.01, # discounted factor of zero as per paper\n",
    "    target_model_update=1e-3\n",
    ")\n",
    "agent.compile(Adam(lr=3e-5), metrics=['mse'])\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:10:10.006379Z",
     "start_time": "2017-07-16T08:10:09.982935+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.callbacks.keras_rl_callbacks import TrainIntervalLoggerTQDMNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-16T00:10:10.191Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000.0 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 309s - reward: -1.3780e-05   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.008] - loss: 0.141 - mean_squared_error: 0.283 - mean_q: 0.008 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.992 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.001 - steps: 16.490\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 281s - reward: -9.7597e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.008, 0.061] - loss: 0.711 - mean_squared_error: 1.422 - mean_q: 0.024 - reward: -0.000 - log_return: -0.000 - portfolio_value: 1.005 - returns: 1.001 - rate_of_return: 0.000 - cost: 0.001 - steps: 16.500\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 281s - reward: -4.4605e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.004, 0.082] - loss: 0.238 - mean_squared_error: 0.476 - mean_q: 0.016 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.001 - rate_of_return: 0.001 - cost: 0.001 - steps: 16.510\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 284s - reward: -2.0498e-05   \n",
      "333 episodes - episode_reward: -0.001 [-0.010, 0.013] - loss: 0.612 - mean_squared_error: 1.224 - mean_q: 0.040 - reward: -0.000 - log_return: -0.001 - portfolio_value: 0.989 - returns: 1.000 - rate_of_return: -0.001 - cost: 0.001 - steps: 16.490\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 278s - reward: -2.1581e-05   \n",
      "333 episodes - episode_reward: -0.001 [-0.007, 0.005] - loss: 0.066 - mean_squared_error: 0.132 - mean_q: 0.004 - reward: -0.000 - log_return: -0.001 - portfolio_value: 0.989 - returns: 1.000 - rate_of_return: -0.001 - cost: 0.001 - steps: 16.500\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 284s - reward: -6.7922e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.009, 0.116] - loss: 0.102 - mean_squared_error: 0.203 - mean_q: -0.009 - reward: -0.000 - log_return: -0.000 - portfolio_value: 1.036 - returns: 1.002 - rate_of_return: 0.002 - cost: 0.001 - steps: 16.510\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 289s - reward: -1.7433e-05   \n",
      "333 episodes - episode_reward: -0.001 [-0.005, 0.035] - loss: 0.015 - mean_squared_error: 0.030 - mean_q: 0.000 - reward: -0.000 - log_return: -0.001 - portfolio_value: 0.990 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.001 - steps: 16.490\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 299s - reward: -2.0829e-05   \n",
      "333 episodes - episode_reward: -0.001 [-0.009, 0.023] - loss: 6.207 - mean_squared_error: 12.413 - mean_q: -0.006 - reward: -0.000 - log_return: -0.001 - portfolio_value: 0.991 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.001 - steps: 16.500\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 309s - reward: -4.7420e-06    ETA: 0s - rew\n",
      "334 episodes - episode_reward: -0.000 [-0.006, 0.105] - loss: 0.002 - mean_squared_error: 0.004 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 1.041 - returns: 1.002 - rate_of_return: 0.001 - cost: 0.001 - steps: 16.510\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      " 6325/10000 [=================>............] - ETA: 107s - reward: 1.0631e-06"
     ]
    }
   ],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "history = agent.fit(env, \n",
    "                  nb_steps=2e6, \n",
    "                  visualize=False, \n",
    "                  verbose=1,\n",
    "                  callbacks=[\n",
    "#                       TrainIntervalLoggerTQDMNotebook(),\n",
    "#                       ReduceLROnPlateau(monitor='episode_reward', patience = 150)\n",
    "                    ]\n",
    "                 )\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights('outputs/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:08:49.418091Z",
     "start_time": "2017-07-16T00:08:49.386Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.save_weights('outputs/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:08:35.554551Z",
     "start_time": "2017-07-16T00:08:16.459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(env_test, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:08:35.555355Z",
     "start_time": "2017-07-16T00:08:16.461Z"
    }
   },
   "outputs": [],
   "source": [
    "# history\n",
    "df_hist = pd.DataFrame(history.history)\n",
    "df_hist\n",
    "df_hist['episodes'] = df_hist.index\n",
    "\n",
    "g = sns.jointplot(x=\"episodes\", y=\"episode_reward\", data=df_hist, kind=\"reg\", size=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# g = sns.jointplot(x=\"episodes\", y=\"rewards\", data=history, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualise\n",
    "\n",
    "ideally a price with colored actions? like https://hackernoon.com/the-self-learning-quant-d3329fcc9915"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-01T03:02:19.820742Z",
     "start_time": "2017-07-01T11:02:19.740692+08:00"
    }
   },
   "source": [
    "# dummy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T05:43:10.780067Z",
     "start_time": "2017-07-12T05:42:52.587Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:08:35.556237Z",
     "start_time": "2017-07-16T00:08:16.465Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_flat = X_train.reshape((len(X_train),-1))\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_flat, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "def test_env(env, model, memory):\n",
    "    obs = env.reset()\n",
    "    state = memory.get_recent_state(obs)\n",
    "    for t in range(env.days):\n",
    "        x_batch = np.array([state])\n",
    "        x_flat = x_batch.reshape((len(x_batch),-1))\n",
    "        x_flat[np.isnan(x_flat)]=0\n",
    "        y_pred = model.predict(x_flat)\n",
    "        action = y_pred.argmax(1)\n",
    "        obs, rew, done, info = env.step(action[0])\n",
    "        state = memory.get_recent_state(obs)\n",
    "    \n",
    "    df_test = env.sim.to_df()\n",
    "    end = df_test.iloc[-1]\n",
    "    gain = end.bod_nav - end.mkt_nav    \n",
    "    return gain\n",
    "\n",
    "dummy_scores = []\n",
    "for strategy in ['most_frequent', 'uniform', 'prior', 'stratified']:\n",
    "    memory = Memory(window_length=window_length)\n",
    "    clf = DummyClassifier(strategy=strategy)\n",
    "    clf.fit(X_train, y_train)\n",
    "    gain = test_env(env_test, clf, memory)\n",
    "    df=env_test.sim.to_df()\n",
    "    print('{:20.20s}: {: 3.2%} /day NAV gain above market'.format(strategy, (df.mkt_nav-df.bod_nav).mean()))\n",
    "    \n",
    "    plot_env(env_test, title=strategy)  \n",
    "\n",
    "for strategy in ['mean', 'median']:\n",
    "    memory = Memory(window_length=window_length)\n",
    "    clf=DummyRegressor(strategy=strategy)\n",
    "    clf.fit(X_train, y_train)\n",
    "    gain = test_env(env_test, clf, memory)\n",
    "    df=env_test.sim.to_df()\n",
    "    print('{:20.20s}: {: 3.2%} /day NAV gain above market'.format(strategy, (df.mkt_nav-df.bod_nav).mean()))\n",
    "    \n",
    "    plot_env(env_test, title=strategy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T00:08:35.557050Z",
     "start_time": "2017-07-16T00:08:16.468Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_env(env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
