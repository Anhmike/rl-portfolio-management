{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/hackthemarket/gym-trading/blob/master/gym_trading/envs/TradingEnv.ipynb\n",
    "\n",
    "TODO:\n",
    "- test data, val data\n",
    "- multiple stocks?\n",
    "- bitcoin data env?\n",
    "    - quandl bter (200 results) bitfinex (26), BCHARTs\n",
    "- finanical metrics e.g. \n",
    "    - http://www.cs.utexas.edu/~ai-lab/pubs/AMEC04-plat.pdf sharpes\n",
    "    - return\n",
    "    - dummy score\n",
    "        - all buy, all hold, all sell\n",
    "        - random etc\n",
    "    - quantopians\n",
    "- add more observational data\n",
    "    - [x] the last few steps - add memmory\n",
    "    - [ ] sentiment? e.g. https://www.quandl.com/data/NS1-FinSentS-Web-News-Sentiment\n",
    "    - [ ] overall stock market e.g. https://www.quandl.com/data/UMICH/SOC4-University-of-Michigan-Consumer-Survey-Index-of-Consumer-Sentiment-Within-Regions\n",
    "- replay https://github.com/matthiasplappert/keras-rl/issues/40\n",
    "- or try openai baseline with tensorflow\n",
    "- model\n",
    "    - cnn\n",
    "    - lstm\n",
    "- unit tests\n",
    "    - env should give poor result with random steps, only buys, only holds\n",
    "    - model should overfit on small amount of data\n",
    "    \n",
    "- [x] pretraining? helps a lot. Lets the keras-rl beat the market by a few percent initially\n",
    " bugs:\n",
    " - [x] seems to be discontinuities causing huge navs e.g. 1e51\n",
    " \n",
    " \n",
    " regression vs classification\n",
    " \n",
    " window length and memory\n",
    " \n",
    " experience replay\n",
    " \n",
    " I used [arXiv:1612.01277](https://arxiv.org/abs/1706.10059) paper a lot for understanding the problem and ideas for model design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T10:14:01.147406Z",
     "start_time": "2017-06-27T18:14:01.142926+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:25:11.212039Z",
     "start_time": "2017-07-16T14:25:10.121667+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:__main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# numeric\n",
    "import quandl\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:26:07.214980Z",
     "start_time": "2017-07-16T14:25:11.214003+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# reinforcement learning\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization, Conv1D, InputLayer, Dropout, regularizers, Conv2D, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T07:24:50.874995Z",
     "start_time": "2017-07-12T15:24:50.872400+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:26:07.259473Z",
     "start_time": "2017-07-16T14:26:07.217106+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from src.callbacks.rl_callbacks import ReduceLROnPlateau, TrainIntervalLoggerTQDMNotebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "Day trading over 256 days. We scale and augument the training data.\n",
    "\n",
    "You can see the base environment class [here](https://github.com/openai/gym/blob/master/gym/core.py#L13) and openai's nice docs [here](https://gym.openai.com/docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:26:07.657873Z",
     "start_time": "2017-07-16T14:26:07.261752+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:26:07.918630Z",
     "start_time": "2017-07-16T14:26:07.659221+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 50, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_hdf('./data/poliniex_30m.hf',key='train')\n",
    "env = PortfolioEnv(\n",
    "    df=df_train,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.0005    \n",
    ")\n",
    "env.seed = 0   \n",
    "\n",
    "df_test = pd.read_hdf('./data/poliniex_30m.hf',key='test')\n",
    "env_test = PortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=30, \n",
    "    scale=True, \n",
    "    augument=0.00)\n",
    "env_test.seed = 0  \n",
    "\n",
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T23:42:52.471117Z",
     "start_time": "2017-07-16T07:42:06.356455+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T01:21:05.703397Z",
     "start_time": "2017-07-12T09:21:05.644167+08:00"
    },
    "heading_collapsed": true
   },
   "source": [
    "## SELU?\n",
    "\n",
    "I tried SELU but it didn't help, It's mean to replace batchnorm and ELU with less parameters\n",
    "there have been varied reports for it [reddit discussion]( https://www.reddit.com/r/MachineLearning/comments/6g5tg1/r_selfnormalizing_neural_networks_improved_elu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:26:07.968770Z",
     "start_time": "2017-07-16T14:26:07.921010+08:00"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    " \n",
    "# from keras import backend as K\n",
    "# def selu(x):\n",
    "#     \"\"\"Scaled Exponential Linear Unit. (Klambauer et al., 2017)\n",
    "#     # Arguments\n",
    "#         x: A tensor or variable to compute the activation function for.\n",
    "#     # References\n",
    "#         - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n",
    "#     \"\"\"\n",
    "#     alpha = 1.6732632423543772848170429916717\n",
    "#     scale = 1.0507009873554804934193349852946\n",
    "#     return scale * K.elu(x, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-04T01:42:37.345932Z",
     "start_time": "2017-07-04T09:42:37.328860+08:00"
    },
    "collapsed": true
   },
   "source": [
    "# Model\n",
    "\n",
    "arXiv:1612.01277 indicated that CNN's are just as effective. That's great because I like them, they are fast so I can try more things and see the results faster. So we will be using a CNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain the Q model as a normal classification problem\n",
    "\n",
    "We can pretrain on a regular (non-rl) classification problem. This might not be as elegant as end-to-end training but it helps with speed. \n",
    "\n",
    "It also helps me quickly test how a model fit's the data (can it overfit, how much does it generalize?). So it's a good sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:26:08.016347Z",
     "start_time": "2017-07-16T14:26:07.971303+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# augument the data to compensate for the low quantity\n",
    "def random_shift(x, fraction):\n",
    "    min_x, max_x = np.min(x), np.max(x)\n",
    "    m = np.random.uniform(-fraction, fraction, size=x.shape) + 1\n",
    "    c = np.random.uniform(-fraction, fraction, size=x.shape) * x.std()\n",
    "    return np.clip(x * m + c, min_x, max_x)\n",
    "\n",
    "def X_shift(X, fraction):\n",
    "    X = X.copy()\n",
    "    for i in range(X.shape[1]):\n",
    "        x = X[:,:,i]\n",
    "        X[:,:,i] = random_shift(x, fraction)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:26:08.056064Z",
     "start_time": "2017-07-16T14:26:08.017836+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTCBTC', 'LTCBTC', 'DOGEBTC', 'DASHBTC', 'XMRBTC', 'XRPBTC']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50 times, 8 price values (open, close, volume...), 6 assets 42x6x8 BUT we want 50x6x8\n",
    "# W, H, C 11x11x3\n",
    "# Conv2D?\n",
    "env.action_space.shape\n",
    "env.src.asset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:26:08.134303Z",
     "start_time": "2017-07-16T14:26:08.057607+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 50, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:26:08.178156Z",
     "start_time": "2017-07-16T14:26:08.136562+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(5, 50, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T04:49:18.472922Z",
     "start_time": "2017-07-16T12:49:18.436610+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:26:08.363976Z",
     "start_time": "2017-07-16T14:26:08.179692+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1, 5, 50, 3)       0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 5, 50, 3)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 5, 48, 2)          20        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 1, 20)          1940      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 1, 1)           21        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                72        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 2,053\n",
      "Trainable params: 2,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "observation_input (InputLayer)   (None, 1, 5, 50, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "action_input (InputLayer)        (None, 12)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 750)           0           observation_input[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 762)           0           action_input[0][0]               \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 32)            24416       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 32)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 32)            1056        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 32)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             33          activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 1)             0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 25,505\n",
      "Trainable params: 25,505\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, merge, Reshape\n",
    "from keras.layers import concatenate, Conv2D\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.models import Model\n",
    "\n",
    "window_length=50\n",
    "nb_actions=env.action_space.shape[0]\n",
    "reg=1e-8\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(InputLayer(input_shape=(1,)+env.observation_space.shape))\n",
    "actor.add(Reshape(env.observation_space.shape))\n",
    "actor.add(Conv2D(\n",
    "    filters=2,\n",
    "    kernel_size=(1,3),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(Conv2D(\n",
    "    filters=20,\n",
    "    kernel_size=(1,window_length-2),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(Conv2D(\n",
    "    filters=1,\n",
    "    kernel_size=(1,1),\n",
    "    kernel_regularizer=l2(reg),\n",
    "    activation='relu'\n",
    "))\n",
    "actor.add(Flatten())\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('softmax'))\n",
    "print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,)+env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = concatenate([action_input, flattened_observation])\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32)(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:26:08.856862Z",
     "start_time": "2017-07-16T14:26:08.366665+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rl.agents.ddpg.DDPGAgent at 0x7fdc2932b630>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.agents.ddpg import DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(\n",
    "    size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "agent = DDPGAgent(\n",
    "    nb_actions=nb_actions,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    critic_action_input=action_input,\n",
    "    random_process=random_process,\n",
    "    memory=memory,\n",
    "    batch_size=50,\n",
    "    nb_steps_warmup_critic=100,\n",
    "    nb_steps_warmup_actor=100,    \n",
    "    gamma=.00, # discounted factor of zero as per paper\n",
    "    target_model_update=1e-3\n",
    ")\n",
    "agent.compile(Adam(lr=3e-5), metrics=['mse'])\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:26:08.895044Z",
     "start_time": "2017-07-16T14:26:08.860136+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.callbacks.keras_rl_callbacks import TrainIntervalLoggerTQDMNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T16:36:15.601146Z",
     "start_time": "2017-07-16T15:36:18.507859+08:00"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000.0 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 177s - reward: 4.0869e-07   \n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 175s - reward: -4.5089e-07   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 173s - reward: 1.9286e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.002] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 175s - reward: -1.7918e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 163s - reward: 5.1499e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 163s - reward: 5.1243e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 162s - reward: 4.9605e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 162s - reward: 4.3875e-06   - ETA: 0s - reward: 4 - ETA: 0s - reward: 4 - ETA: 0s\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 165s - reward: 3.2044e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 164s - reward: 1.3268e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 165s - reward: 1.2028e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 170s - reward: 4.3627e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.011] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 170s - reward: 4.5752e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 170s - reward: 5.1540e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 170s - reward: -6.4603e-07   \n",
      "334 episodes - episode_reward: -0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 169s - reward: 9.7210e-07   \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 169s - reward: 3.5301e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 167s - reward: 3.8852e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.009] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 169s - reward: 2.3860e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 168s - reward: 5.0545e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 167s - reward: 6.4183e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.010] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 167s - reward: 1.8735e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 167s - reward: 4.8859e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.008] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 166s - reward: 5.6814e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 167s - reward: 4.4683e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 169s - reward: 4.9743e-06   - ETA: 0s - reward: 4.99\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 169s - reward: 5.5230e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 167s - reward: 9.2066e-07    - ETA: 0s - rewa\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 169s - reward: 3.5620e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 168s - reward: 6.0415e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 168s - reward: 4.3468e-07   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 169s - reward: 4.1225e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 168s - reward: 4.2306e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 166s - reward: 1.7067e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 168s - reward: 6.1223e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 171s - reward: 4.7175e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 167s - reward: 4.2251e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 168s - reward: 4.8611e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 173s - reward: 4.7480e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 179s - reward: 5.8061e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 170s - reward: 4.7135e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 3.0947e-07   - ETA: 0s - \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 163s - reward: 5.6678e-06   - ETA: 1s - r\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 169s - reward: 4.1515e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 201s - reward: 6.8782e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 46 (450000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 180s - reward: 3.8127e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 176s - reward: 4.4647e-06   - ETA: 0s - rewa - ETA: 0s - reward: 4.46\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 165s - reward: -2.7551e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.007, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.998 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 6.5552e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: 3.5778e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 4.8629e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.005, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: 5.7674e-06   - ETA: 0s - rewar\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 163s - reward: -1.2525e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.998 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: 3.9317e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.008] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 3.1673e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 3.0921e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: 4.6790e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 2.9110e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 5.6591e-06   - ETA: 2s - reward: 5. \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 2.5352e-07   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: -9.9199e-07   \n",
      "333 episodes - episode_reward: -0.000 [-0.005, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 3.1078e-06   - ETA: 1s - reward: - ETA: 1s - rew - ETA: 0s  -\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 156s - reward: 1.9735e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.006, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 2.4536e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 2.9550e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 2.3330e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 2.2538e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: -1.3667e-06    ETA: \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.998 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 4.7832e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: -4.1060e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.997 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 156s - reward: 4.5869e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 5.7223e-06   - ETA: 1s - rew - ETA: 0s - reward: 5.6907e- - ETA: 0s - reward: 5 -\n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 156s - reward: 7.9328e-07   - E - ETA: 0s - rewar - ETA: 0s - rewa - E - ETA: 0s - reward: 7.9221e-0\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 156s - reward: 1.9257e-06   - ETA: 1s - reward: 1.4 - ETA: 1s - reward: 1.5 - ETA:  - ETA: 0s -\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 156s - reward: 2.0155e-06   - ETA: 0s - rewa - ETA: 0s - reward: 1.979\n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 4.5200e-08   - ETA: 0s - reward: 2. - ETA: 0s - reward: 1.24\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 156s - reward: 5.7752e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: -1.2593e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.003, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.998 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 6.5922e-07   - ETA: 1s - reward: 7.1746e-0 - ETA: 1s - reward: - ET - ETA: 0s - reward: 6.607 -\n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 5.1871e-06   - ETA: 0 - ETA: 0s - reward:\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 4.5533e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 4.1116e-06   - ETA: 0s - reward: 4.1140e-0 \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.008] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 2.4755e-06   - ETA: 0s - reward: \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.002] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 4.7970e-06   - ETA: 0s - reward: 4.8169e-\n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.009] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 156s - reward: -6.9627e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.005, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 4.4199e-06   - ETA: 0s - reward: 4. - ET\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 156s - reward: 3.4074e-06   - ETA: 1s - reward: \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 3.8027e-06   - ETA: 0s - rew\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 5.6117e-07   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 90 (890000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 156s - reward: 2.9551e-06   - ETA:\n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 156s - reward: 1.3495e-06   - ETA: 2s - reward: 1.1529e-0 - ETA: 2s - - ETA: 2s - rewar\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 156s - reward: 3.8918e-06   - ETA: 0s - reward: - ETA: 0s - reward: 3.8699e-\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 2.0616e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 156s - reward: 2.8610e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 5.7216e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 6.4701e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 1.4667e-06   - ETA: 0s\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 2.6894e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 4.8869e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: -1.1519e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 6.1191e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 3.4438e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 2.9952e-06   - ETA: 0s - reward:  - ETA: 0s - reward: 2.9952e-0\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 6.0606e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 2.4186e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 2.6571e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 4.7722e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 4.3089e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 5.7169e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 4.8230e-06   - ETA: 0s - rewa\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 6.2785e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 157s - reward: 6.8413e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 183s - reward: 5.8823e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 3.4861e-06   - ETA: 0s - reward:\n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 4.2702e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 3.8439e-06   - ETA:   - ETA: 0s - re\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 164s - reward: 4.0707e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 3.3946e-06   - ETA: 0s - reward:\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 4.5945e-06   - ETA: 3s -\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: -1.6978e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.004, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: 5.7420e-06   - ETA: 0s - reward: 5.7461e-\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: 4.3786e-06   - ETA: 0s - reward: 4 - ETA:  - ETA: 0s - reward: 4.3981e- - ETA:\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 6.1447e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 2.2053e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: -7.5306e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.005, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: -7.2231e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.004, 0.002] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: -7.0391e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.007, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: -7.2343e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.005, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: -7.5096e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: 2.8503e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 4.5526e-06   - ETA: 0s  - ETA: 0s - reward: \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 2.9469e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.007, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 3.2736e-06   - ETA: 1s - reward: 3.27\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 158s - reward: 1.2935e-06   - ETA: 0s - rewar\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 3.1961e-06   - ETA: 0s - reward: 3.1968e-\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 159s - reward: 3.3760e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 5.8073e-06   - E - ETA: 0s \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 5.3289e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.001, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 2.4350e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 158s - reward: 3.4758e-06   - ETA: 1s - reward: 3.1502e-0 - ETA:  - ETA: 0s - reward: 3.44 - ETA: - ETA: 0s - reward: \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 157s - reward: 9.1560e-06   - ETA: 1s - re - \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 165s - reward: 1.6841e-06   - ET\n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 1.4338e-07   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 3.3107e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 6.5336e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 7.6777e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 5.9384e-06   - ETA: 2\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 6.1227e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -3.2346e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.998 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 4.4440e-06   - ETA: 0s - reward: \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 5.4449e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 152 (1510000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 4.9466e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 153 (1520000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 3.5472e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 154 (1530000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 3.6523e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 155 (1540000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 5.6425e-06   - ETA\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 156 (1550000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - ETA: 0s - reward: 3.0808e-06- ETA: 0s - ETA: 0s - reward: 3.0806e- - 160s - reward: 3.0654e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 157 (1560000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 4.8715e-06   - ETA: 0s - reward: 4.9\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 158 (1570000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -4.5028e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 159 (1580000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -8.4947e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.005, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 160 (1590000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -7.5831e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.007, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 161 (1600000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -8.3400e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.006, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 162 (1610000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -4.7083e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.997 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 163 (1620000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -4.7490e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.997 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -6.1968e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: -6.9835e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -7.8271e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.007, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.995 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -7.5343e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.996 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -4.1565e-06   \n",
      "334 episodes - episode_reward: -0.000 [-0.006, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.997 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -3.4372e-06   \n",
      "333 episodes - episode_reward: -0.000 [-0.005, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.997 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: -9.3391e-07   \n",
      "333 episodes - episode_reward: -0.000 [-0.006, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: -0.000 - reward: -0.000 - log_return: -0.000 - portfolio_value: 0.999 - returns: 1.000 - rate_of_return: -0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 4.2128e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 2.4032e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 2.2883e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.006, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 3.2143e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 1.3484e-06   - ETA: 0 - ETA: 0s - rewar\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 4.8153e-06   - ETA: 0\n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 6.2889e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 159s - reward: 1.9393e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 5.1054e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.006] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 4.4154e-06   - ETA: 0s - reward:\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 3.0299e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 3.4093e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 8.0742e-06   - ETA:\n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.010] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 3.8177e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 7.3555e-06   - ETA: 0s - reward: 7.3592e-\n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 1.1479e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 162s - reward: 4.3212e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 1.4543e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.005, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 6.4509e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 6.2257e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 4.1650e-06   - ETA: 0s - reward - ETA: 0s - \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 2.2396e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.003, 0.003] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 160s - reward: 2.2445e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.000 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 5.1330e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 3.8687e-06   \n",
      "334 episodes - episode_reward: 0.000 [-0.005, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 3.5137e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.004, 0.005] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 6.1738e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.003, 0.004] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.500\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 5.2702e-06   - ETA: 0s - reward: 5.2568e- - ETA: 0s -\n",
      "334 episodes - episode_reward: 0.000 [-0.004, 0.007] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.001 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.510\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 161s - reward: 8.5593e-06   \n",
      "333 episodes - episode_reward: 0.000 [-0.002, 0.009] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.000 - reward: 0.000 - log_return: 0.000 - portfolio_value: 1.002 - returns: 1.000 - rate_of_return: 0.000 - cost: 0.000 - steps: 16.490\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 161s - reward: 2.8561e-06   - ETA: 0s - rewar\n",
      "done, took 32396.610 seconds\n"
     ]
    }
   ],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "history = agent.fit(env, \n",
    "                  nb_steps=2e6, \n",
    "                  visualize=False, \n",
    "                  verbose=1,\n",
    "                  callbacks=[\n",
    "#                       TrainIntervalLoggerTQDMNotebook(),\n",
    "#                       ReduceLROnPlateau(monitor='episode_reward', patience = 150)\n",
    "                    ]\n",
    "                 )\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights('outputs/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:27:43.780221Z",
     "start_time": "2017-07-16T06:25:10.125Z"
    }
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:27:43.781294Z",
     "start_time": "2017-07-16T06:25:10.127Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.save_weights('outputs/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:27:43.782348Z",
     "start_time": "2017-07-16T06:25:10.129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(env_test, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:27:43.783432Z",
     "start_time": "2017-07-16T06:25:10.131Z"
    }
   },
   "outputs": [],
   "source": [
    "# history\n",
    "df_hist = pd.DataFrame(history.history)\n",
    "df_hist\n",
    "df_hist['episodes'] = df_hist.index\n",
    "\n",
    "g = sns.jointplot(x=\"episodes\", y=\"episode_reward\", data=df_hist, kind=\"reg\", size=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# g = sns.jointplot(x=\"episodes\", y=\"rewards\", data=history, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualise\n",
    "\n",
    "ideally a price with colored actions? like https://hackernoon.com/the-self-learning-quant-d3329fcc9915"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-01T03:02:19.820742Z",
     "start_time": "2017-07-01T11:02:19.740692+08:00"
    }
   },
   "source": [
    "# dummy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T05:43:10.780067Z",
     "start_time": "2017-07-12T05:42:52.587Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:27:43.784482Z",
     "start_time": "2017-07-16T06:25:10.136Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_flat = X_train.reshape((len(X_train),-1))\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_flat, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "def test_env(env, model, memory):\n",
    "    obs = env.reset()\n",
    "    state = memory.get_recent_state(obs)\n",
    "    for t in range(env.days):\n",
    "        x_batch = np.array([state])\n",
    "        x_flat = x_batch.reshape((len(x_batch),-1))\n",
    "        x_flat[np.isnan(x_flat)]=0\n",
    "        y_pred = model.predict(x_flat)\n",
    "        action = y_pred.argmax(1)\n",
    "        obs, rew, done, info = env.step(action[0])\n",
    "        state = memory.get_recent_state(obs)\n",
    "    \n",
    "    df_test = env.sim.to_df()\n",
    "    end = df_test.iloc[-1]\n",
    "    gain = end.bod_nav - end.mkt_nav    \n",
    "    return gain\n",
    "\n",
    "dummy_scores = []\n",
    "for strategy in ['most_frequent', 'uniform', 'prior', 'stratified']:\n",
    "    memory = Memory(window_length=window_length)\n",
    "    clf = DummyClassifier(strategy=strategy)\n",
    "    clf.fit(X_train, y_train)\n",
    "    gain = test_env(env_test, clf, memory)\n",
    "    df=env_test.sim.to_df()\n",
    "    print('{:20.20s}: {: 3.2%} /day NAV gain above market'.format(strategy, (df.mkt_nav-df.bod_nav).mean()))\n",
    "    \n",
    "    plot_env(env_test, title=strategy)  \n",
    "\n",
    "for strategy in ['mean', 'median']:\n",
    "    memory = Memory(window_length=window_length)\n",
    "    clf=DummyRegressor(strategy=strategy)\n",
    "    clf.fit(X_train, y_train)\n",
    "    gain = test_env(env_test, clf, memory)\n",
    "    df=env_test.sim.to_df()\n",
    "    print('{:20.20s}: {: 3.2%} /day NAV gain above market'.format(strategy, (df.mkt_nav-df.bod_nav).mean()))\n",
    "    \n",
    "    plot_env(env_test, title=strategy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T06:27:43.785459Z",
     "start_time": "2017-07-16T06:25:10.139Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_env(env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
