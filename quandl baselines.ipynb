{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This tried to train a reinforcement learning agent to trade bitcoin. This uses openai's baseline library.\n",
    " \n",
    " Data:\n",
    " - daily bitcoin returns\n",
    " - supplmental data\n",
    "     - bitcoin returns\n",
    "     - twitter, reddit sentiment\n",
    "     - gold, steel, S&P500, and more\n",
    " \n",
    "Reinforcement learning features:\n",
    " \n",
    " \n",
    " - memory: the agent remembers the last few states so it can make prediction based on the history\n",
    " - [prioritised experience replay](https://arxiv.org/abs/1511.05952)\n",
    " - [dueling](https://arxiv.org/pdf/1511.06581.pdf)\n",
    " - [double q learning](https://arxiv.org/abs/1509.06461) adjust some situations when the agent overestimates reward. Leads to better performance\n",
    " \n",
    "Machine learning features:\n",
    "\n",
    "- dropout\n",
    "- regularisation\n",
    "- batchnorm\n",
    " \n",
    "\n",
    "References:\n",
    " \n",
    " I used the [\"A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem\"](https://arxiv.org/abs/1706.10059) paper a lot for understanding the problem and ideas for model design.\n",
    " \n",
    " The trading environment is modified from here https://github.com/hackthemarket/gym-trading/blob/master/gym_trading/envs/TradingEnv.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T10:14:01.147406Z",
     "start_time": "2017-06-27T18:14:01.142926+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:36:52.025673Z",
     "start_time": "2017-07-15T14:36:51.268391+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# numeric\n",
    "import quandl\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# util\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import time\n",
    "import tempfile\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:05.238695Z",
     "start_time": "2017-07-15T14:36:52.027687+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rl\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "import baselines.common.tf_util as U\n",
    "from baselines import logger\n",
    "from baselines import deepq\n",
    "from baselines.common.schedules import LinearSchedule, PiecewiseSchedule\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "from baselines.common.misc_util import (\n",
    "    boolean_flag,\n",
    "    pickle_load,\n",
    "    pretty_eta,\n",
    "    relatively_safe_pickle_dump,\n",
    "    set_global_seeds,\n",
    "    RunningAvg,\n",
    "    SimpleMonitor\n",
    ")\n",
    "\n",
    "# ml\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:05.265634Z",
     "start_time": "2017-07-15T14:37:05.240431+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:05.327354Z",
     "start_time": "2017-07-15T14:37:05.267236+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\"DQN experiments for Atari games\")\n",
    "    # Environment\n",
    "    parser.add_argument(\"--env\", type=str, default=\"Pong\", help=\"name of the game\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"which seed to use\")\n",
    "    # Core DQN parameters\n",
    "    parser.add_argument(\"--replay-buffer-size\", type=int, default=int(1e6), help=\"replay buffer size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"learning rate for Adam optimizer\")\n",
    "    parser.add_argument(\"--num-steps\", type=int, default=int(2e8), help=\"total number of steps to run the environment for\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=32, help=\"number of transitions to optimize at the same time\")\n",
    "    parser.add_argument(\"--learning-freq\", type=int, default=4, help=\"number of iterations between every optimization step\")\n",
    "    parser.add_argument(\"--target-update-freq\", type=int, default=40000, help=\"number of iterations between every target network update\")\n",
    "    # Bells and whistles\n",
    "    boolean_flag(parser, \"double-q\", default=True, help=\"whether or not to use double q learning\")\n",
    "    boolean_flag(parser, \"dueling\", default=False, help=\"whether or not to use dueling model\")\n",
    "    boolean_flag(parser, \"prioritized\", default=False, help=\"whether or not to use prioritized replay buffer\")\n",
    "    parser.add_argument(\"--prioritized-alpha\", type=float, default=0.6, help=\"alpha parameter for prioritized replay buffer\")\n",
    "    parser.add_argument(\"--prioritized-beta0\", type=float, default=0.4, help=\"initial value of beta parameters for prioritized replay\")\n",
    "    parser.add_argument(\"--prioritized-eps\", type=float, default=1e-6, help=\"eps parameter for prioritized replay buffer\")\n",
    "    # Checkpointing\n",
    "    parser.add_argument(\"--save-dir\", type=str, default=None, help=\"directory in which training state and model should be saved.\")\n",
    "    parser.add_argument(\"--save-azure-container\", type=str, default=None,\n",
    "                        help=\"It present data will saved/loaded from Azure. Should be in format ACCOUNT_NAME:ACCOUNT_KEY:CONTAINER\")\n",
    "    parser.add_argument(\"--save-freq\", type=int, default=1e6, help=\"save model once every time this many iterations are completed\")\n",
    "    boolean_flag(parser, \"load-on-start\", default=True, help=\"if true and model was previously saved then training will be resumed\")\n",
    "    return parser.parse_args(['--dueling', '--prioritized','--num-steps','900000',\"--save-dir\",\"./models\", '--target-update-freq', '10000'])\n",
    "args = parse_args()\n",
    "args\n",
    "\n",
    "savedir = args.save_dir\n",
    "if args.save_azure_container is not None:\n",
    "    account_name, account_key, container_name = args.save_azure_container.split(\":\")\n",
    "    container = Container(account_name=account_name,\n",
    "                          account_key=account_key,\n",
    "                          container_name=container_name,\n",
    "                          maybe_create=True)\n",
    "    if savedir is None:\n",
    "        # Careful! This will not get cleaned up. Docker spoils the developers.\n",
    "        savedir = tempfile.TemporaryDirectory().name\n",
    "else:\n",
    "    container = None\n",
    "    \n",
    "window_len = 7*8\n",
    "window_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:05.372711Z",
     "start_time": "2017-07-15T14:37:05.328691+08:00"
    },
    "code_folding": [
     2,
     20
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_save_model(savedir, container, state):\n",
    "    \"\"\"This function checkpoints the model and state of the training algorithm.\"\"\"\n",
    "    if savedir is None:\n",
    "        return\n",
    "    start_time = time.time()\n",
    "    model_dir = \"model-{}\".format(state[\"num_iters\"])\n",
    "    U.save_state(os.path.join(savedir, model_dir, \"saved\"))\n",
    "    if container is not None:\n",
    "        container.put(os.path.join(savedir, model_dir), model_dir)\n",
    "    relatively_safe_pickle_dump(state, os.path.join(savedir, 'training_state.pkl.zip'), compression=True)\n",
    "    if container is not None:\n",
    "        container.put(os.path.join(savedir, 'training_state.pkl.zip'), 'training_state.pkl.zip')\n",
    "    relatively_safe_pickle_dump(state[\"monitor_state\"], os.path.join(savedir, 'monitor_state.pkl'))\n",
    "    if container is not None:\n",
    "        container.put(os.path.join(savedir, 'monitor_state.pkl'), 'monitor_state.pkl')\n",
    "    logger.log(\"Saved model in {} seconds\\n\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "def maybe_load_model(savedir, container):\n",
    "    \"\"\"Load model if present at the specified path.\"\"\"\n",
    "    if savedir is None:\n",
    "        return\n",
    "\n",
    "    state_path = os.path.join(os.path.join(savedir, 'training_state.pkl.zip'))\n",
    "    if container is not None:\n",
    "        logger.log(\"Attempting to download model from Azure\")\n",
    "        found_model = container.get(savedir, 'training_state.pkl.zip')\n",
    "    else:\n",
    "        found_model = os.path.exists(state_path)\n",
    "    if found_model:\n",
    "        state = pickle_load(state_path, compression=True)\n",
    "        model_dir = \"model-{}\".format(state[\"num_iters\"])\n",
    "        if container is not None:\n",
    "            container.get(savedir, model_dir)\n",
    "        U.load_state(os.path.join(savedir, model_dir, \"saved\"))\n",
    "        logger.log(\"Loaded models checkpoint at {} iterations\".format(state[\"num_iters\"]))\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T11:25:07.304579Z",
     "start_time": "2017-07-12T19:25:07.252366+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:05.401294Z",
     "start_time": "2017-07-15T14:37:05.374291+08:00"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rl.memory import Memory, SequentialMemory\n",
    "class FlatMemory(Memory):\n",
    "    def get_recent_state(self, current_observation):\n",
    "        \"\"\"\n",
    "        I want a window but modifying baselines internals to handle another\n",
    "        dimension is hard. Lets just flatten instead.\n",
    "        \"\"\"\n",
    "        state = super(FlatMemory, self).get_recent_state(current_observation)\n",
    "        return np.array(state)#.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:05.471725Z",
     "start_time": "2017-07-15T14:37:05.402957+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modified the baselines monitor to record the mean of ALL infos\n",
    "class SimpleMonitor(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"Adds three qunatities to info returned by every step:\n",
    "            num_steps: int\n",
    "                Number of steps takes so far\n",
    "            rewards: [float]\n",
    "                All the cumulative rewards for the episodes completed so far.\n",
    "            infos: [float]\n",
    "                Infos by episode\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        # current episode state\n",
    "        self._current_reward = None\n",
    "        self._current_info = None\n",
    "        self._num_steps = None\n",
    "        # temporary monitor state that we do not save\n",
    "        self._time_offset = None\n",
    "        self._total_steps = None\n",
    "        # monitor state\n",
    "        self._episode_rewards = []\n",
    "        self._episode_infos = []\n",
    "        self._episode_lengths = []\n",
    "        self._episode_end_times = []\n",
    "\n",
    "    def _reset(self):\n",
    "        obs = self.env.reset()\n",
    "        # recompute temporary state if needed\n",
    "        if self._time_offset is None:\n",
    "            self._time_offset = time.time()\n",
    "            if len(self._episode_end_times) > 0:\n",
    "                self._time_offset -= self._episode_end_times[-1]\n",
    "        if self._total_steps is None:\n",
    "            self._total_steps = sum(self._episode_lengths)\n",
    "        # update monitor state\n",
    "        if self._current_reward is not None:\n",
    "            self._episode_rewards.append(self._current_reward)\n",
    "            # FIXME a bit slow to use a dataframe here\n",
    "            self._episode_infos.append(pd.DataFrame(self._current_infos).mean().to_dict())\n",
    "            self._episode_lengths.append(self._num_steps)\n",
    "            self._episode_end_times.append(time.time() - self._time_offset)\n",
    "        # reset episode state\n",
    "        self._current_reward = 0\n",
    "        self._current_infos = []\n",
    "        self._num_steps = 0\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _step(self, action):\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        self._current_reward += rew\n",
    "        self._current_infos += [info]\n",
    "        self._num_steps += 1\n",
    "        self._total_steps += 1\n",
    "        info['steps'] = self._total_steps\n",
    "        info['rewards'] = self._episode_rewards\n",
    "        info['infos'] = self._episode_infos\n",
    "        return (obs, rew, done, info)\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\n",
    "#             'env_id': self.env.unwrapped.spec.id,\n",
    "            'episode_data': {\n",
    "                'episode_rewards': self._episode_rewards,\n",
    "                'episode_infos': self._episode_infos,\n",
    "                'episode_lengths': self._episode_lengths,\n",
    "                'episode_end_times': self._episode_end_times,\n",
    "                'initial_reset_time': 0,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def set_state(self, state):\n",
    "#         assert state['env_id'] == self.env.unwrapped.spec.id\n",
    "        ed = state['episode_data']\n",
    "        self._episode_rewards = ed['episode_rewards']\n",
    "        self._episode_infos = ed['episode_infos']\n",
    "        self._episode_lengths = ed['episode_lengths']\n",
    "        self._episode_end_times = ed['episode_end_times']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-12T08:28:53.526Z"
    }
   },
   "source": [
    "# Environment\n",
    "\n",
    "Day trading over 256 days. We scale and augument the training data.\n",
    "\n",
    "You can see the base environment class [here](https://github.com/openai/gym/blob/master/gym/core.py#L13) and openai's nice docs [here](https://gym.openai.com/docs)\n",
    "\n",
    "Our environment is based on https://github.com/hackthemarket/gym-trading/blob/master/gym_trading/envs/TradingEnv.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:05.499089Z",
     "start_time": "2017-07-15T14:37:05.473565+08:00"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.portfolio import PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:06.097236Z",
     "start_time": "2017-07-15T14:37:05.500464+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(6, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_hdf('./data/poliniex_30m.hf',key='train')\n",
    "env = PortfolioEnv(\n",
    "    df=df_train,\n",
    "    steps=128, \n",
    "    scale=True, \n",
    "    augument=0.0005    \n",
    ")\n",
    "env.seed = 0   \n",
    "monitored_env = SimpleMonitor(env)\n",
    "monitored_env\n",
    "\n",
    "df_test = pd.read_hdf('./data/poliniex_30m.hf',key='test')\n",
    "env_test = PortfolioEnv(\n",
    "    df=df_test,\n",
    "    steps=128, \n",
    "    scale=True, \n",
    "    augument=0.00)\n",
    "env_test.seed = 0   \n",
    "monitored_env_test = SimpleMonitor(env_test)\n",
    "monitored_env_test.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:06.132064Z",
     "start_time": "2017-07-15T14:37:06.099596+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SELU?\n",
    "\n",
    "I tried SELU but it didn't help, It's mean to replace batchnorm and ELU with less parameters\n",
    "there have been varied reports for it [reddit discussion]( https://www.reddit.com/r/MachineLearning/comments/6g5tg1/r_selfnormalizing_neural_networks_improved_elu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:06.169484Z",
     "start_time": "2017-07-15T14:37:06.133961+08:00"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/44621731/how-to-handle-the-batchnorm-layer-when-training-fully-convolutional-networks-by\n",
    "def selu(x, name=\"selu\"):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * tf.where(x >= 0.0, x, alpha * tf.nn.elu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "arXiv:1612.01277 indicated that CNN's are just as effective. That's great because I like them, they are fast so I can try more things and see the results faster. So we will be using a CNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:06.254054Z",
     "start_time": "2017-07-15T14:37:06.171660+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/openai/baselines/blob/master/baselines/deepq/models.py\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "def _cnn_mlp(convs, hiddens, dueling, inpt, num_actions, scope, reuse=False):\n",
    "    \"\"\"CNN=>Dense model using dropout\"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = inpt\n",
    "        with tf.variable_scope(\"convnet\"):\n",
    "            for num_outputs, kernel_size, stride in convs:\n",
    "                out = layers.convolution2d(out,\n",
    "                    num_outputs=num_outputs,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    weights_regularizer=tf.contrib.layers.l2_regularizer(1e-8),\n",
    "                    activation_fn=tf.nn.relu,\n",
    "#                     normalizer_fn=tf.layers.batch_normalization,                    \n",
    "                )\n",
    "                out = layers.dropout(out, 0.3)\n",
    "        out = layers.flatten(out)\n",
    "        with tf.variable_scope(\"action_value\"):\n",
    "            action_out = out\n",
    "            for hidden in hiddens:\n",
    "                action_out = layers.fully_connected(\n",
    "                    action_out, \n",
    "                    num_outputs=hidden, \n",
    "                    activation_fn=tf.nn.relu,\n",
    "                    weights_regularizer=tf.contrib.layers.l2_regularizer(1e-8),\n",
    "#                     normalizer_fn=tf.layers.batch_normalization, \n",
    "                )\n",
    "#                 action_out = tf.layers.batch_normalization(action_out)\n",
    "                action_out = layers.dropout(action_out, 0.3)\n",
    "            action_scores = layers.fully_connected(action_out, num_outputs=num_actions, activation_fn=tf.nn.softmax)\n",
    "        \n",
    "        if dueling:\n",
    "            with tf.variable_scope(\"state_value\"):\n",
    "                state_out = out\n",
    "                for hidden in hiddens:\n",
    "                    state_out = layers.fully_connected(\n",
    "                        state_out, num_outputs=hidden, activation_fn=tf.nn.relu,\n",
    "                        weights_regularizer=tf.contrib.layers.l2_regularizer(1e-8),\n",
    "                        normalizer_fn=tf.layers.batch_normalization,\n",
    "                    )\n",
    "#                     state_out = tf.layers.batch_normalization(state_out)\n",
    "                    state_out = layers.dropout(state_out, 0.3)\n",
    "                state_score = layers.fully_connected(state_out, num_outputs=1, activation_fn=None)\n",
    "            action_scores_mean = tf.reduce_mean(action_scores, 1)\n",
    "            action_scores_centered = action_scores - tf.expand_dims(action_scores_mean, 1)\n",
    "            return state_score + action_scores_centered\n",
    "        else:\n",
    "            return action_scores\n",
    "\n",
    "def cnn_mlp(convs, hiddens, dueling):\n",
    "    \"\"\"Factory to return a model function without input and output dimensions\"\"\"\n",
    "    return lambda *args, **kwargs: _cnn_mlp(convs, hiddens, dueling, *args, **kwargs)\n",
    "\n",
    "model = cnn_mlp(\n",
    "    convs=[(12, 1, 1), (24, window_len, 4), (24, 1, 1)],#, (64, 4, 2), (64, 3, 1)],\n",
    "#     convs=[(32, 8, 1), (64, 8, 4)],#, (64, 4, 2), (64, 3, 1)],\n",
    "    hiddens=[],\n",
    "    dueling=args.dueling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T04:17:47.046198Z",
     "start_time": "2017-07-15T12:17:46.968318+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:06.290878Z",
     "start_time": "2017-07-15T14:37:06.256014+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remeber our last observations: agent memory\n",
    "memory = FlatMemory(window_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:06.369335Z",
     "start_time": "2017-07-15T14:37:06.293096+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepq.build_train?\n",
    "env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:06.406351Z",
     "start_time": "2017-07-15T14:37:06.371242+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.where?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:06.433362Z",
     "start_time": "2017-07-15T14:37:06.408379+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'stack:0' shape=(1,) dtype=int32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random_uniform?\n",
    "tf.stack([11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:06.590053Z",
     "start_time": "2017-07-15T14:37:06.435445+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need custom build_train since we don't want to argmax it\n",
    "import tensorflow as tf\n",
    "import baselines.common.tf_util as U\n",
    "\n",
    "\n",
    "def build_act(make_obs_ph, q_func, num_actions, scope=\"deepq\", reuse=None):\n",
    "    \"\"\"Creates the act function:\n",
    "    Parameters\n",
    "    ----------\n",
    "    make_obs_ph: str -> tf.placeholder or TfInput\n",
    "        a function that take a name and creates a placeholder of input with that name\n",
    "    q_func: (tf.Variable, int, str, bool) -> tf.Variable\n",
    "        the model that takes the following inputs:\n",
    "            observation_in: object\n",
    "                the output of observation placeholder\n",
    "            num_actions: int\n",
    "                number of actions\n",
    "            scope: str\n",
    "            reuse: bool\n",
    "                should be passed to outer variable scope\n",
    "        and returns a tensor of shape (batch_size, num_actions) with values of every action.\n",
    "    num_actions: int\n",
    "        number of actions.\n",
    "    scope: str or VariableScope\n",
    "        optional scope for variable_scope.\n",
    "    reuse: bool or None\n",
    "        whether or not the variables should be reused. To be able to reuse the scope must be given.\n",
    "    Returns\n",
    "    -------\n",
    "    act: (tf.Variable, bool, float) -> tf.Variable\n",
    "        function to select and action given observation.\n",
    "`       See the top of the file for details.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        observations_ph = U.ensure_tf_input(make_obs_ph(\"observation\"))\n",
    "        stochastic_ph = tf.placeholder(tf.bool, (), name=\"stochastic\")\n",
    "        update_eps_ph = tf.placeholder(tf.float32, (), name=\"update_eps\")\n",
    "\n",
    "        eps = tf.get_variable(\"eps\", (), initializer=tf.constant_initializer(0))\n",
    "\n",
    "        q_values = q_func(observations_ph.get(), num_actions, scope=\"q_func\")\n",
    "        deterministic_actions = q_values # tf.argmax(q_values, axis=1)\n",
    "\n",
    "        batch_size = tf.shape(observations_ph.get())[0]\n",
    "        random_actions = tf.random_uniform(tf.stack([batch_size, num_actions]), minval=0, maxval=num_actions, dtype=tf.int64)\n",
    "        chose_random = tf.random_uniform(tf.stack([batch_size, num_actions]), minval=0, maxval=1, dtype=tf.float32) < eps\n",
    "        stochastic_actions = tf.where(chose_random, random_actions, deterministic_actions)\n",
    "        \n",
    "#         import pdb; pdb.set_trace()\n",
    "\n",
    "        # do either random or deterministic actions based on exploration\n",
    "        output_actions = tf.cond(stochastic_ph, lambda: stochastic_actions, lambda: deterministic_actions)\n",
    "        \n",
    "        update_eps_expr = eps.assign(tf.cond(update_eps_ph >= 0, lambda: update_eps_ph, lambda: eps))\n",
    "\n",
    "        act = U.function(inputs=[observations_ph, stochastic_ph, update_eps_ph],\n",
    "                         outputs=output_actions,\n",
    "                         givens={update_eps_ph: -1.0, stochastic_ph: True},\n",
    "                         updates=[update_eps_expr])\n",
    "        return act\n",
    "\n",
    "\n",
    "def build_train(make_obs_ph, q_func, num_actions, optimizer, grad_norm_clipping=None, gamma=1.0, double_q=True, scope=\"deepq\", reuse=None):\n",
    "    \"\"\"Creates the train function:\n",
    "    Parameters\n",
    "    ----------\n",
    "    make_obs_ph: str -> tf.placeholder or TfInput\n",
    "        a function that takes a name and creates a placeholder of input with that name\n",
    "    q_func: (tf.Variable, int, str, bool) -> tf.Variable\n",
    "        the model that takes the following inputs:\n",
    "            observation_in: object\n",
    "                the output of observation placeholder\n",
    "            num_actions: int\n",
    "                number of actions\n",
    "            scope: str\n",
    "            reuse: bool\n",
    "                should be passed to outer variable scope\n",
    "        and returns a tensor of shape (batch_size, num_actions) with values of every action.\n",
    "    num_actions: int\n",
    "        number of actions\n",
    "    reuse: bool\n",
    "        whether or not to reuse the graph variables\n",
    "    optimizer: tf.train.Optimizer\n",
    "        optimizer to use for the Q-learning objective.\n",
    "    grad_norm_clipping: float or None\n",
    "        clip gradient norms to this value. If None no clipping is performed.\n",
    "    gamma: float\n",
    "        discount rate.\n",
    "    double_q: bool\n",
    "        if true will use Double Q Learning (https://arxiv.org/abs/1509.06461).\n",
    "        In general it is a good idea to keep it enabled.\n",
    "    scope: str or VariableScope\n",
    "        optional scope for variable_scope.\n",
    "    reuse: bool or None\n",
    "        whether or not the variables should be reused. To be able to reuse the scope must be given.\n",
    "    Returns\n",
    "    -------\n",
    "    act: (tf.Variable, bool, float) -> tf.Variable\n",
    "        function to select and action given observation.\n",
    "`       See the top of the file for details.\n",
    "    train: (object, np.array, np.array, object, np.array, np.array) -> np.array\n",
    "        optimize the error in Bellman's equation.\n",
    "`       See the top of the file for details.\n",
    "    update_target: () -> ()\n",
    "        copy the parameters from optimized Q function to the target Q function.\n",
    "`       See the top of the file for details.\n",
    "    debug: {str: function}\n",
    "        a bunch of functions to print debug data like q_values.\n",
    "    \"\"\"\n",
    "    act_f = build_act(make_obs_ph, q_func, num_actions, scope=scope, reuse=reuse)\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # set up placeholders\n",
    "        obs_t_input = U.ensure_tf_input(make_obs_ph(\"obs_t\"))\n",
    "        act_t_ph = tf.placeholder(tf.int32, [None], name=\"action\")\n",
    "        rew_t_ph = tf.placeholder(tf.float32, [None], name=\"reward\")\n",
    "        obs_tp1_input = U.ensure_tf_input(make_obs_ph(\"obs_tp1\"))\n",
    "        done_mask_ph = tf.placeholder(tf.float32, [None], name=\"done\")\n",
    "        importance_weights_ph = tf.placeholder(tf.float32, [None], name=\"weight\")\n",
    "\n",
    "        # q network evaluation\n",
    "        q_t = q_func(obs_t_input.get(), num_actions, scope=\"q_func\", reuse=True)  # reuse parameters from act\n",
    "        q_func_vars = U.scope_vars(U.absolute_scope_name(\"q_func\"))\n",
    "\n",
    "        # target q network evalution\n",
    "        q_tp1 = q_func(obs_tp1_input.get(), num_actions, scope=\"target_q_func\")\n",
    "        target_q_func_vars = U.scope_vars(U.absolute_scope_name(\"target_q_func\"))\n",
    "\n",
    "        # q scores for actions which we know were selected in the given state.\n",
    "        q_t_selected = tf.reduce_sum(q_t * tf.one_hot(act_t_ph, num_actions), 1)\n",
    "\n",
    "        # compute estimate of best possible value starting from state at t + 1\n",
    "        if double_q:\n",
    "            q_tp1_using_online_net = q_func(obs_tp1_input.get(), num_actions, scope=\"q_func\", reuse=True)\n",
    "            q_tp1_best_using_online_net = tf.arg_max(q_tp1_using_online_net, 1)\n",
    "            q_tp1_best = tf.reduce_sum(q_tp1 * tf.one_hot(q_tp1_best_using_online_net, num_actions), 1)\n",
    "        else:\n",
    "            q_tp1_best = tf.reduce_max(q_tp1, 1)\n",
    "        q_tp1_best_masked = (1.0 - done_mask_ph) * q_tp1_best\n",
    "\n",
    "        # compute RHS of bellman equation\n",
    "        q_t_selected_target = rew_t_ph + gamma * q_tp1_best_masked\n",
    "\n",
    "        # compute the error (potentially clipped)\n",
    "        td_error = q_t_selected - tf.stop_gradient(q_t_selected_target)\n",
    "        errors = U.huber_loss(td_error)\n",
    "        weighted_error = tf.reduce_mean(importance_weights_ph * errors)\n",
    "        # compute optimization op (potentially with gradient clipping)\n",
    "        if grad_norm_clipping is not None:\n",
    "            optimize_expr = U.minimize_and_clip(optimizer,\n",
    "                                                weighted_error,\n",
    "                                                var_list=q_func_vars,\n",
    "                                                clip_val=grad_norm_clipping)\n",
    "        else:\n",
    "            optimize_expr = optimizer.minimize(weighted_error, var_list=q_func_vars)\n",
    "\n",
    "        # update_target_fn will be called periodically to copy Q network to target Q network\n",
    "        update_target_expr = []\n",
    "        for var, var_target in zip(sorted(q_func_vars, key=lambda v: v.name),\n",
    "                                   sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "            update_target_expr.append(var_target.assign(var))\n",
    "        update_target_expr = tf.group(*update_target_expr)\n",
    "\n",
    "        # Create callable functions\n",
    "        train = U.function(\n",
    "            inputs=[\n",
    "                obs_t_input,\n",
    "                act_t_ph,\n",
    "                rew_t_ph,\n",
    "                obs_tp1_input,\n",
    "                done_mask_ph,\n",
    "                importance_weights_ph\n",
    "            ],\n",
    "            outputs=td_error,\n",
    "            updates=[optimize_expr]\n",
    "        )\n",
    "        update_target = U.function([], [], updates=[update_target_expr])\n",
    "\n",
    "        q_values = U.function([obs_t_input], q_t)\n",
    "\n",
    "        return act_f, train, update_target, {'q_values': q_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.221373Z",
     "start_time": "2017-07-15T14:37:06.591720+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "[2017-07-15 14:37:06,771] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "[2017-07-15 14:37:06,850] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    }
   ],
   "source": [
    "# Create training graph and replay buffer\n",
    "input_shape = (window_len, )+env.observation_space.shape\n",
    "act, train, update_target, debug = deepq.build_train(\n",
    "    make_obs_ph=lambda name: U.Uint8Input(shape=input_shape, name=name),\n",
    "    q_func=model,\n",
    "    num_actions=env.action_space.shape[0],\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=args.lr, epsilon=1e-4),\n",
    "    gamma=0.99,\n",
    "    grad_norm_clipping=10,\n",
    "    double_q=args.double_q\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.244857Z",
     "start_time": "2017-07-15T14:37:07.223046+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random vs predicted actions: agent playfullness\n",
    "approximate_num_iters = args.num_steps / 4\n",
    "exploration = PiecewiseSchedule([\n",
    "    (0, 1.0),                         #     0% of iters, 100% random actions\n",
    "    (approximate_num_iters / 50, 0.1),#     2% of iters, 10%  random actions\n",
    "    (approximate_num_iters / 10, 0.05),#   10% of iters,  5%  random actions\n",
    "    (approximate_num_iters / 5, 0.01) # at 20% of iters,  1%  random actions\n",
    "], outside_value=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.390449Z",
     "start_time": "2017-07-15T14:37:07.246284+08:00"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prioritised replay: agent dreaming which enhances learning speed\n",
    "if args.prioritized:\n",
    "    replay_buffer = PrioritizedReplayBuffer(args.replay_buffer_size, args.prioritized_alpha)\n",
    "    beta_schedule = LinearSchedule(approximate_num_iters, initial_p=args.prioritized_beta0, final_p=1.0)\n",
    "else:\n",
    "    replay_buffer = ReplayBuffer(args.replay_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.456956Z",
     "start_time": "2017-07-15T14:37:07.396006+08:00"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_env(env):\n",
    "    # get data\n",
    "    i1=env.src.idx\n",
    "    di=env.src.step\n",
    "    df_data = env.src.data[i1-di:i1-1]\n",
    "    print('df_data', len(df_data))\n",
    "\n",
    "    df_sim = env.sim.to_df()[-di:]\n",
    "    print('df_sim', len(df_sim))\n",
    "\n",
    "    if len(df_data)>len(df_sim):\n",
    "        df_data=df_data[:-1]\n",
    "    df_sim.index = df_data.index\n",
    "    df = pd.merge(df_data,df_sim,left_index=True,right_index=True)\n",
    "\n",
    "    # Plot prices\n",
    "    df.Close.plot(alpha=0.5, figsize=(12,6), color=\"black\")\n",
    "    plt.title('Trades')\n",
    "\n",
    "    # Plot actions\n",
    "    colors = dict(\n",
    "        LONG=\"green\",\n",
    "        SHORT=\"red\",\n",
    "        FLAT=\"blue\"\n",
    "    )\n",
    "    for i in range(env.action_space.n):\n",
    "        dfa=df[df.action==i]\n",
    "        action_name = env.sim.action_names[i]\n",
    "        plt.scatter(dfa.index,dfa.Close.values, s=15, marker='x',label='{}'.format(action_name), c=colors.get(action_name,None))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    df[['bod_nav','mkt_nav']].plot(figsize=(12,6))    \n",
    "    plt.title('Net asset values')\n",
    "    plt.show()\n",
    "    \n",
    "# plot_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.562079Z",
     "start_time": "2017-07-15T14:37:07.459078+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/openai/baselines/blob/master/baselines/deepq/experiments/atari/train.py\n",
    "\n",
    "U.initialize()\n",
    "update_target()\n",
    "\n",
    "# # Load the model\n",
    "mon_state = maybe_load_model(savedir, container)\n",
    "if mon_state is not None:\n",
    "    num_iters, replay_buffer = mon_state[\"num_iters\"], mon_state[\"replay_buffer\"],\n",
    "    monitored_env.set_state(mon_state[\"monitor_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:27:21.060833Z",
     "start_time": "2017-07-15T14:27:20.963802+08:00"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.838682Z",
     "start_time": "2017-07-15T14:37:07.563813+08:00"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb4080307bf4689954bab958014e490"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e2458f96d1b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Take action and store transition in the replay buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonitored_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_obs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add remembered observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isisilon/.pyenv/versions/3.6.0/envs/jupyter3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9f84d7505089>\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'infos'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_infos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "\n",
    "log_intv = 2000\n",
    "num_iters = 0\n",
    "\n",
    "\n",
    "start_time, start_steps = None, None\n",
    "steps_per_iter = RunningAvg(0.999)\n",
    "iteration_time_est = RunningAvg(0.999)\n",
    "obs = monitored_env.reset()\n",
    "state = memory.get_recent_state(obs)\n",
    "\n",
    "# Main training loop\n",
    "with tqdm(total=args.num_steps, mininterval=0.5) as progbar:\n",
    "    while True:\n",
    "        num_iters += 1\n",
    "\n",
    "        # Take action and store transition in the replay buffer.\n",
    "        action = act(np.array(state)[None], update_eps=exploration.value(num_iters))[0]\n",
    "        new_obs, rew, done, info = monitored_env.step(action)\n",
    "        new_state = memory.get_recent_state(new_obs) # add remembered observations\n",
    "        replay_buffer.add(state, action, rew, new_state, float(done))\n",
    "        state = new_state\n",
    "        \n",
    "        if done and episodes%log_intv==0:\n",
    "#             plot_env(env)\n",
    "            pass\n",
    "        if done: # reset the game environment\n",
    "            obs = monitored_env.reset()\n",
    "            state = memory.get_recent_state(obs)\n",
    "\n",
    "        # Replay/Dream\n",
    "        if (num_iters > max(5 * args.batch_size, args.replay_buffer_size // 20) and\n",
    "                num_iters % args.learning_freq == 0 and\n",
    "                len(memory.recent_observations)>window_len):\n",
    "            \n",
    "            # Sample a bunch of transitions from replay buffer, in batch\n",
    "            if args.prioritized:\n",
    "                experience = replay_buffer.sample(args.batch_size, beta=beta_schedule.value(num_iters))\n",
    "                (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n",
    "            else:\n",
    "                obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(args.batch_size)\n",
    "                weights = np.ones_like(rewards)\n",
    "            \n",
    "            # Minimize the error in Bellman's equation and compute TD-error\n",
    "            td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
    "            \n",
    "            # Update the priorities in the replay buffer\n",
    "            if args.prioritized:\n",
    "                new_priorities = np.abs(td_errors) + args.prioritized_eps\n",
    "                replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
    "\n",
    "        # Update target network.\n",
    "        if num_iters % args.target_update_freq == 0:\n",
    "            update_target()\n",
    "\n",
    "        # Stats\n",
    "        if start_time is not None:\n",
    "            steps_per_iter.update(info['steps'] - start_steps)\n",
    "            iteration_time_est.update(time.time() - start_time)\n",
    "        start_time, start_steps = time.time(), info[\"steps\"]\n",
    "\n",
    "        # Save the model and training state.\n",
    "        if num_iters > 0 and (num_iters % args.save_freq == 0 or info[\"steps\"] > args.num_steps):\n",
    "            maybe_save_model(savedir, container, {\n",
    "                'replay_buffer': replay_buffer,\n",
    "                'num_iters': num_iters,\n",
    "                'monitor_state': monitored_env.get_state()\n",
    "            })\n",
    "\n",
    "        if info[\"steps\"] > args.num_steps:\n",
    "            break\n",
    "            \n",
    "        # TODO validation test\n",
    "\n",
    "        # log\n",
    "        episodes = len(info[\"rewards\"])\n",
    "        if done and episodes%log_intv==0:\n",
    "            steps_left = args.num_steps - info[\"steps\"]\n",
    "            completion = np.round(info[\"steps\"] / args.num_steps, 1)\n",
    "            \n",
    "            info_means = pd.DataFrame(info[\"infos\"])[-log_intv:].mean()\n",
    "\n",
    "            logger.record_tabular(\"% completion\", completion)\n",
    "            logger.record_tabular(\"steps\", info[\"steps\"])\n",
    "            logger.record_tabular(\"iters\", num_iters)\n",
    "            logger.record_tabular(\"episodes\", episodes)\n",
    "            logger.record_tabular(\"reward (%s epi mean)\" % log_intv, info_means.reward)\n",
    "            logger.record_tabular(\"nav (%s epi mean)\" % log_intv, info_means.nav)\n",
    "            logger.record_tabular(\"nav_abv_mkt (mean)\", info_means.nav_abv_mkt)\n",
    "            logger.record_tabular(\"cost (%s epi mean)\" % log_intv, info_means.costs)\n",
    "            logger.record_tabular(\"exploration\", exploration.value(num_iters))\n",
    "            if args.prioritized:\n",
    "                logger.record_tabular(\"max priority\", replay_buffer._max_priority)\n",
    "            fps_estimate = (float(steps_per_iter) / (float(iteration_time_est) + 1e-6)\n",
    "                            if steps_per_iter._value is not None else \"calculating...\")\n",
    "            logger.dump_tabular()\n",
    "            logger.log()\n",
    "            logger.log(\"ETA: \" + pretty_eta(int(steps_left / fps_estimate)))\n",
    "            logger.log()\n",
    "            \n",
    "        # Probar\n",
    "        progbar.desc = 'reward={reward: 2.4f}, nav={nav: 2.2f} nav_abv_mkt={nav_abv_mkt: 2.2f} costs={costs: 2.4f}'.format(\n",
    "            reward=info['reward'],\n",
    "            nav=info[\"nav\"],\n",
    "            nav_abv_mkt=info[\"nav_abv_mkt\"],\n",
    "            costs=info[\"costs\"]\n",
    "        )\n",
    "        progbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.840064Z",
     "start_time": "2017-07-15T06:36:51.303Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.840879Z",
     "start_time": "2017-07-15T06:36:51.306Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info_means = pd.DataFrame(info[\"infos\"])[-log_intv:].mean()\n",
    "info_means.nav_abv_mkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.842173Z",
     "start_time": "2017-07-15T06:36:51.310Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maybe_save_model(savedir, container, {\n",
    "            'replay_buffer': replay_buffer,\n",
    "            'num_iters': num_iters,\n",
    "            'monitor_state': monitored_env.get_state()\n",
    "        })\n",
    "save_path = '{}/model-{}'.format(savedir, num_iters)\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-02T22:25:55.168854Z",
     "start_time": "2017-07-03T06:25:55.147845+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.843413Z",
     "start_time": "2017-07-15T06:36:51.314Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(info[\"infos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.844712Z",
     "start_time": "2017-07-15T06:36:51.317Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obses_t.shape, actions.shape, rewards.shape, obses_tp1.shape, dones.shape, weights.shape\n",
    "# memory.recent_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.846019Z",
     "start_time": "2017-07-15T06:36:51.320Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# history\n",
    "history = pd.DataFrame(info['infos'])[['nav', 'reward']]\n",
    "history['episodes'] = history.index\n",
    "\n",
    "g = sns.jointplot(x=\"episodes\", y=\"nav\", data=history, kind=\"reg\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "g = sns.jointplot(x=\"episodes\", y=\"reward\", data=history, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# visualise\n",
    "\n",
    "ideally a price with colored actions? like https://hackernoon.com/the-self-learning-quant-d3329fcc9915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.847293Z",
     "start_time": "2017-07-15T06:36:51.323Z"
    },
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test, play\n",
    "def test_env(env, verbose=True):\n",
    "    obs = env.reset()\n",
    "    state = memory.get_recent_state(obs)\n",
    "    for t in range(env.days):\n",
    "        action = act(state[None], update_eps=exploration.value(t))[0]\n",
    "        obs, rew, done, info = env.step(action)\n",
    "        state = memory.get_recent_state(obs)\n",
    "    if verbose:\n",
    "        print('nav', env.sim.navs[-1], 'market_nav', env.sim.mkt_nav[-1])\n",
    "    return env.sim.to_df()\n",
    "df_test = test_env(env_test)\n",
    "df_test[['bod_nav','mkt_nav']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.848558Z",
     "start_time": "2017-07-15T06:36:51.326Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-03T10:43:27.866076Z",
     "start_time": "2017-07-03T10:40:07.605Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.849900Z",
     "start_time": "2017-07-15T06:36:51.332Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tests={}\n",
    "for i in tqdm(range(10)):\n",
    "    df = test_env(env_test, verbose=False)\n",
    "    tests[i]=df.iloc[-1]\n",
    "tests = pd.DataFrame(tests).T\n",
    "tests.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.851159Z",
     "start_time": "2017-07-15T06:36:51.335Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(tests.bod_nav, label='model nav')\n",
    "sns.distplot(tests.mkt_nav, label='holding nav')\n",
    "plt.xlabel('Net Asset Value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T22:37:27.319065Z",
     "start_time": "2017-07-01T06:37:27.090650+08:00"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Test sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.852475Z",
     "start_time": "2017-07-15T06:36:51.338Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# try 100 runs with random guessing\n",
    "navs=[]\n",
    "for _ in range(100):\n",
    "    obs = env.reset()\n",
    "    state = memory.get_recent_state(obs)\n",
    "    for t in range(252):\n",
    "        action = 1\n",
    "        obs, rew, done, info = env.step(action)\n",
    "        state = memory.get_recent_state(obs)\n",
    "    df_display = env.sim.to_df()\n",
    "    # mean of last 50 days\n",
    "    nav = df_display.bod_nav[-50:].mean()\n",
    "    navs.append(nav)\n",
    "# show dist\n",
    "plt.title('hold')\n",
    "sns.distplot(navs)\n",
    "plt.show()\n",
    "\n",
    "# try 100 runs with random guessing\n",
    "navs=[]\n",
    "for _ in range(100):\n",
    "    obs = env.reset()\n",
    "    state = memory.get_recent_state(obs)\n",
    "    for t in range(252):\n",
    "        action = 0\n",
    "        obs, rew, done, info = env.step(action)\n",
    "        state = memory.get_recent_state(obs)\n",
    "    df_display = env.sim.to_df()\n",
    "    # mean of last 50 days\n",
    "    nav = df_display.bod_nav[-50:].mean()\n",
    "    navs.append(nav)\n",
    "# show dist\n",
    "plt.title('short')\n",
    "sns.distplot(navs)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# try 100 runs with random guessing\n",
    "navs=[]\n",
    "for _ in range(100):\n",
    "    obs = env.reset()\n",
    "    state = memory.get_recent_state(obs)\n",
    "    for t in range(252):\n",
    "        action = 2\n",
    "        obs, rew, done, info = env.step(action)\n",
    "        state = memory.get_recent_state(obs)\n",
    "    df_display = env.sim.to_df()\n",
    "    # mean of last 50 days\n",
    "    nav = df_display.bod_nav[-50:].mean()\n",
    "    navs.append(nav)\n",
    "# show dist\n",
    "plt.title('long')\n",
    "sns.distplot(navs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T08:34:29.827468Z",
     "start_time": "2017-06-30T16:34:29.806637+08:00"
    },
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-03T11:09:47.536213Z",
     "start_time": "2017-07-03T19:09:47.505352+08:00"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.853748Z",
     "start_time": "2017-07-15T06:36:51.343Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make X and y for traditiona ML training\n",
    "window_length = memory.window_length\n",
    "\n",
    "X = []\n",
    "D = env.src.data.as_matrix()\n",
    "D[np.isnan(D)]=0\n",
    "for i in range(window_length,len(D)):\n",
    "    # at each step we get the past few observations\n",
    "    x = D[i-window_length:i]\n",
    "    X.append(x)\n",
    "X=np.array(X)\n",
    "\n",
    "\n",
    "\n",
    "# convert y to down, flat, up categorical labels\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "y = np.concatenate([[0],np.diff(env.src.data.Close)])\n",
    "y = y[window_length:]\n",
    "\n",
    "short = y<0\n",
    "flat = y==0\n",
    "long = y>0\n",
    "y[short]=0\n",
    "y[flat]=1\n",
    "y[long]=2\n",
    "\n",
    "y=to_categorical(y)\n",
    "\n",
    "print('X.shape',X.shape)\n",
    "print('y.shape', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.854996Z",
     "start_time": "2017-07-15T06:36:51.346Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_flat = X.reshape((len(D)-window_length,-1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flat, y, test_size=0.2, random_state=0)\n",
    "\n",
    "dummy_scores = []\n",
    "for strategy in ['most_frequent', 'uniform', 'prior', 'stratified']:\n",
    "    clf = DummyClassifier(strategy=strategy)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(strategy, score)\n",
    "\n",
    "for strategy in ['mean', 'median']:\n",
    "    clf=DummyRegressor(strategy=strategy)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    score=clf.score(X_test, y_test)\n",
    "    print(strategy, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.856290Z",
     "start_time": "2017-07-15T06:36:51.349Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.857536Z",
     "start_time": "2017-07-15T06:36:51.352Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile('adam','mse',metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train, \n",
    "          verbose=True,\n",
    "          nb_epoch=100\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.858835Z",
     "start_time": "2017-07-15T06:36:51.354Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test,y_test)\n",
    "score = dict(zip(model.metrics_names,score))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T06:37:07.860188Z",
     "start_time": "2017-07-15T06:36:51.357Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred.argmax(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
